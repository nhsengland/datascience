{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"PhDInterns/","title":"PhD Internship Scheme","text":"<p>Our internships are aimed at current PhD students looking for an industrial placement of around five months with the right to work in the UK. The projects are focussed on innovation, in particular around getting the most value out of NHS data.</p> <p>The projects often have a focus on emerging data science techniques and so we advertise mainly to data science programmes, however previous interns have come from other disciplines such as clinical, mathematics, computer science and bioinformatics, which have added huge value through the range of approaches and knowledge.</p> <p>For more information and details on how to apply see the Scheme Overview page on the microsite</p> <p>For details on open projects see the Projects page on the microsite</p> <p>Available outputs from previous projects can also be seen at Previous Projects on the microsite</p>"},{"location":"PhDInterns/#current-projects","title":"Current Projects","text":"<p>Currently our interns are working on the following projects in two waves.  These are the original briefs they applied to and their work and outputs will be available on our organisation GitHub.</p> Wave 6 February - July 2024 NHS Language Corpus Extension Understanding Fairness and Explainability in Multi-modal Approaches within Healthcare Wave 7 July - December 2024 Evaluating NER-focussed models and LLMs for identifying key entities in histopathology reports \u2013 working with GOSH DRIVE Investigating Privacy Concerns and Mitigations for Healthcare Language and Foundation Models"},{"location":"about/","title":"About Data Science in NHS England","text":"<p>We are the NHS England Data Scientists.   </p> <p>We are passionate about getting the most value out of the data collected by NHS England and the wider NHS through applying innovative techniques in appropriate and well-considered ways.</p> <p>Our vision is:</p> <p>Embed ambitious yet accessible data science in health and care to help people live healthier longer lives\u200b</p> <p>See our work</p> <p>Contact Us (datascience@nhs.net)</p>"},{"location":"about/#teams","title":"Teams","text":"<p>In NHSE data scientists are concentrated in the Data Science &amp; Applied AI Team but also embedded across a number of other areas.</p> <ul> <li> <p> Data Linkage</p> <p>The Data Linkage Hub aims at providing a unified and quality solution to the data linkage needs in NHS England. Data Science is central to achieving this objective, and it covers many aspects, from the mathematical models of entity resolution and record linkage, to identifying and correcting linkage errors, assessing their impact on downstream applications, and ensuring quality. </p> <p>See the Data Linkage Hub.</p> </li> <li> <p> Data Science &amp; Applied AI Team</p> <p>We develop and deploy data science products to make a positive impact on NHS patients and workforce.  We investigate applying novel techniques that increase the insight we get from health-related data.\u200b  We prioritise code-first ways of working, transparency and promoting best practice. We champion quality, safety and ethics in the application of methods and use of data. \u200b We have the remit to be open and collaborative and have the aim of sharing products with the wider healthcare community. </p> <p>See our Projects.</p> </li> <li> <p> National SDE Team</p> <p>Working with customer researchers and analysts to identify how they can do their research, overcome and rectify data issues and use the platform and data to its fullest. There is also work to create products and tools that facilitate research in the environment such as data quality and completeness visualisations, example analysis and machine learning code as well as continuous improvement and increasing automation of the processes to get data both into the SDE and out through output checking. \u200b </p> <p>See the SDE website.</p> </li> <li> <p> Other Embedded Data Scientists</p> <p>Across the organisation individual data scientists are embedded within specific team (including: Workforce, Training and Education (WT&amp;E); Medicines; Patient Safety; AI Lab; Digital Channels etc.).   </p> <p>We come together through the data science assembly to align our professional development and standards.</p> </li> </ul>"},{"location":"about/#learn-about-data-science-in-healthcare","title":"Learn about Data Science in Healthcare","text":"<p>To support knowledge share of data science in healthcare we've put together a monthly newsletter with valuable insights, training opportunities and events.</p> <p>Note</p> <p>The newsletter is targeted towards members of the NHS England Data Science Profession, so some links may only be accessible to those with the necessary login credentials, however the newsletter and its archive are available for all at the link above.</p> NHS England Data Science Professional Development Newsletter <p>We also support the NHS Data Science Community hosted in AnalystX, which is the home of spreading data science knowledge within the NHS.  You can also learn a lot about data science from the other communities we support:</p> <ul> <li>Govt Data Science Community</li> <li>NHS R Community</li> <li>NHS Pycom</li> </ul>"},{"location":"about/#our-members","title":"Our Members","text":"Our Members <p> Name Role Team Github NameRoleTeamGithub Sarah CulkinDeputy DirectorData Science &amp; Applied AI TeamSCulkin-code Rupert ChaplinAssistant DirectorData Science &amp; Applied AI Teamrupchap Jonathan HopeData Science LeadData Science &amp; Applied AI TeamJonathanHope42 Jonathan PearsonData Science LeadData Science &amp; Applied AI TeamJRPearson500 Achut ManandharData Science LeadData Science &amp; Applied AI Teamachutman Simone ChungPrincipal Data Scientist (Section Head)Data Science &amp; Applied AI Teamsimonechung Nicholas Groves-KirkbyPrincipal Data Scientist (Section Head)Data Science &amp; Applied AI Teamngk009 Eladia Valles CarreraPrincipal Data Scientist (Section Head)Data Science &amp; Applied AI Teamlilianavalles Daniel Schofield Principal Data Scientist (Section Head)Data Science &amp; Applied AI Teamdanjscho Elizabeth JohnstonePrincipal Data Scientist (Section Head)Data Science &amp; Applied AI TeamLiziJohnstone Hadi ModarresPrincipal Data Scientist (Section Head)Data Science &amp; Applied AI Teamhadimodarres1 Alice WaterhousePrincipal Data Scientist (Section Head)Data Science &amp; Applied AI TeamAliceWaterhouse1 <p>Giulia MantovaniHead of Data Linkage HubData Linkage HubGiuliaMantovani1 Divya BalasubramanianData Science LeadData Linkage Hubdivyabala09 Efrosini SetakisData Science LeadData Linkage Hubefrosini-s Jonny LaidlerPrincipal Data Scientist (Section Head)Data Linkage HubJonathanLaidler Alice TapperPrincipal Data ScientistData Science &amp; Applied AI Teamalicetapper1 Adam HollingsPrincipal Data ScientistData Science &amp; Applied AI TeamAdamHollings Mia NoonanPrincipal Data ScientistData Science &amp; Applied AI Teamamelianoonan1-nhs Sean AllerPrincipal Data ScientistData Science &amp; Applied AI Teamseanaller Ben WallacePrincipal Data ScientistData Science &amp; Applied AI Team Angeliki AntonarouPrincipal Data ScientistNational SDE Data Science TeamAnelikiA Shoaib Ali AjaibSenior Data ScientistNational SDE Team Nickie WareingSenior Data ScientistNational SDE Teamnickiewareing Helen RichardsonSenior Data ScientistNational SDE Teamhelrich Humaira HusseinSenior Data ScientistNational SDE Teamhumairahussein1 Xiyao ZhuangSenior Data ScientistNational SDE Teamxiyaozhuang Elizabeth KellySenior Data ScientistNational SDE Teamejkcode Michael SpenceSenior Data ScientistData Science &amp; Applied AI Teammspence-nhs Kenneth QuanSenior Data ScientistData Science &amp; Applied AI Teamquan14 Joseph WilsonSenior Data ScientistData Science &amp; Applied AI Teamjosephwilson8-nhs Thomas BouchardSenior Data ScientistData Science &amp; Applied AI Teamtom-bouchard Catherine SadlerSenior Data ScientistData Science &amp; Applied AI TeamCatherineSadler&lt;/a William PoulettSenior Data ScientistData Science &amp; Applied AI Teamwillpoulett Amaia Imaz BlancoSenior Data ScientistData Science &amp; Applied AI Teamamaiaita Scarlett KynochSenior Data ScientistData Science &amp; Applied AI Teamscarlett-k-nhs Jennifer StruthersSenior Data ScientistData Science &amp; Applied AI Teamjenniferstruthers1-nhs Matthew TaylorSenior Data ScientistData Science &amp; Applied AI Teammtaylor57 Warren DaviesData ScientistData Science &amp; Applied AI Teamwarren-davies4 Chaeyoon KimData ScientistWorkforce, Training &amp; Education TeamChaeyoonKimNHSE Ilja LomkovData ScientistWorkforce, Training &amp; Education TeamIljaLomkovNHSE Harriet Sands(former) Senior Data ScientistData Science &amp; Applied AI Teamharrietrs Oluwadamiloju Makinde(former) Senior Data ScientistNational SDE Team Jane Kirkpatrick(former) Senior Data ScientistData Science &amp; Applied AI Team Sam Hollings(former) Principal Data ScientistData Science &amp; Applied AI TeamSamHollings Alistair Jones (former) Senior Data ScientistNational SDE Teamalistair-jones Daniel Goldwater(former) Senior Data ScientistData Science &amp; Applied AI TeamDanGoldwater1 Jennifer Hall(former) Data Science LeadData Linking Hub Paul Carroll (former) Principal Data Scientist (Section Head)Data Science &amp; Applied AI Teampauldcarroll Kevin Fasusi(former) Principal Data ScientistNational SDE Data Science TeamKevinFasusi Sami Sultan(former) Data ScientistWorkforce, Training &amp; Education TeamSamiSultanNHSE Marek Salamon(former) Senior Data ScientistNational SDE Team Jake Kasan(former) Senior Data Wrangler (contract)National SDE Team Lucy Harris(former) Senior Data ScientistMeds Team Vithursan Vijayachandrah(former) Senior Data ScientistWorkforce, Training &amp; Education TeamVithurshanVijayachandranNHSE <p>Shelby Thompson(former) Data ScientistData Science &amp; Applied AI Team Sudeshna Mallik(former) Data ScientistData Science &amp; Applied AI Team Michelle Nwachukwu(former) Data ScientistData Science &amp; Applied AI Team </p>"},{"location":"meta_page/","title":"Site Info","text":""},{"location":"meta_page/#contribution","title":"Contribution","text":"<p>The guide for contributing to this website can be be found here. Please feel free to follow those steps and reach out to amaia.imazblanco1@nhs.net or sam.hollings1@nhs.net for review and approval. </p>"},{"location":"meta_page/#contributors","title":"Contributors \u2728","text":"<p>This website exists with the hard work of our contributors so thank you to the following (emoji key):</p> <sub>amaiaita</sub>\ud83d\udc1b \ud83d\udcdd \ud83d\udcbb \ud83d\udd8b \ud83c\udfa8 \ud83d\udea7 \ud83d\udce3 \ud83d\udc40 \ud83e\uddd1\u200d\ud83c\udfeb \ufe0f\ufe0f\ufe0f\ufe0f\u267f\ufe0f \ud83d\udce2 \ud83d\udcd6 <sub>jrpearson-nhs</sub>\ud83d\udd8b \ud83d\udea7 \ud83d\udc40 <sub>Sam Hollings</sub>\ud83d\udc1b \ud83d\udcdd \ud83d\udcbb \ud83d\udd8b \ud83d\udcd6 \ud83c\udfa8 \ud83e\udd14 \ud83d\udea7 \ud83e\uddd1\u200d\ud83c\udfeb \ud83d\udce3 \ud83d\udc40 <sub>Kin Quan</sub>\ud83d\udc1b <sub>benwallace2nhs</sub>\ud83d\udd8b \ud83d\udc1b <sub>jenniferstruthers1-nhs</sub>\ud83e\udd14 \ud83d\udcbb \ud83d\udd8b \ud83d\udce2 \ud83d\udea7 \ud83d\udc1b \ud83d\udcdd <sub>amelianoonan1-nhs</sub>\ud83d\udd8b \ud83d\udc40 <sub>Will Poulett</sub>\ud83d\udcdd \ud83d\udd8b \ud83d\udc40 \ud83d\udcbb \ud83d\udce3 <sub>ngk009</sub>\ud83d\udc40 <sub>Adam Hollings</sub>\ud83d\udc40 \ud83d\udd8b \ud83d\udcdd \ud83e\uddd1\u200d\ud83c\udfeb \ud83d\udc1b \ud83d\udcd6 <sub>Giulia Mantovani</sub>\ud83d\udc40 \ud83d\udd8b \ud83e\udd14 <sub>JonathanLaidler</sub>\ud83d\udc40 \ud83d\udd8b <sub>Harriet Sands</sub>\ud83d\udd8b \ud83d\udea7 \ud83d\udc40 <sub>mtaylor57</sub>\ud83d\udea7 \ud83d\udd8b \ud83d\udcbb \ud83d\udcd6 <sub>chaeyoon</sub>\ud83d\udc1b <sub>DanGoldwater1</sub>\ud83d\udd8b <sub>Dan Schofield</sub>\ud83d\udcdd \ud83d\udc1b <sub>Scarlett Kynoch</sub>\ud83d\udd8b <sub>warren-davies4</sub>\ud83d\udcbb \ud83c\udfa8 \ud83d\udc1b \ufe0f\ufe0f\ufe0f\ufe0f\u267f\ufe0f \ud83d\udc40 <sub>hadimodarres1</sub>\ud83d\udcbb \ud83d\udd8b \ud83d\udcd6 <sub>michelle7707</sub>\ud83d\udcbb \ud83d\udd8b \ud83d\udcd6 \ud83d\udcdd \ud83d\udce2 <sub>Joe Wilson</sub>\ud83d\udcdd \ud83e\udd14 <sub>Sean Aller</sub>\ud83d\udcbb \ud83d\udd8b \ud83d\udcdd <sub>AliceWaterhouse1</sub>\ud83d\udcbb \ud83d\udd8b <p>This project follows the all-contributors specification. Contributions of any kind welcome!</p> <p>This site uses:</p> <ul> <li>mkdocs-material</li> <li>mkdocs-awesome-pages</li> </ul>"},{"location":"playbooks/","title":"Playbooks","text":"<p>In the Data Science &amp; Applied AI Team, we've contributed to a number of playbooks, policies and other major documents, that you can see below:</p> Name Description RAP Playbook A playbook on how to get your organisation going with RAP, made in concert with other organisations in the public sector. NHS Open Source Policy A proposed open source policy to guide NHS colleagues in working more openly. MPS Person_ID handbook for HES users Guidance on how Person_ID is calculated in HES. NHS AI Skunkworks Playbooks The NHS AI Skunkworks made some playbooks about the applications of AI."},{"location":"useful_links/","title":"Useful Links","text":"<p>This is a list, mostly copied from RAP guidance pages and shows a range of resources throughout the Government and other areas. Let us know if you have any links to useful information, resources or guides that could be added to the list.</p>"},{"location":"useful_links/#strategic","title":"Strategic","text":"<ul> <li>The Goldacre Review proposes 185 recommendations for secure, broad, ethical health data analysis in UK for research and improved care.</li> </ul>"},{"location":"useful_links/#guidance-standards-and-best-practice","title":"Guidance, Standards and Best Practice","text":"<ul> <li>The AQUA book of analytical standards.</li> <li>The ONS best practice team have a useful website - often called the Quack book covering many of the same topics we cover here. Their best-practice checklist is particularly useful.</li> <li>The Government Analysis Functions website contains a lot of best practice guides such as on plotting colours, accessibility and standards for ethnicity data.</li> </ul>"},{"location":"useful_links/#reproducible-analytical-pipelines-rap-and-best-practice","title":"Reproducible Analytical Pipelines (RAP) and Best Practice","text":"<ul> <li>The NHS England Data Science &amp; Applied AI Team have created an in-depth resource in best practice and on how to accomplish Reproducible Analytical Pipelines (RAP) that is well worth a look.</li> <li>RAP guidance pages useful links page which includes links to policy and other documentation that supports the adoption of RAP</li> <li>How to implement RAP and training resources in Git, Python and R.</li> </ul>"},{"location":"useful_links/#the-turing-way","title":"The Turing Way","text":"<p>The Turing Way have also produced a vast range of resources including guides on:</p> <ul> <li>Reproducible research</li> <li>Project Design</li> <li>Communication</li> <li>Ethical research</li> </ul>"},{"location":"useful_links/#causal-inference","title":"Causal Inference","text":"<p>The Causal Inference Interest Group (CIIG) hosts monthly seminars which discuss recent advances in the field of causal inference, from both empirical and formal perspectives. Everyone with an interest in discussing causal inference is very welcome to come along. Causal inference applies robust methodology in order to answer the questions \"How can we distinguish causation from correlation, and in turn find out how one thing influences another?\"</p>"},{"location":"useful_links/#tools-and-nhs-england-shared-code","title":"Tools and NHS England Shared Code","text":""},{"location":"useful_links/#the-nhs-england-secure-data-environment-nhse-sde","title":"The NHS England Secure Data Environment (NHSE SDE)","text":"<ul> <li>The NHSE SDE is the preferred platform and associated service through which researchers get access to English national level data to preform health research. It is part of the NHS Research SDE Network.</li> <li>Some NHSE Data Scientists function as Data Wranglers for the NHSE SDE service creating research ready datasets, resources, tools and visualisations to allow users to understand the data more easily and facilitate their research.</li> </ul>"},{"location":"useful_links/#nhs-england-digital-github","title":"NHS England / Digital Github","text":"<ul> <li>The NHS Digital and NHS England Githubs contain code shared from NHS Digital and NHS England projects. Well worth a look. </li> </ul>"},{"location":"useful_links/#community-spaces","title":"Community Spaces","text":"<p>There are several slack channels that discuss RAP and related topics: the govdatascience.slack.com RAP channel, the NHS-R community, and the NHS-pycom community. </p> <p>We have an MS Teams page (internal to NHS England).</p>"},{"location":"articles/","title":"Articles and Blog posts","text":""},{"location":"articles/2023/01/05/rap-in-NHSE/","title":"Why we\u2019re getting our data teams to RAP","text":"<p>Reproducible analytical pipelines (RAP) help ensure all published statistics meet the highest standards of transparency and reproducibility. Sam Hollings and Alistair Bullward share their insights on adopting RAP and give advice to those starting out.</p> <p>Reproducible analytical pipelines (RAP) are automated statistical and analytical processes that apply to data analysis. It\u2019s a key part of national strategy and widely used in the civil service. </p> <p>Over the past year, we\u2019ve been going through a change programme and adopting RAP in our Data Services directorate. We\u2019re still in the early stages of our journey, but already we\u2019ve accomplished a lot and had some hard-learnt lessons.</p> <p></p> <p>This is about analytics and data, but knowledge of RAP isn\u2019t just for those cutting code day-to-day. It\u2019s crucial that senior colleagues understand the levels and benefits of RAP and get involved in promoting this new way of working and planning how we implement it.</p> <p>This improves the lives of our data analysts and the quality of our work.</p> <p>Read the whole article HERE (opens in new tab)</p>"},{"location":"articles/2024/04/11/privLM/","title":"Investigating Privacy Concerns and Mitigations for Language Models in Healthcare","text":"<p>Over recent years, larger, more data-intensive Language Models (LMs) with greatly enhanced performance have been developed. The enhanced functionality has driven widespread interest in adoption of LMs in Healthcare, owing to the large amounts of unstructured text data generated within healthcare pathways. </p> <p>However, with this heightened interest, it becomes critical to comprehend the inherent privacy risks associated with these LMs, given the sensitive nature of Healthcare data. This PhD Internship project sought to understand more about the Privacy-Risk Landscape for healthcare LMs through a literature review and exploration of some technical applications.</p>"},{"location":"articles/2024/04/11/privLM/#overview","title":"Overview","text":""},{"location":"articles/2024/04/11/privLM/#lms-can-memorize-their-training-data","title":"LMs can memorize their Training Data","text":"Figure 1: xkcd 2169 - Predictive Models (opens in new tab) <p>Studies have shown that LMs can inadvertently memorise and disclose information verbatim from their training data when prompted in certain ways, a phenomenon referred to as training data leakage. This leakage can violate the privacy assumptions under which datasets were collected and can make diverse information more easily searchable.</p> <p>As LMs have grown, their ability to memorize training data has increased, leading to substantial privacy concerns. The amount of duplicated text in the training data also correlates with memorization in LMs. This is especially relevant in healthcare due to the highly duplicated text in Electronic Healthcare Records (EHRs).</p> <p>If LMs have been trained on private data and are subsequently accessible to users who lack direct access to the original training data, the model could leak this sensitive information. This is a concern even if the user has no malicious intent.</p>"},{"location":"articles/2024/04/11/privLM/#privacy-attacks","title":"Privacy Attacks","text":"<p>A malicious user can stage a privacy attack on an LM to extract information about the training data purposely. Researchers can also use these attacks to measure memorization in LMs. There are several different attack types with distinct attacker objectives. </p> <p>One of the most well-known attacks is Membership inference attacks (MIAs). MIAs determine whether a data point was included in the training data of the targeted model. Such attacks can result in various privacy breaches; for instance, discerning that a text sequence generated by Clinical LMs (trained on EHRs) originating from the training data can disclose sensitive patient information. </p> <p>At the simplest level, MIAs use the confidence of the target model on a target data instance to predict membership. A threshold is set against the confidence of the model to ascertain membership status. For a specific example, if the confidence is greater than the threshold then the attacker assumes the target is a member of the training data, as the model is \"unsurprised\" to see this example, indicating it has likely seen this example before during training. Currently, the most successful MIAs use reference models. This refers to a second model trained on a dataset similar to the training data of the target model. The reference model filters out uninteresting common examples, which will also be \"unsurprising\" to the reference model.</p>"},{"location":"articles/2024/04/11/privLM/#privacy-mitigations","title":"Privacy Mitigations","text":"<p>There are three primary approaches to mitigate privacy risks in LMs:</p> <ul> <li>Methods for data preprocessing - Data Sanitization aims to remove all sensitive information from data before the model aims to eliminate all sensitive information before model training. Data sanitization approaches are very effective when sensitive information follows a context-independent, consistent format (e.g., NHS numbers, email addresses, etc.). Still, they cannot guarantee the privacy of contextual text. Data Deduplication aims to remove duplicated sequences of text of a certain length in the dataset, leaving only one unique instance. This has been demonstrated to reduce overall memorization in large LMs. </li> <li>Training strategies include utilizing privacy-preserving learning algorithms, e.g., differentially private (DP) training - In DP training, during back propagation, the gradients for individual examples are clipped to a fixed norm, and noise is added to this before updating model parameters. This limits the effect that any one example can have on the model parameters, reducing the ability of the model to memorize the example. </li> <li>Post-training techniques such as Machine Unlearning or Editing - Machine Editing and Unlearning comprise a set of techniques for modifying or erasing information post-training, rendering them highly applicable in real-world scenarios. This could be used, for example, when somebody wishes to practice their Right-to-be-Forgotten, removing private data from the training data after model training. </li> </ul>"},{"location":"articles/2024/04/11/privLM/#what-we-did","title":"What we did","text":"<p>In this project, we sought to understand more about the Privacy-Risk Landscape for Healthcare LMs and conduct a practical investigation of some existing privacy attacks and defensive methods.  </p> <p>Initially, we conducted a thorough literature search to understand the privacy risk landscape. Our first applied work package explored data deduplication before model training as a mitigation to reduce memorization and evaluated the approach with Membership Inference Attacks. We showed that RoBERTa models trained on patient notes are highly vulnerable to MIAs, even when only trained for a single epoch. We investigated data deduplication as a mitigation strategy but found that these models were just as vulnerable to MIAs. Further investigation of models trained for multiple epochs is needed to confirm these results. In the future, semantic deduplication could be a promising avenue for medical notes. </p> <p>Our second applied work package explored editing/unlearning approaches for healthcare LMs. Unlearning in LMs is poised to become increasingly relevant, especially in light of the growing awareness surrounding training data leakage and the 'Right to be Forgotten'. We found that many repositories for performing such approaches were not adapted for all LM types, and some are still not mature enough to be easy to use as packages. Exploring a Locate-then-Edit approach to Knowledge Neurons, we found this was not well suited to the erasure of information we needed in medical notes. Our findings suggest that the focus from a privacy perspective on these methods should be on those which allow the erasure of specific training data instances instead of relational facts. </p>"},{"location":"articles/2024/04/11/privLM/#where-next","title":"Where Next?","text":"<p>This work primarily explored privacy in pre-trained Masked Language Models. The growing adoption of generative LMs underscores the importance of expanding this work to Encoder and Encoder-Decoder models like the GPT family and T5. Also, due to the common practice of freezing parameters and tuning the last layer of a LM on a private dataset, it is critical to expand investigations of privacy risks to LMs fine-tuned on healthcare data. </p> <p>Within the scope of this exploration, the field of Machine Unlearning/Editing applied to LMs was in its infancy, but it is gaining momentum. As this field matures, comparing the efficacy of different methods becomes crucial. Furthermore, it is important to explore the effect of removing the influence of a set of data points. A holistic examination of the effectiveness, privacy implications, and broader impacts of Machine Unlearning/Editing methods on healthcare LMs is essential to inform the development of robust and privacy-conscious LMs in the NHS.</p> <p>When considering explainability of models, this often involves generating explanations or counterfactuals alongside the decisions made by the LM. However, integrating explanations into the output of LMs can introduce vulnerabilities related to training data leakage and privacy attacks. Additionally, efforts to enhance privacy, such as employing Privacy-preserving training techniques, can inadvertently impact fairness, particularly in datasets lacking diversity. In healthcare, all three elements are paramount, so investigating the privacy-explainability-fairness trade-off is crucial for developing private, robust and ethically sound LMs.</p> <p>Finally, privacy concerns in several emerging trends for LMs need to be understood in Healthcare scenarios. Incorporating external Knowledge Bases to enhance LMs, known as retrieval augmentation, could make LMs more likely to leak private information. Further, Multimodal Large Language Models (MLLM), referring to LM-based models that can take in and reason over multimodal information common in healthcare, could be susceptible to leakage from one input modality through another output modality. </p>"},{"location":"articles/2024/07/08/annotation-tools/","title":"Investigating Annotation Tools for Named Entity Recognition","text":"<p>We have been building a proof-of-concept tool that scores the privacy risk of free text healthcare data. To use our tool effectivly, users need a basic understanding of the entities within their dataset which may contribute to privacy risk. </p> <p>There are various tools for annotating and exploring free text data. The author explores some of these tools and discusses his experiences. </p>"},{"location":"articles/2024/07/08/annotation-tools/#introduction","title":"Introduction","text":"<p>We have been building a proof-of-concept tool that scores the privacy risk of free text healthcare data called Privacy Fingerprint (opens in new tab).</p> <p>Named Entity Recognition (NER) is a particularly important part of our pipeline. It is the task of identifying, categorizing and labelling specific pieces of information, known as entities, within a given piece of text. These entities can include the names of people, dates of birth, or even unique identifiers like NHS Numbers.</p> <p>As of the time of writing, there are two NER models fully integrated within the Privacy Fingerprint pipeline used to identify entities which may contribute towards a privacy risk. These are:</p> <ul> <li>UniversalNER (opens in new tab): A prompted-based NER Model, where a language model has been finetuned with a conversation-style prompt to output a list containing all entities in the text corresponding to an input entity type.</li> <li>GLiNER (opens in new tab): A BERT-like bidirectional transformer encoder with a key benefit over UniversalNER in that it is a smaller model in terms of memory size.</li> </ul> <p>Both NER models in our pipeline need to be fed a list of entities to extract. This is true for many NER models, although some like Stanza (opens in new tab) from Stanford NLP Group (opens in new tab) and BERT (opens in new tab) token classifiers do not need an initial entity list for extraction. For our privacy tool to be effective, we want our list of entities to be representative of the real entities in the data, and not miss any important information.</p> <p></p> Figure 1: A frustrated user trying to extract entites!.  <p>Let's consider a new user who wants to investigate the privacy risk of a large unstructured dataset. Maybe they want to use this data to train a new generative healthcare model and don\u2019t want any identifiable information to leak into the training data. Or maybe this dataset is a large list of outputs from a similar model and they want to ensure that no identifiable information has found it's way into the data. They may ask:</p> <p>What does my data look like?</p> <p>What entities within my data have a high privacy risk?</p> <p>Wait a second, what even is an entity?</p> <p>We want to offer an easy and interactive starting point for new users of our tool, where they can easily explore their data, understand the role of NER and identify what risks lie in their data. If they miss certain entities, this could have large implications on the scoring aspect of our pipeline.</p> <p>Of course, we want people to use our tool efficiently and effectively! So we asked:</p> <p>How can a new user efficiently explore their data to understand what entities exist within the data, and in particular, which entities may contribute to a privacy risk?</p>"},{"location":"articles/2024/07/08/annotation-tools/#annotation-tools","title":"Annotation Tools","text":"<p>Interactive annotation tools offer a solution to the above problem. If we can include a tool which allows a user to manually label their dataset, alongside live feedback from the NER model, it would allow a user to very quickly understand the entities in their data.</p> <p>Further to this, some NER models can be surprisingly affected by the wording of entities. The entity titled 'name' may extract both the name of an individual and the name of a hospital. The entity 'person' might only extract the name of the person. We have found that changing the entity 'person' to 'name' in UniversalNER reduced how often names were picked up by the model. If a user gets live feedback from a model whilst labelling, this will help them both finetune which entity names work best, alongside picking out which entities to use at all.</p> <p>We want a tool that:</p> <ul> <li>Is easy for a user to setup and use.</li> <li>Allows users to label new entities.</li> <li>Gets live feedback from the NER model.</li> </ul> <p>There were two approaches we took to develop an annotation tool.</p>"},{"location":"articles/2024/07/08/annotation-tools/#displacy-and-ipywidgets","title":"DisplaCy and ipyWidgets","text":"Figure 2: An example of the ipyWidgets and DisplaCy labelling application. All clinicial notes are synthetic.  <p>First, we used DisplaCy (opens in new tab), ipyWidgets (opens in new tab), and a NER model of choice to generate an interactive tool that works inside Jupyter notebooks. DisplaCy is a visualiser integrated into the SpaCy library which allows you to easily visualise labels. Alongside ipyWidgets, a tool that allows you to create interactive widgets such as buttons, we created an interface which allowed a user to go through reviews and add new entities.</p> <p>One of the main advantages of this method is that everything is inside a Jupyter notebook. The entity names you want to extract come straight from the experiment parameters, so if you used this in the same notebook as the rest of your pipeline the entitiy names could be updated automatically from the labelling tool. This would allow easy integration into a user workflow.</p> <p>There is also a button which allows for live feedback from the NER model which is useful given our previous comment on different entitity names having different effects on the NER model.</p> <p>This approach was simple and resulted in a fully working example. However, highlighting entities manually was not possible, and this meant it was hard to correct predictions that the model got wrong. You are fully reliant on the labels given by the model, and can't add your own.</p>"},{"location":"articles/2024/07/08/annotation-tools/#streamlit","title":"Streamlit","text":"Figure 3: An example of the Streamlit labelling application. All clinicial notes are synthetic.  <p>We explored a second option using Streamlit (opens in new tab). Streamlit is a python framework that allows you to build simple web apps. We can use it alongside a package called Streamlit Annotation Tools (opens in new tab) to generate a more interactive user interface. As an example, a user can now use their cursor to highlight particular words and assign them an entity type which is more hands-on and engaging. Unlike our ipyWidgets example, users can select different labels to be displayed which makes the tool less cluttered, and you can easily navigate using a slider to separate reviews. Like the previous widget, there is a button which uses a NER model to label the text and give live feedback. Including this, the tool is more synergistic, easier to use and more immersive than the ipyWidgets alternative.</p> <p>However, there were still a few teething issues when developing the Streamlit app. Firstly, Streamlit annotation tool\u2019s has an inability to display <code>\\n</code> as a new line and instead prints <code>\\n</code>, resulting in the structure of text being lost. This is a Streamlit issue and we haven\u2019t yet found a way to keep the structure of the text intact. There was an easy fix in which each <code>\\n</code> was replaced with two spaces (this means the start and end character count for each labelled entity remains consistent with the original structured text), but the structure of the text is still lost which may cause issues for some future users.</p> <p>Secondly, Streamlit involves a little bit more set up than ipyWidgets. Rather than interacting with the reviews in your notebook you run the app on a local port and access it through your browser. This also makes it harder to retrieve back into your pipeline the list of entities you have labelled. Whilst there is benefit to running all your analysis in one jupyter notebook, the Streamlit app gives a better user experience.</p>"},{"location":"articles/2024/07/08/annotation-tools/#future-work-and-conclusion","title":"Future Work and Conclusion","text":"<p>Both labelling tools we have identified have key advantages. DisplaCy and ipyWidgets fit well into your workflow, whilst Streamlit offers a nicer user experience. ipyWidgets and Streamlit are both versatile tools, and so users can edit the annotation tools in the future to fit their own use case.</p> <p>Following the research and development of these two tools, we believe the ability to interactively annotate, explore and extract entities from your data greatly improves the user experience when using our privacy risk scorer pipeline.</p> <p>We will publish working examples of annotation using both ipyWidgets and Streamlit, such that a future user can build on them or use them to improve their workflow. The code is available on our github (opens in new tab).</p>"},{"location":"articles/2024/12/04/microsoft-hackathon/","title":"NHS x Microsoft Hack for Health","text":"<p>The NHS England Data Science team, as well as a range of other analysts from across the organisation, attended an AI Hackathon at Microsoft, organised by the Data Science Team together with Microsoft and Kainos, with the key stakeholders being the NHS Websites Services Team. In this article, the author shares her experiences at the event. </p> <p>Last week, the long awaited Hack for Health hosted at the beautiful Story Club, in Paddington London, by Microsoft and NHSE finally happened! At the hackathon, every team had either a Kainos or Microsoft representative, as well as a range of participants from across NHS England, including a strong Data Science Team presence, with several of our team members in each group. And unlike a usual hackathon, this one had stakeholders and well defined use cases, we were all working to one common goal. </p> <p>The use cases were outlined as: </p> <ol> <li>Search &amp; Summarise: using an extract of the NHS Corpus developed by one of our PhD interns, Sam Hollands, (we used two websites worth of data, out of 600 NHS websites, and it was already 25GB worth of data!), we had to develop an LLM that used RAG (see the Data Science Team's RAG Project Website Page) to sift through that data and retrieve answers to queries based on these NHS websites.</li> <li>Explaining to a 12 year old: Adapting the search and summarise for different personas, one of which being a 12 year old child</li> <li>Duplication &amp; Conflict Detection: Comparing the two sites and identifying duplicate information and conflicting information.</li> </ol> <p>The aim was to develop generative AI solutions for website services, who could adapt and improve them for their own use cases. Having use cases meant that it was so much easier to split work up, and get our heads down, with everyone having something to actively work on. It also meant we had a structured approach, making the experience so much more fulfilling. I was delighted by the creativity and range of approaches that the different teams took when it came to presenting at the end of the two days. We ranged from teams that had taken a thin slice approach to all three usecases, to teams that had gone really in depth in just one of them, to teams that had taken use case number three and made it into a tool that could be used by the websites team to improve the websites on the backend, removing conflict and duplication. </p> <p>Overall, I was left in awe by the creativity and technical skills of our team, as well as of all the other attendees. Hopefully the work gets used in the future by the websites team, and I hope that any hackathons I attend in the future are of this high quality! (Of course it did help that my team won)</p> <p></p> <p>The final results were: </p> <p>First Place: A thin slice approach of all three usecases, a project which included 4 of our very own data scientists: Sean Aller, Sudeshna Mallik, Xiyao Zhuang, and myself, as well as Rob Mansfield, Veta Ngammekchay, and our wonderful Kainos helper Peter Bodnar. </p> <p>Second Place: Data Scientists Chaeyoon Kim and Warren Davis, with Mary Amanuel, Piyali Dutta, and Farwah Kazmi from elsewhere in the NHS, together with Microsoft's Dan Watkinson and Josh Mercurio developed an AI career coach that was able to draw from and cite relevant information from the HEE website, and was easily customisable to improve communication towards different user personas.</p> <p>Third Place: Contradiction Finder by Data Science's Ben Wallace, Matt Taylor, and Jenny Chim, as well as Andrew Walker and Microsoft's Hannah Howell and Hanna Riaz. Focused on use case 3, making a usable tool for the websites team to find contraditions. </p> <p></p> <p>Quotes from some of the attendees about their experience: </p> <p>Bashir Abubakar</p> <p>The hackathon was an incredible experience that not only allowed me to learn how to use Azure AI Foundry but also deepened my understanding of how large language models (LLMs) can transform healthcare, particularly within the NHS. My fascination with transformers began when I first read Attention is All You Need paper, which revolutionised the NLP space with its groundbreaking approach to self-attention. Seeing this theory in action, from research papers to practical applications, has been nothing short of inspiring. The hackathon felt like a full-circle moment, as it opened new pathways for applying LLMs in healthcare, a vision I\u2019ve long held for the future of AI professionals (Industry 4.0). It also reinforced the transformative role of prompt engineering, a skill I believe is pivotal in unlocking the potential of AI in creating meaningful solutions. </p> <p>Will Poulett:</p> <p>The variety of professions within each team was great, it's not often that GP's and data scientists can work together using generative AI. The solutions developed by each team were varied and interesting, I'm looking forward to seeing how they are implemented in the future!</p>"},{"location":"articles/2024/12/12/RISE_tool/","title":"The RISE Tool \u2013 An Easy Way for Testers and Assurers to Evaluate AI Classifiers","text":"<p>We have built a proof-of-concept tool which will help assurers, data scientists and clinicians to evaluate AI classifiers. We call this the RISE tool, it utilises LLM's, AI Image Generators and an interactive plot to allow users to easily evaluate image classifiers. We carried out careful experimentation to ensure its effectiveness, and plan to continue this research in the future.</p>"},{"location":"articles/2024/12/12/RISE_tool/#introduction","title":"Introduction","text":"<p>Within NHS England, testers and assurers are increasingly being asked to assure AI models and systems, including AI classifiers. For assurers who are used to deterministic code and functional testing, this can be quite the challenge. F1 scores, AUC-ROC curves and aptly named confusion matrices are all used by data scientists to evaluate these AI models. These metrics can be hard to understand and for multi-class models can easily trip up anyone \u2013 assurer or data scientist. As the development of increasingly complex models increases, it\u2019s important we make it easier for assurers to evaluate AI systems, bridging the gap that currently exists between data scientists and technical assurers.</p> <p>Bridging this gap is the aim of the AI Quality Community of Practice - a group of both data scientists and technical assurers. Alongside upskilling technical assurers with training and offering guidance on AI assurance, we have also spent time developing new tools to improve the testing and assurance of AI models \u2013 such as using mixup images to try and identify a model\u2019s decision boundary (see our paper here!).</p> <p>In this article we present our preliminary study of a new tool - RISE. It is a pipeline leveraging generative AI that aims to make evaluating AI classifiers quicker, easier, and less reliant on data science technical knowledge. It will support assurers, data scientists and even clinicians.  We don\u2019t intend for this to replace other AI evaluation methods, rather to compliment them. We believe this tool can help identify potential biases that can\u2019t be found via other techniques, making it incredibly useful throughout the AI development lifecycle.</p>"},{"location":"articles/2024/12/12/RISE_tool/#so-what-is-rise","title":"So, what is RISE?","text":"<p>RISE stand for Risk-Informed Synthetic Embeddings. On a very high level it follows these steps (don\u2019t worry \u2013 we\u2019ll go into more detail later!):</p> <ol> <li>A tester, data scientist or clinician creates a list of Risk-Informed scenarios they expect may occur within the AI classifier.</li> <li>Large Language Models (LLMs) are used to first increase the number of scenarios, and then to generate image prompts. These are small paragraphs used to explain to an image generation model what sort of images to generate.</li> <li>An AI Image Generator creates Synthetic images.</li> <li>The AI classifier being evaluated makes predictions on these images.</li> <li>Using the model\u2019s final hidden layer activations and dimension reduction techniques, we create two-dimensional interactive scatter plots of the model Embeddings.</li> <li>Users can use this tool to spot potential biases, incorrect predictions and identify areas where more test data is required.</li> </ol> <p>This tool has a lot of moving parts. Whilst one level of success would be to simply make a working prototype, we want to ensure that this work helps real assurers within NHS England. To do this we created an experiment with the aim of answering the following 4 questions:</p> <ol> <li>Can we use generative AI to turn scenarios into test data?</li> <li>Does an interactive tool make evaluating AI classifiers easier?</li> <li>Do image labels improve the tool?</li> <li>Does knowing the model\u2019s prediction change how evaluators interpret the results?</li> </ol> <p></p> Figure 1: Our RISE tool experiment. Split into four sections, this experiment aimed to answer the key questions on the right-hand side."},{"location":"articles/2024/12/12/RISE_tool/#the-experiment","title":"The Experiment","text":"<p>Figure 1 demonstrates how we structured and ran our experiment. It is split into four steps: <code>Traditional AI Training + Evaluation</code> refers to training and evaluating an AI classifier using typical data science techniques.</p> <p><code>Risk Informed Image Generation</code> is the first step of the RISE pipeline, where LLM\u2019s first increase the list of scenarios, then use it to generate image prompts. These are fed into an image generator to create our synthetic test dataset.</p> <p>The <code>Human Labelling</code> stage used 14 volunteers to label our synthetic dataset.</p> <p>The <code>Interactive Tool</code> stage of the experiment completes the RISE pipeline. Predictions are made by the AI classifier and using dimension reduction techniques we plot the model embeddings on an interactive scatter plot.</p> <p>Throughout the experiment, we noted results from our evaluations and gained feedback from labellers and end users. At the end of this article, we will refer to these four questions and asses just how successful the experiment was.</p>"},{"location":"articles/2024/12/12/RISE_tool/#traditional-ai-training-evaluation","title":"Traditional AI Training + Evaluation","text":"<p>To train a model, we first needed an image dataset. If you had a keen eye, you may have noticed some pictures of dogs and cats in Figure 1, and indeed we used the Animal Faces Dataset to train and evaluate our AI classifier. This may seem like an interesting choice for NHS England where we treat humans rather than pets, but there were various reasons behind this choice.</p> <ol> <li>We needed a dataset that was easily interpretable. For this pilot we were not engaging with experts or clinicians, and therefore needed a dataset that all volunteers (and I!) could interpret, understand and accurately label.</li> <li>We wanted a multiclass dataset \u2013 this has the classes 'Domestic Dogs', 'Domestic Cats' and 'Wildlife'.  I had previously built a smaller proof-of-concept with two classes which was successful however I wanted to up the complexity.</li> <li>We wanted high quality documentation.</li> <li>We wanted the dataset to have some similarities to medical datasets. We see little variation between images in medical datasets (consider chest X-rays), and therefore wanted to replicate this with our dataset. As the Animals Faces Dataset only contained the face of animals in each image, this resulted in similar images across the dataset.</li> <li>We wanted to easily generate synthetic images. AI image generators are very good at generating images of pets and wildlife. Whilst they may also be good at generating medical images, without experts we cannot be sure of their quality.</li> </ol> <p></p> Figure 2: Images from the Animal Faces Dataset.  <p>It goes without saying that we intend to use this tool on medical datasets in the future, with guidance from clinicians as to how realistic and useful AI generated medical images are. If you want to read ahead, our exact plans on future research can be found at the end of this article.</p>"},{"location":"articles/2024/12/12/RISE_tool/#model-setup","title":"Model Setup","text":"<p>Once our dataset was selected, it was time to train a model. Our model was trained using transfer learning on top of the EfficientNetV2S model, with ImageNet weights. The model performed exceptionally well on a test dataset containing 1467 images (493, 491 and 483 images for cats, dogs and wildlife respectively) with 99.8% accuracy. The precision for the cat and dog class was perfect. The only incorrect classifications were three images, all predicted as a wild animal when their label was either a cat or a dog. </p> <p>The three images that were incorrectly classified are shown in Figure 3. The left-most image stands out most due to a possible instance of label noise. The image is likely a clouded leopard \u2013 wildlife, yet has a ground truth label of a cat. This means the model probably got the prediction right! If we have identified possible label noise in the test dataset, we can assume there are probably instances of it in the training dataset.</p> <p>There are certainly improvements to the model training process that we could have used when running this experiment, such as screening for label noise and bias within the training dataset. This is something we would undoubtably do as data scientists working on NHS England projects, however for this experiment having a non-perfect model has some advantages. It means we can expect some areas of poor performance in the model, and then ensure end users are able to spot these errors when trialling the tool.</p> <p></p> Figure 3: Incorrectly classified images in the test dataset."},{"location":"articles/2024/12/12/RISE_tool/#scenario-generation","title":"Scenario Generation","text":"<p>Once the model was trained, it was time to develop the first stage of the RISE tool \u2013 turning a list of scenarios into a synthetic image dataset. This involved the use of both an LLM and an AI Image Generator.</p> <p>We wanted a list of scenarios that tested both likely and non-likely scenarios. Likely scenarios are those that are likely to have appeared in the training data - simple images of dogs and cats. Non-likely scenarios are those which won\u2019t have appeared often in the training dataset but may still occur in the future. We may also wish to make an initial guess as to what sort of scenarios may trip the model up. For example, a cat holding a tennis ball may be mistaken as a dog, given this a typically a dog-like behaviour.</p> <p>Our LLM of choice for this step was Llama 3.1 8B. Whilst not the most powerful of LLMs, its main advantage was that it could be run locally on a laptop. For future iterations of this tool using medical datasets, this means possible sensitive data never has to leave your computer or data platform. DallE3 was used for image generation. This cannot be run locally, but we found its generation capabilities to be much better than smaller models such as Stable Diffusion v1 which we trialled locally. We expect that higher quality image generation models will be able to run locally in the near future, so were happy to use DallE3 for this experiment.</p> <p>An assurance college was given the initial evaluation results alongside a description of the dataset and generated an initial list of 14 scenarios for us to test.  These included 'domestic dogs that look like wild dogs' and 'multiple animals in one picture'. We asked Llama 3.1 8B with a temperature of 0.7 to generate an additional list of scenarios, and it did so generating a list of 44 new scenarios. We then asked it again to consider its previous risks and generate some more, this time adding 15 new scenarios. Whenever we used an LLM we followed good prompt guidance, this included asking the model to adopt a persona, asking the model if it missed anything on previous passes and giving examples.</p> <p>Compiling all of these risks together we ended up with 20 high quality scenarios, including new scenarios not considered in our initial list. New scenarios included 'unusual or creative use of colour' and 'dogs and cats with medical injuries'. It was clear that an LLM was helpful for generating and considering new scenarios. </p>"},{"location":"articles/2024/12/12/RISE_tool/#prompt-generation","title":"Prompt Generation","text":"<p>We then used Llama 3.1 8B to generate five image prompts for each scenario, again following good prompt guidance. This was successful, although there were a few interesting errors we experienced when generating LLM responses.</p> <p>Here are two examples:</p> <p><code>I've created six detailed prompts to generate synthetic images. These prompts focus on creating images \u0627\u0644\u0623\u0633\u0631of animals in motion, blurred faces, and other related scenarios.</code></p> <p><code>2 \u0437\u0430\u043a\u0440\u044b\u0442A dog with a fluffy, cream-colored coat and black markings that resemble a panda's distinctive fur pattern, sitting in a serene garden surrounded by blooming flowers and a tranquil pond. The dog's eyes are closed, and its paws are tucked under its body as it enjoys the peaceful atmosphere.</code></p> <p>In both of the above examples, apparently random foreign words appeared in the output. The rest of the image prompt seems fine! We still aren\u2019t sure why this occurred, but it did mean we had to manually review all prompts before using them to generate images.</p>"},{"location":"articles/2024/12/12/RISE_tool/#image-generation","title":"Image Generation","text":"<p>We used DallE3, accessed via Bing's Copilot to generate our images using the generated prompts. This was an inefficient step, and in future we will either use a local model or use an API call. However, this method was free and still provided sufficiently high quality images for this piece of work.</p> <p>Some example images are shown in Figure 4, including 'edge-case' images.</p> <p></p>  Figure 4: A sample of AI generated images in our dataset."},{"location":"articles/2024/12/12/RISE_tool/#edge-case-examples","title":"Edge-Case Examples","text":"<p>We wanted to pay particular attention to edge-case image examples, as this mirrors boundary analysis in traditional software testing. Boundary analysis is the testing of values very close to a decision boundary. Some of our scenarios already included edge-case examples. This included 'Crossbreed or hybrid animals', which we hoped would lie closer to the model's decision boundary and would help us identify where the model changes its mind, and which features in an image correspond to this.</p> <p>Additionally, we used GPT4o and DallE3 to generate 40 more image prompts for edge case scenarios \u2013 in particular hybrid animals. These were animals that had features from multiple classes, and to a human were hard to classify. In a medical dataset, this may be a certain disease with symptoms similar to an alternative disease.</p>"},{"location":"articles/2024/12/12/RISE_tool/#human-labelling","title":"Human Labelling","text":"<p>We used 14 volunteers to label our dataset. Our total dataset was 288 images, of which we considered 148 as 'hard' to classify. Making our dataset smaller we hoped would result in higher quality labels, as volunteers wouldn't get 'button fatigue' \u2013 losing engagement in the tool as they did more and more labelling.</p> <p>To gather labels we put these 148 images into a new dataset where they were resized to the size used by the model. Model predictions were gathered, and each image was randomly assigned a number of 1 or 0, splitting the dataset randomly in two. </p> <p>We then created an image labelling tool using ipywidgets. For each image, users were asked to select whether the image was a domestic cat, domestic dog or of wildlife. There was a 50% chance the user would be told the model\u2019s prediction. As the dataset was randomly split in half, we ensured that for each image there would be seven occasions when the prediction was given, and seven without. This allowed us to explore the effect of a user being told a model's prediction on their classification.. </p> <p>We decided to keep the labelling tool simplistic to ensure that volunteers did not get bored, and thus gave us high quality labels. This meant removing possible features such as an 'other' button, or 'multiple classes' button. Even with this, we did see button fatigue, when some users got on a roll they made mistakes. If generating a similar label tool in the future, we may wish to consider adding additional features such as a timer which records the how long it takes the user to make a decision, and possibly a back button. </p> <p></p>  Figure 5: The image labelling tool, this time showing the model's prediction for an edge-case image.  <p></p>  Figure 6: Five images that had equal counts of multiple classes. All five had seven votes for either domestic cat or domestic dog, and seven for wildlife.   <p>For each image, we assigned a label based on the most common vote. If an image had six votes as a domestic cat, five as a domestic dog and three as wildlife, we would label it as a domestic cat. We defined confidence as the number of votes for that class divided by the total number of votes. For this example, that would be 6 / 14 which is approximately 43%. For each image, we also assigned a label based on if the users were shown a prediction when classifying the image, or if they were not. When all labels are considered, there were five images where the two most common classes had an equal number of votes. These images are shown in Figure 6.</p> <p>For this proof-of-concept piece, the argmax function built into numpy assigned each of these images a label. However, in future iterations of the tool we should handle these occurrences in a more sophisticated way. What this does demonstrate is that we really did generate some edge case images, ones even humans struggle to classify.</p> Labels Domestic Cat Domestic Dog Wildlife Predictions shown 49 38 61 Predictions not shown 49 36 63 All labels 53 37 58 <p>The above table shows the different label counts when users were shown model predictions, were not shown model predictions and the combination of both. The differences are small. We knew that some edge case images were hard to classify, these are the ones that received equal numbers of votes for multiple classes. We also knew that some users made mistakes whilst labelling these images with the label tool. This might have explained some of the small differences we see in the above table. The prediction being shown doesn't seem to have had a significant impact. </p> <p>There were nine occasions where the labels changed depending on whether or not predictions were shown, this number excluded the five images that were shown in Figure 6. All of these images had a confidence of no greater than 64% across all 14 votes. Given the confidence was low and the images are edge-case, it is not surprising that the label changed. The nature of these images is more likely to explain the changing label, as opposed to users being shown the prediction. </p> <p>Across the whole dataset and considering all 14 voters, 82% of labels agreed with model predictions and 39% of labels had 100% confidence in their label. </p> <p>There are plenty of ways to improve this section of the experiment for future studies. We have already mentioned the use of a timer, but if we are to move on to a medical dataset and let clinicians use the tool, we may also want a back button or an 'I don't know' button, alongside using a larger cohort of labellers to try and get statistically significant results. </p>"},{"location":"articles/2024/12/12/RISE_tool/#interactive-tool","title":"Interactive Tool","text":"<p>Let's finally talk about the interactive tool. We had synthetic images, we had labels, all that remained was to create a clear way of plotting and interacting with them. </p>"},{"location":"articles/2024/12/12/RISE_tool/#dimension-reduction","title":"Dimension Reduction","text":"<p>To create our scatter plots, we needed a way to turn model predictions into a set of two-dimensional coordinates. This was achieved using hidden layer activations and dimension reduction techniques. </p> <p>Our image classifier was a type of neural network. Essentially, neural networks are made up of layers, with each layer containing a number of neurons. When a model makes a prediction on an image, each layer influences the next, using patterns and rules it learned during training. The final layer makes the prediction and in our case contained three neurons, each corresponding to a class: dogs, cats, and wildlife.</p> <p>Just before this is the 'final hidden layer', which in our model contained 32 neurons. When making a prediction, each of these neurons produced a number that the final layer used to decide how to classify the image. We could examine the values of these 32 neurons for each image in our dataset. Dimension reduction techniques compressed these 32 values into two dimensions and displayed them as scatter plots. At this stage in the neural network, the model had already identified patterns and similarities between classes, which we could visualise as clusters in the plot, with similar images appearing closer together.</p> <p>We picked out five different dimension reduction techniques for our tool: TSNE, PCA, Feature Agglomeration, Isomap and Umap. Each one contains a link to some documentation if you'd like to learn more about how they work. Each technique was given all of the long list of 'hidden layer activations' and compressed these down into two dimensions.</p> <p>If an assurer were to inspect these clusters, they may find occurrences where a certain group of similar images are misclassified. They might even have similar features or themes which can then be used to identify risks within the model. With dogs and cats, perhaps a cluster of images of cats holding tennis balls are all misclassified as dogs. In a clinical chest X-ray dataset, perhaps a chest X-ray with a broken rib is instead classified as having a tumour. </p>"},{"location":"articles/2024/12/12/RISE_tool/#tool-design","title":"Tool Design","text":"<p>We used Bokeh to create our tool that can be accessed within a Jupyter Notebook. The tool was essentially an interactive scatter plot, where you could use a slider to navigate between different dimension reduction techniques.</p> <p></p>  Figure 7: A RISE tool gif, this was shown to users as preparation for how they could use the tool. <p>Points were coloured based on the model's prediction, and there was the ability to change the shape of each point based on the human assigned labels. When you hovered over a point the image was shown. If you looked at the right had side, you could see a selection of images based on the cluster you highlighted. </p> <p>This tool is available on GitHub, and you can try it out yourself here.</p> <p>Once our tool was created, we asked a group of assurers to trial it. Two respondents used human assigned labels within the tool, whilst three did not. Each assurer was asked to identify potential risks with the AI classifier, flag which images demonstrated said risks, asked whether they would want to generate additional test data, and also comment on the usability of the tool. We'll summarise the responses from each user below.</p>"},{"location":"articles/2024/12/12/RISE_tool/#did-this-tool-highlight-any-potential-risks-with-the-ai-classifier","title":"Did this tool highlight any potential risks with the AI Classifier?","text":"<p>Colleagues using the tool with and without labels were able to identify potential risks. When no labels were included, images of wildlife were misclassified, the image of a cat holding a tennis ball was misclassified, and some users found occasions where images containing no animals were predicted as dogs. One user pointed out the two images seen in Figure 8, which were predicted differently despite looking very similar.</p> <p></p> Figure 8: Two images that appear very similar but were classified differently by the model. This example was found by a user using the tool. <p>When labels were included, users were similarly successful, finding many of the same examples of those mentioned above.</p>"},{"location":"articles/2024/12/12/RISE_tool/#would-you-want-to-generate-any-additional-images-for-testing","title":"Would you want to generate any additional images for testing?","text":"<p>When labels were not included, users wanted to see more kittens, puppies and cubs, animals with their eyes closed, animals in action and more wildlife images. One user specified it would be useful to find images that are close to identical but with key features changed. This might be easier to implement now due to the recent release of 'Add it' - a tool using generative AI to easily add new features to existing images. With labels included, users wanted to look further into domestic cats.</p>"},{"location":"articles/2024/12/12/RISE_tool/#did-you-find-this-tool-useful-and-are-there-any-improvements-youd-like-to-see","title":"Did you find this tool useful and are there any improvements you\u2019d like to see?","text":"<p>The headline for this section all users found this tool to be very useful!</p> <p>That being said, we received various suggestions for improvements. Some of these improvements included:</p> <ol> <li> <p>Make the prediction of thumbnail images on the right clearer.</p> </li> <li> <p>Include a way to flag the image as 'questionable'.</p> </li> <li> <p>Add the capability to generate and add new images within the tool.</p> </li> </ol> <p>Overall, the tool appears to have been used successfully, with users enjoying the experience and identifying risks and misclassifications within the tool. The feedback we received from users was mixed in length and quality, which means directly comparing whether the tool is more useful with or without labels is hard. What is clear is that both cohorts were similarly successful in using the tool.</p> <p>I did notice that within the feedback there was some confusion regarding whether some behaviour was part of the tool or part of the AI classifier. One user suggested that the tool should have another class called 'other'. Whilst this is a good insight, it is in fact the model which would need another class. Additionally, some users mixed up terms such as efficiency and accuracy.</p> <p>This highlights the need for thorough staff training when using this tool, and many suggestions regarding the tools usability should be acted upon before the next iteration of this work.</p> <p>Additionally, none of the feedback referenced any edge-case (hybrid animal) images, instead pointing out occurrences when easily identifiable animals were performing certain behaviours or contained certain features. This is interesting, and may imply that the edge-case images we used were not found to be very helpful by the assurers. Alternatively, our guess of what an edge-case image was might have been completely wrong \u2013 the model interpreted images in a completely different way to human users.</p>"},{"location":"articles/2024/12/12/RISE_tool/#how-successful-were-we","title":"How successful were we?","text":"<p>Let's look back at the key questions identified earlier in this article and assess how well we can answer them following the experiment.</p> <p>Can we use generative AI to turn scenarios into test data?</p> <p>Simply, yes. We were successful in using generative AI to turn a list of scenarios into an image dataset. However, there were lots of manual steps involved in doing so. Ideally, this tool can be turned into a semi-autonomous pipeline where humans check intermediate steps but have less of a need to edit them or clean LLM outputs. </p> <p>We don't want users to have to remove random tokens written in another language every time they use the tool!</p> <p>Does an interactive tool make evaluating AI classifiers easier?</p> <p>This again seems to be successful. Assurers without much knowledge of AI systems were able to identify images and risks, whilst suggesting additional images they\u2019d like to generate and evaluate.</p> <p>Do image labels improve the tool?</p> <p>This is harder to measure. An ideal answer would be no, as this tool could be used without the time-consuming labelling step, whilst still being useful for evaluations. Whilst there are indications of this, we don\u2019t have enough information to come to a strong conclusion.</p> <p>Does knowing the model\u2019s prediction change how evaluators interpret the results?</p> <p>Again, this is hard to measure. The differences between seeing the model prediction and not seeing the model prediction when using the labelling tool is small. We likely need to run a larger experiment in order to get more statistically significant results.</p> <p>Adding an additional comment, we also gained very little evidence of assurers using hybrid animal pictures to make conclusions about the behaviour of the model. This doesn\u2019t suggest that AI generated images aren\u2019t useful for evaluation, more that our 'guess' of what an 'edge-case' image is, doesn\u2019t line up with the model\u2019s decision boundary.</p>"},{"location":"articles/2024/12/12/RISE_tool/#whats-next","title":"What's next?","text":"<p>So, what next? Whilst working with images of cats and dogs is fun, the entire aim of this work has been to transition to real, clinical datasets, helping assurers, data scientists and testers to evaluate real-world systems. We hope to soon engage with clinicians to find an image dataset that an expert can interpret, and run a similar experiment with them. Figure 9 explains partially why we think this will tool be successful in a clinical dataset, we know that this tool can identify key issues in an animal classifier, so why not a cancer detection model using chest X-rays?</p> <p>In the meantime, there are plenty of other improvements that can be made to the tool. New and improved open-source models are being released all the time, these can increase the reliability of the tool and possibly allow high quality image generation to be performed locally. We also received plenty of user feedback on the useability of the interactive tool, all of this should be considered for future iterations.</p> <p>The success of this work means that hopefully this is just the beginning. Keep your eyes peeled for more work, articles and research in this area.</p> <p></p> Figure 9: A representation of how this work using a dog cat classifier can be mapped onto a medical dataset."},{"location":"articles/2024/12/31/LLM-as-a-Judge/","title":"What is LLM-as-a-Judge and how can we use it?","text":"<p>We have been exploring using an LLM to evaluate and assess LLM summaries. This utilises the speed and language understanding of LLMs to score summaries, but how much trust can we put into an 'LLM-as-a-Judge'?</p> <p></p> Figure 1: An AI Generated cartoon depiction of 'LLM-as-a-Judge' <p>The rise of Large Language Models (LLMs) has resulted in AI summarisation tools popping up across the NHS. These tools essentially compress information found across multiple documents into a smaller and more concise summary. Given the amount of text data that exists across the NHS \u2013 these are very useful! Examples of their use include:</p> <ul> <li>A clinician might hand-pick documents and ask for a specific type of summary, saving them hours on a task that now takes a matter of minutes.</li> <li>Someone might want to learn more about a specific policy, which is written about on multiple online documents. A system could both retrieve the most relevant documents to this policy, and then use an LLM to summarise the key points.</li> </ul> <p>Regardless of the use-case, any system using AI needs to be meticulously evaluated, especially when used in a healthcare setting. With free text summaries, this can be quite challenging.</p>"},{"location":"articles/2024/12/31/LLM-as-a-Judge/#how-would-you-evaluate-a-summary","title":"How would you evaluate a summary?","text":"<p>Imagine you were given a list of 1000 AI-generated summaries to evaluate, alongside the documents used to create them. Even without being an expert in the field, you'd probably do quite a good job:</p> <ul> <li>First, you could first check whether all the facts included in the summary are from the reference documents. This checks whether the summary is grounded, and also whether the LLM has hallucinated.</li> <li>Then, you could check whether any key information was missed from each of the documents.</li> <li>You could also check whether the summary is well written for its target audience, is it cohesive and relevant?</li> <li>If you had human gold-standard summaries, you could do direct comparisons to see whether the two summaries convey the same information. Are AI generated summaries equally grounded, cohesive and relevant?</li> </ul> <p>Whilst this method of evaluation is very thorough and often considered the gold-standard, it\u2019s also incredibly time consuming.</p>"},{"location":"articles/2024/12/31/LLM-as-a-Judge/#surely-there-is-a-quicker-way","title":"Surely there is a quicker way?","text":"<p>There are alternative automatic evaluations to human feedback. If you have both human and AI summaries there are some quick similarity metrics you can run. BLEU [1] and ROUGE [2] compare whether words or short phrases overlap in the two summaries. This can be useful, but a lack of language understanding means if the exact same information is written in different ways, these algorithms wouldn\u2019t pick up the summaries are equally valid.</p> <p>This brings us on to the star of the show: LLM-as-a-Judge. We want an evaluation method that understands the intricacies of language and is significantly quicker than humans.</p> <p>An LLM-as-a-Judge uses an LLM to score certain characteristics of a summary. Typically, each characteristic is numerically scored based on instructions set out in a schema, and these characteristics can include groundedness, cohesiveness and relevance \u2013 metrics that can also be measured by humans.</p> <p>What's great about LLM-as-a-Judge is that you can score on characteristic you\u2019d like. Figure 2 shows an example a schema written to measure a new characteristic: fluency.</p> <p></p> Figure 2: This is an example schema used for an LLM judge that evaluates the fluency of a review. Fluency can be measured with only the summary; no additional information is needed. This is a shortened example, note that there are gaps where additional context would be, such as the example summary and the list of guidance.   <p>LLM-as-a-Judge is scientifically grounded. It has been shown that LLM judgements can align to human feedback [3], and there are various articles on how to improve your LLM judges [4].</p>"},{"location":"articles/2024/12/31/LLM-as-a-Judge/#who-judges-the-judges","title":"Who judges the judges?","text":"<p>Whilst LLM judges have been shown to be comparable to humans, we still need to be aware of the mistakes that LLMs can make. First, LLMs are typically non-deterministic, meaning an LLM can vary the score it gives a summary if ran multiple times. Secondly, LLM\u2019s are known to hallucinate, so could give an entirely incorrect score.</p> <p>Whilst human judges can be self-aware of their limitations and effectively self-manage themselves implicitly, our LLM-as-a-judge may not have the self-awareness to identify when it\u2019s not up to the task. In current literature three main approaches appear to be taken to evaluate the evaluators.</p> <ol> <li> <p>Comparison to Benchmark: Several studies [3, 5, 6] have explored using established benchmarks to compare the judges to.  However, all highlight various limitations of the general benchmarks and so this approach is not recommended as a final evaluation but can be useful in earlier development stages as a guide.</p> </li> <li> <p>Assessing Against Humans: Metrics are used in several studies [7, 8] to compare human and LLM judges. The resulting recommendation is to use Cohen\u2019s Kappa metric [7] to take into account chance agreements but overall, the metric approach is difficult and very bespoke especially for tasks where the LLM judge is attempting to assess none prescribed outputs or when the judging has a subjective component.</p> </li> <li> <p>Identify Strategies to Improve the Judge: This approach aims to highlight issues with the judge and then put in active strategies to mitigate them (similar to mandatory training for consistent issues recognised for human judges). There are a range of potential biases that judges can exhibit [3], these include:</p> <ul> <li>Position bias</li> <li>Verbosity bias (favouring longer more verbose responses)</li> <li>Self-Enhancement bias (prefer answers generated by themselves)</li> <li>Knowledge Bias</li> </ul> <p>Suggested mitigations for each bias include [11]:</p> <ul> <li>Use a 'Swap' Augmentation to mitigate position bias</li> <li>Reference support to mitigate knowledge bias</li> <li>Reference drop to mitigate format bias</li> </ul> </li> </ol> <p>There is no defined or straightforward way to assess the LLM-as-a-judge. Instead, a series of comparisons is required during the development phase to test for identified biases and to create a task specific benchmark.</p>"},{"location":"articles/2024/12/31/LLM-as-a-Judge/#conclusions","title":"Conclusions","text":"<p>An LLM-as-a-Judge is an exciting alternative to existing methods of evaluating summaries. It is faster than human evaluations and understands language better than traditional automatic evaluations. Whilst LLM\u2019s are not perfect, there are steps we can take to improve and validate their use for evaluation.</p>"},{"location":"articles/2024/12/31/LLM-as-a-Judge/#references","title":"References","text":"<p>1 - Two minutes NLP \u2014 Learn the BLEU metric by examples</p> <p>2 - Two minutes NLP \u2014 Learn the ROUGE metric by examples</p> <p>3 - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</p> <p>4 - Using LLM-as-a-judge \ud83e\uddd1\u200d\u2696\ufe0f for an automated and versatile evaluation</p> <p>5 - LLMsinstead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks</p> <p>6 - Evaluating Large Language Models at Evaluating Instruction Following</p> <p>7 - Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges</p> <p>8 - Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences </p>"},{"location":"articles/2025/03/14/LLM-as-a-Judge-Engagement/","title":"Comms and Marketing Spotlight \u2013 LLM as a Judge","text":"<p>\u201cThere is no joy in possession without sharing.\u201d \u2014 Erasmus</p> <p>A spotlight on how the project 'Using LLM as a Judge to evaluate Gen AI Search' was marketed effectively by Will Poulett.</p>"},{"location":"articles/2025/03/14/LLM-as-a-Judge-Engagement/#using-llm-as-a-judge-article-on-our-website","title":"Using LLM as a Judge \u2013 Article on our website","text":"<p>If we want our work to have the most impact, we need to be communicating it to the right people; and we at the Comms and Marketing function team can help you do that!  Will Poulett gracefully accepted to take part in an engagement with the comms and marketing team so that he could ensure the \"Using LLM as a Judge to evaluate Gen AI Search\" project was getting the most impact it could in the time he had available. In this article we will run through what was being done well and what was improved.</p> <p></p> Figure 1: An AI Generated cartoon depiction of 'LLM-as-a-Judge' <p>A reminder on what this project is:</p> <p>\u201cWe have been exploring evaluation methods for a Generative AI search tool, which both retrieves  and  summarises relevant information based on a user query. One such method is LLM-as-a-Judge, which  utilises the speed and language understanding of LLMs to score summaries. Excluding human scoring,  we recognise this is the best method for evaluating LLM summaries, but to use it safely we must  understand it\u2019s limitations and alignment to human scoring.\u201d</p>"},{"location":"articles/2025/03/14/LLM-as-a-Judge-Engagement/#what-is-a-comms-and-marketing-engagement","title":"What is a comms and marketing engagement?","text":"<p>It was a series of three 30-minute meetings between Will and at least one member of the comms and marketing team where we discussed both the current comms and marketing activities going on, how those activities could be documented and then we helped fill out a marketing plan for other future activities. </p>"},{"location":"articles/2025/03/14/LLM-as-a-Judge-Engagement/#what-comms-and-marketing-work-had-been-done","title":"What Comms and Marketing work had been done","text":"<p>Will had been conducting some great comms work:</p> <ul> <li>Regular stakeholder meetings to keep them informed, interested and using their feedback to keep the work relevant.</li> <li>Code saved to a public github repo including a streamlit labelling app. </li> <li>A set of confluence pages detailing stakeholders, risks, a description of the product and a link to the github </li> </ul>"},{"location":"articles/2025/03/14/LLM-as-a-Judge-Engagement/#how-we-helped","title":"How we helped","text":"Figure 2: A man walking down a road that winds into the distance."},{"location":"articles/2025/03/14/LLM-as-a-Judge-Engagement/#identify-a-clear-narrative","title":"Identify a clear narrative","text":"<p>We helped Will to identify a narrative for his work. Although there were clear duplication reduction and cost saving benefits to the original website summarisation work that this project evaluated, this project itself had its own clear benefits. An evaluation methodology would be the only way for the summarisation project to be seen as value adding and trusted; like a jeweller assessing the quality of a stone. If the work could be re-used again, it would not only save time in assuring the quality of other generative work but also help to continue establishing the NHS England Data Science Team as experts in this area. After all, it is experts who do evaluation.</p>"},{"location":"articles/2025/03/14/LLM-as-a-Judge-Engagement/#dont-forget-to-document-it","title":"Don\u2019t forget to document it","text":"<p>If you are already doing it, why not document it! The quality framework includes comms and marketing. We suggested to Will that he fill in the Comms and Marketing section to create a marketing plan and gave advice on how to do so, and what activities could be reasonable for it; for example Will had been \u201ckeeping an eye out for suitable chances to present work\u201d and that is a valid marketing activity.  We also suggested Will could write an article on his work (which he went and did!), share it on the website and cross-share on other platforms such as LinkedIn to reach a wider audience by being more discoverable. As of Feb 10th 2025 his post on LinkedIn had a respectable 750 impressions!</p>"},{"location":"articles/2025/03/14/LLM-as-a-Judge-Engagement/#key-exploitable-results","title":"Key Exploitable Results","text":"<p>We suggested Will add at least one Key Exploitable Result (KER) from his project to the KERs page; this is the place where you ensure that your results are known in the wider team and used beyond the scope of the project, for example by team leadership when scoping the creation of new work. The KER added was number 030; \u201cA code base for reproducibly evaluating summaries using LLM-as-a-Judge\u201d</p>"},{"location":"articles/2025/03/14/LLM-as-a-Judge-Engagement/#end-result-re-use","title":"End result \u2013 Re-use!","text":"<p>The end result of Will\u2019s efforts are that his project is know about in the wider team and beyond. This has even led to it being suggested for re-use in the Synthetic Data Generation project!</p>"},{"location":"articles/2025/03/14/LLM-as-a-Judge-Engagement/#wills-feedback-on-the-engagement","title":"Will\u2019s feedback on the engagement:","text":"<p>When we first asked you about filling in the marketing section of the quality framework, how apprehensive were you? - \"I didn't really think I was doing any marketing. I wasn't sure what to put in it; for example, I didn't recognise uploading code to GitHub as a form of marketing/comms\".</p> <p>How helpful did you find talking to the comms and marketing team? - \"incredibly helpful, went from not really knowing to understanding and recognising\".</p> <p>How long did it take you to fill in the quality framework pages and keep it up to date? - \"It was quite quick. It helped having spoken to Adam and Amaia. It wasn't arduous\".</p>"},{"location":"articles/2025/03/14/LLM-as-a-Judge-Engagement/#do-you-need-help-with-comms-and-marketing-or-advice-on-presenting","title":"Do you need help with Comms and Marketing, or advice on presenting?","text":"<p>Contact a member of the comms and marketing team (for example adam.hollings1@nhs.net or Amaia.imazblanco1@nhs.net) and we can have a chat on how we can help you! </p> <p>Note: Some links in this article link to the NHS England confluence and are not open to the general public.</p>"},{"location":"articles/2025/03/31/presentation-in-NHSE/","title":"Data Science Marketing Spotlight: Confident Presentation Secrets","text":"<p>\ud83d\udce2 Our spotlight today will be on Confident Presenting!</p> <p>Do you want to present as speakers like Steve Jobs, Ken Robinson, or Simon Sinek, who are known for their exceptional ability to captivate and engage their audience? Keep reading to discover the techniques they use to deliver impactful and memorable presentations, building on the techniques from our away day workshop. Also make sure to check out some of our other resources for presenting!</p> <p> </p>"},{"location":"articles/2025/03/31/presentation-in-NHSE/#confident-presenting-4-simple-presentation-secrets","title":"\ud83d\udca1 Confident Presenting: 4 Simple Presentation Secrets","text":"<p>Great presentations are built on how well you engage your audience. Here\u2019s how you can elevate your presentation game:</p> <ul> <li>\ud83d\udde3\ufe0f Engage Your Audience</li> </ul> <p>Involve your audience early by starting with a compelling story or question. Use interactive tools like real-time polls, quizzes (e.g., Menti), or word clouds to capture feedback instantly. Personalizing your content based on audience responses helps keep them engaged throughout. Something you may want to consider, to capture your audience is not starting with \u201cMy name is ... and I am here to present about...\u201d. Jump directly into an engaging hook about your content and then go back to introduce yourself later. </p> <ul> <li>\ud83c\udfa8 Clear and Visually Appealing Slides</li> </ul> <p>Keep your slides visually simple but impactful. Follow the \"10-20-30 rule\" \u2014 no more than 10 slides, keep the presentation under 20 minutes, and use a font size of at least 30. Use high-quality images and icons that support your message and avoid overwhelming your audience with too much text. It is also worth considering if the slides are just punchy images with no text are appropriate for your purpose as they are more memorable. </p> <ul> <li>\ud83d\udcaa Master Body Language and Voice</li> </ul> <p>Non-verbal communication is key. Maintain eye contact to create a connection and use hand gestures to emphasize key points. Vary your tone and pace to avoid monotony. Moving around the stage and pausing for effect can also keep your audience's attention on you, not just your slides. </p> <ul> <li>\u23f1\ufe0f Practice, Practice, Practice!</li> </ul> <p>Rehearse several times, ideally in front of a test audience. This helps you refine your delivery, smooth out transitions, and improve your timing. Anticipate possible technical issues (e.g., microphone failure) and have a backup plan.</p> <ul> <li>In Summary:</li> </ul> <p>To deliver a memorable and impactful presentation:</p> <p>\u2705 Engage your audience from the start</p> <p>\u2705 Keep your slides clear and simple</p> <p>\u2705 Use confident body language</p> <p>\u2705 Rehearse thoroughly</p> <p>I hope you have learnt some skills and would put it in practise when presenting and marketing your work.  To learn more, you could check out the slides from the DS Team's January Away Day Presentation Training here and this detailed blog.</p> <p>\ud83d\udce2 NHS England Data Science Team Comms and Marketing </p>"},{"location":"articles/2025/04/15/driving-responsible-data-science/","title":"Driving Responsible Data Science: A Case Study of NHS England's Data Science Team","text":"<p>This is a reproduction of the case study completed by the NHS England Data Science Team as a part of the Turing Experts in Residence programme. The original can be found at this website</p> <p>The NHS is built on values  of fairness, trust, compassion and safety. As artificial intelligence (AI) becomes a more prominent tool in healthcare \u2013 from automation of administrative processes to disease prediction and medical imaging analysis \u2013 NHS England\u2019s data science team is working to ensure these values remain central to technological advancement.</p> <p>This is much more than just a box-ticking exercise: it\u2019s about shaping best practices, embedding those practices in projects right from the start, and being as transparent and open as possible throughout the development lifecycle. That way, patients can have confidence that their (often sensitive) personal information is being used safely, fairly and responsibly, and that the innovative use of data-driven technologies including AI can add genuine value to services and activities, improving outcomes for the tens of millions of people who rely on NHS England to support their healthcare needs.</p> <p>Members of the NHS England data science team have been taking part in the 2024 cohort of The Turing Way Practitioners Hub\u2019s Experts in Residence programme, giving them a platform to discuss with other professionals the opportunities, challenges and best practices of AI development and adoption within a large public-sector organisation.</p> <p>This case study will explore some of the insights and ideas arising from those discussions, alongside a selection of NHS England\u2019s ongoing projects. Particularly, we put a spotlight on ethical considerations in data science, from design to monitoring and evaluation, ensuring responsible decision-making at all stages.</p>"},{"location":"articles/2025/04/15/driving-responsible-data-science/#ethics-in-ai-design-open-source-framework-of-data-hazards","title":"Ethics in AI Design: Open Source Framework of Data Hazards","text":"<p>We are all familiar with the symbols used to denote potentially harmful chemicals: fire for flammable, skull and crossbones for toxic, and so on. Misuse of data can also cause harm, which is why Dr Natalie Zelenka and Dr Nina Di Cara founded the community-driven, open source Data Hazards project in 2021. Data Hazards labels cover potential hazards such as automated decision-making, risk to privacy, and bias reinforcement. The project members, who have worked with The Turing Way, have now incorporated a chapter on this topic in The Turing Way Guide for Ethical Research.</p> <p>NHS England\u2019s data science team has been exploring the Data Hazards project as part of their own efforts to integrate structured, proactive, and accessible risk assessment into the team\u2019s data science practice. By making use of Data Hazards, NHS England builds on a community-developed and customisable framework. This open source solution saves time and constitutes a concrete approach to the ethical evaluation of projects across the team and potentially the wider organisation. This strand of work evolved from an internal \u2018white paper\u2019 authored by the team, exploring ethical data science &amp; AI  development in NHS England.</p> <p>Mia Noonan, Principal Data Scientist at NHS England and Practitioners Hub Expert in Residence, says: \u201cThe Data Hazards project is about developing a common vocabulary to help creators of AI products become aware of, think about and critically engage with the ethical risks involved with their projects. Because Data Hazards is open source, we are able to easily adapt it to our needs using NHS-specific examples. It\u2019s something that  people in many different roles, not just data scientists, can pick up and start using pretty quickly, and it\u2019s applicable in different contexts.\u201d</p> <p>To increase engagement with the Data Hazards labels and normalise conversations around ethics the team has adopted a cross-project, interactive \u2018show and tell\u2019 approach with project leads and has sought to evaluate the success of the labels\u2019 implementation through extensive feedback interviews. The feedback has been used to identify knowledge gaps in the team, for example, awareness of the environmental implications of their work, and led to work to create methods for environmental benchmarking to create a standardised approach for tracking CO2 emissions produced when training models.</p> <p>Harriet Sands, AI Technical Specialist at NHS England and also an Expert in Residence, adds: \u201cOne of the reasons we decided to join the Practitioners Hub is that we were embarking on testing the Data Hazard labels at NHS England and felt there would be value in bringing that discussion to the broader community. That\u2019s another benefit of the project being free and open source \u2013 we feel confident reaching out and having frank discussions about our experiences, which has been really helpful.\u201d</p>"},{"location":"articles/2025/04/15/driving-responsible-data-science/#ethics-in-ai-assurance-ai-quality-community-of-practice","title":"Ethics in AI Assurance: AI Quality Community of Practice","text":"<p>Ensuring AI systems in healthcare are safe, effective and ethically sound is a priority for NHS England. The AI Quality Community of Practice (AIQCOP) was established at NHS England to bring together technical assurers, data scientists and other stakeholders to explore collaboratively how AI assurance should be approached. Unlike traditional software assurance, AI presents unique challenges, including the need to evaluate not just whether a system functions as intended, but also its broader ethical and societal impacts, especially in healthcare\u2013 circling back to NHS principles of fairness, transparency and public trust.</p> <p>Ben Wallace, Principal Data Scientist at NHS England and a founding member of the AIQCOP, helped develop the initiative using insights from The Turing Way handbook on AI ethics and responsible use. While the Data Hazards framework ensures ethical considerations from the outset, quality assurance tools and methods integrate these principles into the \u2018embedosphere\u2019\u2014the ongoing processes of implementing, evaluating, and monitoring AI systems.</p> <p>Ben says: \u201cAIQCOP is about preparing for the reality in which AI is part of the systems we assure. We\u2019re building a community of people who will encounter AI in their work and making sure they have the knowledge, skills and tools to assess it properly and ask the right ethical questions alongside the technical validation.\u201d The AIQCOP community features a diverse range of voices and perspectives from across the organisation, fostering a dynamic, evolving approach to ongoing evaluation that is essential for AI assurance and which may be difficult to achieve through static compliance toolkits. In this way, ethical practice becomes more than simply a matter of adherence to existing standards.</p> <p>To support this work, NHS England is establishing internal good practice guidance for assurance. Ben adds: \u201cA big part of AIQCOP is recognising that traditional assurance methods don\u2019t always work for AI. We need new ways of thinking, new tools, and an understanding that AI assurance isn\u2019t just a checklist \u2013 it\u2019s an ongoing process.\u201d</p>"},{"location":"articles/2025/04/15/driving-responsible-data-science/#ethics-in-ai-monitoring-and-evaluation-an-approach-to-large-language-models-llms","title":"Ethics in AI Monitoring and Evaluation: An Approach to Large Language Models (LLMs)","text":"<p>Among the latest initiatives in the Data Science team at NHS England\u2019s work towards a holistic approach to AI is the ongoing work to establish an approach to monitoring and evaluation for LLMs. A set of questions and corresponding evidence standards for projects that involve LLMs to meet (dubbed the LLM evaluation &amp; monitoring framework), seeks to make sure LLMs are continued to be implemented and used safely.  </p> <p>This work aims to help those involved evaluate the LLM components of systems in NHS England, and potentially more widely in health and care. It is designed to aid teams in generating the necessary evidence to ensure continued quality and to complement compliance, business cases and decision-making processes (including when an AI tool may need to be updated or decommissioned). A draft version is being tested on an AI product in development, in collaboration with an NHS trust acting as an incubator for the product and other teams from different functions of NHS England.</p>"},{"location":"articles/2025/04/15/driving-responsible-data-science/#ethics-in-transparency-trialling-the-algorithmic-transparency-recording-standard-atrs","title":"Ethics in Transparency: Trialling the Algorithmic Transparency Recording Standard (ATRS)","text":"<p>The ATRS is a means by which public sector organisations can provide clear information about algorithmic tools they have made, or make use of, to support decision-making, ensuring transparency and accountability. The NHS was an early contributor to the Government\u2019s Algorithmic Transparency Recording Standard; publishing a record of the QCovid Algorithm in July 2022.</p> <p>More recently, the NHS Data Science Team has published an ATRS record of the NHS.UK Reviews Automoderation Tool. As documented in the record, this tool automates the moderation of thousands of reviews of NHS services received each year. The tool uses natural language processing (NLP) techniques to meet efficiency &amp; scalability needs while ensuring user safety. By openly sharing information about the algorithms that assist the delivery of public services, the record supports the mitigation of concerns the public may have about a so-called \u2018black-box\u2019 algorithm; ensuring that work is subject to scrutiny and helping to maintain the high standards of care and trust that patients and the public expect from the NHS.</p>"},{"location":"articles/2025/04/15/driving-responsible-data-science/#key-takeaways","title":"Key takeaways","text":"<ul> <li>NHS England's data science team is proactively integrating responsible AI practices into its work to ensure fairness, trust, and safety in healthcare.</li> <li>Leveraging open-source resources like Data Hazards and The Turing Way, the team is adapting community-developed solutions to their projects.</li> <li>Recognising the limitations of traditional assurance methods for AI, the AI Quality Community of Practice (AIQCOP) has been established to collaboratively explore effective AI assurance approaches, addressing the unique challenges posed by AI compared to traditional software.</li> <li>The Data Science Team is working on a practical process to assist in the evaluation and monitoring of Large Language Models in products.</li> <li>The NHS.UK Reviews Automoderation Tool ATRS record is an exemplar of the team\u2019s commitment to transparency and openness.</li> </ul>"},{"location":"articles/2025/04/15/driving-responsible-data-science/#expert-in-residence-spotlight-harriet-sands-mia-noonan","title":"Expert in Residence Spotlight: Harriet Sands &amp; Mia Noonan","text":"<p>Harriet Sands is an AI Technical Specialist in the Joint Digital Policy Unit at NHS England, with a career in Data Science that includes roles at HM Treasury, 10 Downing Street, the Office for National Statistics (ONS), and dunnhumby (with a stint as a Data Science for Social Good fellow). Harriet's focus is on practically realising Responsible AI at NHS England, creating strategies to help design, evaluate, and deploy AI solutions ethically. She is also the co-founder and coordinator of a public sector reading group on the societal implications of data-driven technologies.</p> <p>Mia Noonan joined NHS England as a graduate in 2021 after earning her MSc in Computer Science. Over the past three years, Mia has worked on projects involving data quality, regression, and data linkage. She focuses on operationalizing data science solutions, ensuring tools are deployed effectively and have meaningful impacts. Mia has been instrumental in implementing quality assurance frameworks for data science teams.</p> <p>Highlights from The Turing Way Practitioners Hub</p> <p>\u201cHaving access to The Turing Way and other open initiatives is really helpful for us because we can pick them up and work with them without feeling like we have to create our own resources from scratch. It\u2019s a great way for us to experiment with what the research community is coming up with. We\u2019ve also found having access to an open community of peers through the Practitioners Hub to be really beneficial in terms of discussing shared experiences and challenges.\u201d</p>"},{"location":"articles/2025/04/15/driving-responsible-data-science/#acknowledgements","title":"Acknowledgements","text":"<p>This case study is published under The Turing Way Practitioners Hub 2024-25 Cohort - case study series. The Practitioners Hub is The Turing Way project that works with experts from partnering organisations to promote data science best practices. In 2024, Harriet Sands and Mia Noonan jined the cohort as Experts in Residence to represent interests and opportunities to discuss, adopt and share their implementation approaches for data science and AI in the NHS England Data Science Team and Healthcare. </p> <p>This project is supported by Innovate UK BridgeAI. The Practitioners Hub has also received funding and support from the Ecosystem Leadership Award under the EPSRC Grant EP/X03870X/1 &amp; The Alan Turing Institute.</p> <p>Cite this publication</p> <p>Noonan, M., Sands, H., Wallace, B., Gillespie, S., Sharan, M., Bennett, A., &amp; Demertzi, L. (2025). Driving Responsible Data Science: A Case Study of NHS England's Data Science Team. Zenodo. https://doi.org/10.5281/zenodo.15053194</p>"},{"location":"articles/2025/04/16/interview_with_joseph_wilson/","title":"Interview with a Data Scientist - The Aerospace Engineer who Loved Spreadsheets","text":"<p>Welcome to the first installment of our \u201cInterview with a Data Scientist\u201d series, where we explore the careers and work of the talented members of the NHS England Data Science team. We aim to showcase the fantastic individuals who contribute to the NHS England Data Science Profession and provide valuable insights for those considering a career in Data Science within the healthcare sector.</p> <p>The first interview is with Joe Wilson, a former Engineer whose desire to make the perfect F1 Fantasy Team during the COVID pandemic drove him to become a Data Scientist who won\u2019t stop talking about Reproducible Analytical Pipelines.</p> <p>This interview orginally was published in the March edition of the Data Science Community for Health and Care Newsletter. You can subscribe and join the community here.</p> <p> </p>"},{"location":"articles/2025/04/16/interview_with_joseph_wilson/#how-did-you-end-up-in-data-science-at-the-nhs-what-did-you-do-before-and-what-really-sparked-your-interest-in-this-field","title":"How did you end up in data science at the NHS? What did you do before, and what really sparked your interest in this field?","text":"<p>My journey to data science at the NHS is a blend of engineering roots, a pandemic pivot, and a strong personal connection to healthcare.</p> <p>My academic background began with an Integrated Master's in Aerospace Engineering. I was captivated by the problem-solving inherent in understanding how complex systems, particularly aircraft, function. This fascination with engineering persists, as evidenced by my continued interest in aerospace content.</p> <p>A pivotal moment that hinted at my future direction occurred during a second-year Business Simulation module. We were tasked with managing car companies within a simulated European market. I developed a comprehensive spreadsheet to optimise production, enabling us to create cost-effective cars with popular features. I would go on to make many more spreadsheets, including one that calculated my mark margins for upcoming exams to allow me to still graduate with first class honours.</p> <p>Following graduation, I joined a small engineering firm, where I contributed to the development of control software for automated test rigs. We built custom rigs to test components like aircraft actuators and truck brake assemblies, using the visual programming language LabVIEW. This involved wiring code sections together, and I still recognise LabVIEW's distinctive interface in various applications, including airport airbridge controls.</p> <p>Unfortunately, I lost my job around the onset of the COVID-19 pandemic, coinciding with a severe contraction of the job market. During the lockdown, I used the time to learn Python, which sparked a realisation that I wanted to pursue a career in data science. While I'd always enjoyed numbers, spreadsheets, problem-solving, and programming, the pandemic provided the catalyst to connect these interests.</p> <p>I enrolled in a Data Science conversion Master's program at Loughborough University, joining the inaugural cohort of the degree. The decision to apply to the NHS was deeply personal. My father was a GP and healthcare consultant, and my then-girlfriend (now wife), was training to be a doctor. The NHS's mission resonated with me, and I discovered the NHS Digital Graduate Scheme. I applied and was fortunate enough to be accepted.</p> <p>I began my role at the NHS just two weeks after submitting my Master's project.</p>"},{"location":"articles/2025/04/16/interview_with_joseph_wilson/#once-you-joined-the-nhs-what-was-that-experience-like-what-different-roles-and-teams-have-you-been-a-part-of-and-how-have-they-shaped-your-career","title":"Once you joined the NHS, what was that experience like? What different roles and teams have you been a part of, and how have they shaped your career?","text":"<p>Joining the NHS was a real whirlwind, but in the best way possible! I've been here for three years now, starting at NHS Digital and then transitioning into NHS England after the merger. My first two years were spent on the NHS Digital Graduate Scheme, and since then, I've been working as a full-fledged Data Scientist.</p> <p>That Graduate Scheme was incredible. We were the first cohort of its kind, a group of just under twenty, and we kicked things off with an intense eight-week coding bootcamp. It was a fantastic bonding experience \u2013 we'd be hammering away at code and then decompressing with board games during breaks.</p> <p>My first placement on the NHS Spine project, which is this massive, intricate system connecting all sorts of different NHS systems. Because it's so complex, everyone was constantly learning, and that made it a really supportive environment for a newbie like me. Plus, I got to see firsthand what a well-oiled DevOps team looked like, which really shaped my approach moving forward, especially when I landed in the Business Intelligence Development Team.</p> <p>That team was all about building Tableau dashboards and trying to standardise things. My manager asked me to dig into the Tableau API. They had this super slow, manual process for checking dashboards, and I realised we could automate it with Python. So, I built a proof-of-concept testing framework, which was a great chance to put my NHS Spine lessons to work in a totally different area.</p> <p>My final placement was with the Reproducible Analytical Pipelines (RAP) Squad. We were all about promoting good coding practices for analytics and helping teams automate their workflows. Basically, we were bringing in the best of data and software engineering into the data science and analytics space. In that role, I got to write articles, create training guides, and even run workshops. We also did these 'Engagements' with other analytical teams, helping them improve their processes. After a year, I passed my end-of-scheme interview and got promoted to a Band 7 Data Scientist, staying with the RAP Squad and leading those Engagements.</p> <p>More recently, I moved to the National Secure Data Environment Service Team, or the SDE Data Wranglers. We make sure researchers get the right, pseudonymised data. While there, I also got to work on a side project developing an ML-powered tool to detect potentially sensitive information in huge datasets. It was a fun challenge, working with Docker containers, PySpark, and figuring out how to deploy ML models in a clustered computing environment.</p> <p>Overall, it's been a really varied and rewarding experience. Each role has built on the last, and I've learned so much about the NHS and the power of data.</p>"},{"location":"articles/2025/04/16/interview_with_joseph_wilson/#what-are-you-currently-working-on-are-there-any-projects-that-youre-particularly-excited-about-or-that-you-feel-are-making-a-real-difference-what-impact-are-you-having","title":"What are you currently working on? Are there any projects that you're particularly excited about, or that you feel are making a real difference? What impact are you having?","text":"<p>Right now, I'm embedded within an analytical team focused on commissioning specialised NHS services. It's a really interesting area, and I'm helping them boost their Reproducible Analytical Pipelines (RAP) compliance, automate some of their manual processes, and bring in some Python expertise to diversify their toolkit.</p> <p>One project I'm particularly excited about is automating their monthly reporting. We've managed to cut the processing time from a week down to just a few minutes, which is a huge win! It's incredibly rewarding to free up my colleagues' time, allowing them to focus on more impactful, data-driven insights rather than repetitive tasks.</p> <p>I'm also personally driven to achieve something new for me; building a pipeline that meets Gold RAP compliance standard; essentially, a fully automated and packaged analytical process. We're facing some interesting challenges, but I'm confident that overcoming them will pave the way for more efficient and robust projects in the future.</p>"},{"location":"articles/2025/04/16/interview_with_joseph_wilson/#if-you-could-give-someone-just-starting-out-in-data-science-a-few-pieces-of-advice-what-would-they-be-and-what-resources-have-you-found-particularly-helpful-along-the-way-that-you-can-share","title":"If you could give someone just starting out in data science a few pieces of advice, what would they be? And what resources have you found particularly helpful along the way that you can share?","text":"<p>If I could give some advice to someone starting out in data science, I'd say:</p> <ul> <li>Consider a conversion master's if it's feasible. Mine was a game-changer. It accelerated my learning so much, and it gave me a strong portfolio to show potential employers right away.</li> <li>Dive into online resources, but don't get lost in them. There's a wealth of information out there! Kaggle Learn is a fantastic starting point.</li> <li>Master Git and GitHub. Seriously, this is crucial. I even helped create a workshop for the NHS-R Community and some training materials on it, which you can find on the RAP Community of Practice website. Learning Git and GitHub will also allow you to use GitHub codespaces, and avoid the pain of setting up your local environment.</li> <li>Don't just stick to tutorials. It's easy to get stuck in tutorial hell. Find a simple, personal project that excites you. Mine was building a tool to optimise my Fantasy F1 team. Passion projects are a great way to build a portfolio and showcase your skills.</li> </ul> <p>Basically, combine structured learning with practical application, and don't be afraid to explore your own interests!</p>"},{"location":"articles/2025/04/30/interview_with_sarah_culkin/","title":"Interview with a Data Scientist - Blowing the Trumpet for Data Science","text":"<p>Welcome to the second installment of our \u201cInterview with a Data Scientist\u201d series, where we explore the careers and work of the talented members of the NHS England Data Science team. We aim to showcase the fantastic individuals who contribute to the NHS England Data Science Profession and provide valuable insights for those considering a career in Data Science within the healthcare sector.</p> <p>This week, we have the pleasure of hearing from Sarah Culkin, the Deputy Director of the Data Science team. Leveraging her experience in both health and data, Sarah is leading the charge to support the digital transformation of the NHS and champion the growth of data science as a vital profession across the organisation.</p> <p>This interview orginally was published in the April edition of the Data Science Community for Health and Care Newsletter. You can subscribe and join the community here.</p> <p></p>"},{"location":"articles/2025/04/30/interview_with_sarah_culkin/#how-did-you-end-up-in-data-science-at-the-nhs-what-did-you-do-before-and-what-really-sparked-your-interest-in-this-field","title":"How did you end up in data science at the NHS? What did you do before, and what really sparked your interest in this field?","text":"<p>I studied chemistry at the University of Leeds; after my undergraduate degree, I stayed on to pursue a PhD in organic chemistry.</p> <p>My research focused on peptides and proteins, which meant a significant amount of time in the lab, and more time than I\u2019d like to recall spent washing glassware! However, a turning point came when the department acquired a robot to run experiments. This was my first real immersion in programming and data handling.</p> <p>Towards the end of my PhD, I came across a job advertisement for the Government Operational Research Service (GORS), a service often considered a precursor to modern data science. I applied and was placed within the Department of Health.</p> <p>I\u2019ve remained within the health sector and the realm of data ever since, for about 17 years now, although I\u2019ve had the opportunity to work in many different areas.</p> <p>My passion for data science truly ignited around 2016, and I took the initiative to establish a small data science team within the Department of Health. Since then, I\u2019ve held a variety of roles across the Department of Health and Social Care (DHSC), NHS England, and NHSX. I also spent some valuable time at Leeds Teaching Hospitals Trust and in a data policy role. Ultimately, though, leading a data science team is where my true passion lies.</p>"},{"location":"articles/2025/04/30/interview_with_sarah_culkin/#what-are-you-currently-working-on-are-there-any-projects-that-youre-particularly-excited-about-or-that-you-feel-are-making-a-real-difference-what-impact-are-you-having","title":"What are you currently working on? Are there any projects that you're particularly excited about, or that you feel are making a real difference? What impact are you having?","text":"<p>As Deputy Director of the Data Science team at NHS England, my focus has shifted from direct project involvement to leading the team and shaping its strategic direction. While I miss the day-to-day project work, I now dedicate my time to raising the team's profile and securing new opportunities for us to contribute.</p> <p>Two current areas of work particularly excite me. Firstly, the ongoing development of data science as a recognised profession within the NHS.</p> <p>For example, our PhD intern programme, which connects the NHS with early-career researchers in academia, has been running successfully for several years (more information can be found here).  This initiative brings fresh perspectives and skills into the NHS.  </p> <p>We also now oversee a Data Science Masters programme, a fantastic opportunity to provide advanced training to our colleagues, enhancing their capabilities and the overall data science capacity within the NHS.</p> <p>I was also involved in developing the data science competency frameworks and, more recently, a continuous professional development policy. I firmly believe that having a clear, fair, well-defined, and well-promoted profession is crucial for attracting and retaining talent and ensuring high standards.</p> <p>More recently, I've had the rewarding experience of engaging in school outreach. The sessions at primary schools were particularly memorable; the children's questions are often wonderfully wild, funny, and insightful!</p> <p>The second area that truly excites me is shaping the new projects coming into the team. I'm particularly keen to see how data science can effectively support the digital transformation initiatives within the NHS, as well as inform crucial operational decisions.</p> <p>It's fascinating to observe how different teams operate and identify how data science can provide valuable assistance. I play a key role in scoping these projects to ensure they not only meet but potentially exceed the initial requirements.</p> <p>Increasingly, we're seeing colleagues interested in leveraging the power of AI in their work, which brings exciting new challenges, opportunities, and collaborations for our team.</p>"},{"location":"articles/2025/04/30/interview_with_sarah_culkin/#if-you-could-give-someone-just-starting-out-in-data-science-a-few-pieces-of-advice-what-would-they-be-and-what-resources-have-you-found-particularly-helpful-along-the-way-that-you-can-share","title":"If you could give someone just starting out in data science a few pieces of advice, what would they be? And what resources have you found particularly helpful along the way that you can share?","text":"<p>My advice for aspiring data scientists would be two things:</p> <p>Firstly, cultivate the ability to explain complex, technical, and potentially dry subjects in a simple and engaging manner. Think to yourself, 'Can I explain what I'm doing to a 9-year-old in a way that they would not only understand but also find captivating?' Channel your inner kids' TV presenter or science journalist!</p> <p>A great way to achieve this is by using everyday analogies. Try comparing a project, process, or problem to a familiar, everyday situation. Recently, when explaining technical concepts, I've used examples like visiting a restaurant or using an iPhone.</p> <p>If you can help people, feel intelligent (rather than prioritising making yourself appear intelligent!), they are far more likely to engage with and remember what you're trying to convey.</p> <p>It's natural, after completing a complex piece of work, to want to highlight its difficulty and intricacy. While sometimes this is necessary, generally, strive to find common ground and make the concepts accessible.</p> <p>Secondly, ensure you have a solid grasp of the fundamentals of the data science technique or approach you're using, including its potential weaknesses as well as its strengths compared to other methods.</p> <p>In an age where we can execute complicated processes and code with a few clicks, it's not always mandatory to delve into the underlying statistical models. However, to build your own confidence and effectively explain your work to others, this understanding of the fundamentals is, for me, paramount. It also equips you to critically evaluate the next 'new big thing' that emerges. Aim to be a cynical optimist in your approach. </p> <p>We hope you found this interview with Sarah Culkin insightful. Her journey and advice offer valuable perspectives for those interested in contributing to the growing field of data science within the NHS. For more information about the NHS England Data Science Profession, please visit our website. Here you can also find our first \u2018Interview with a Data Scientist\u2019 with Joe Wilson, The Aerospace Engineer who Loved Spreadsheets.</p>"},{"location":"articles/2025/06/12/interview_with_adam_hollings/","title":"Interview with a Data Scientist - Dimsum and Datascience","text":"<p>Welcome to another installment of our \"Interview with a Data Scientist\" series, where we explore the careers and work of the talented members of the NHS England Data Science team. We aim to showcase the fantastic individuals who contribute to the NHSE Data Science Profession and provide valuable insights for those considering a career in Data Science within the healthcare sector.</p> <p>This week our interviewee is Adam Hollings a Principal Data Scientist in the Data Science Team who recently moved from the SDE Service Team to the Central Data Science Team.</p> <p>This interview orginally was published in the May edition of the Data Science Community for Health and Care Newsletter. You can subscribe and join the community here.</p> <p></p>"},{"location":"articles/2025/06/12/interview_with_adam_hollings/#how-did-you-end-up-in-data-science-at-the-nhs-what-did-you-do-before-and-what-really-sparked-your-interest-in-this-field","title":"How did you end up in data science at the NHS? What did you do before, and what really sparked your interest in this field?","text":"<p>Fig 1. What I imagined my career journey to be like and what it actually ended up being like.</p> <p>My career journey has been quite a winding path, much like a plate of spaghetti, with significant detours between the UK and China. Along the way, I've held various roles, including a call centre agent, claims handler, NHS Analyst, University Teacher, school owner, house husband, NHS Higher Analyst, and now, an NHS Data Scientist.</p> <p>It's been a rather circuitous route, but I'll do my best to capture the key details! I always enjoyed science at school and had a keen interest in anime, which led me to pursue Biology and Japanese at the University of Sheffield. After graduating, I spent a brief period working as an analyst in the NHS \u2013 a path my twin brother encouraged me to explore. However, most of my time was spent teaching in various educational institutions in rural China, with a year dedicated to learning Chinese in a cement factory and another as a house husband.</p> <p>Living in China was an incredibly enriching experience. I gained invaluable skills in public speaking and learned how to explain complex concepts in English to non-experts and non-native speakers. Immersing myself in a completely different language and culture, and experiencing life as a minority in a vast country, taught me a great deal about resilience, adaptability, and cultural understanding \u2013 both the highs and the lows. I also developed a strong tolerance for very spicy food and hot weather!</p> <p>After about seven years in China, I was ready for a new challenge. I'd been observing my brother's career at NHS Digital and found myself increasingly drawn to the work he was doing. This prompted me to spend about a year self-learning programming, which ultimately led to me securing a job at NHS Digital in 2020.</p> <p>I thrive on challenges and naturally approach problems with a data-driven, analytical mindset. While my brother's introduction initially opened the door to the NHS, once I saw the potential and understood the impact I could have, I knew it was the right place for me.</p>"},{"location":"articles/2025/06/12/interview_with_adam_hollings/#once-you-joined-the-nhs-what-was-that-experience-like-what-different-roles-and-teams-have-you-been-a-part-of-and-how-have-they-shaped-your-career","title":"Once you joined the NHS, what was that experience like? What different roles and teams have you been a part of, and how have they shaped your career?","text":"<p>I've now worked in the NHS for over five years. My initial roles were as a contingent labour data analyst within the publications teams, primarily focusing on maternity and IAPT (Talking Therapies) data. I then successfully secured a permanent position and subsequently applied for and completed the Data Science MRes programme through NHS Digital at Leeds University.</p> <p>It was the MRes programme itself that truly ignited my passion for data science. The rapid pace of developments in machine learning, the critical questioning of established methods and techniques, and the constant exploration of how cutting-edge innovations could directly benefit patients \u2013 it was exhilarating. I knew I wanted to be at the forefront of this field.</p> <p>Following the MRes, I took on a role as a Data Wrangler in the NHS England Secure Data Environment (SDE) Service Team. I spent several fulfilling years there, working alongside a fantastic team, enabling numerous analysts and researchers to conduct vital research on vast amounts of de-identified NHS data. This position provided me with invaluable experience collaborating with high-profile stakeholders like the Department of Health and Social Care (DHSC) and Health Data Research UK (HDR UK). My previous teaching experience proved incredibly beneficial when it came to inducting and advising new users of the SDE. My time with the SDE service team also allowed me to build connections with members of the Central Data Science Team, which ultimately encouraged me to seek future opportunities within NHS England.</p> <p>Eventually, I wanted to transition from enabling others' data science work to having a more direct, hands-on role myself. So, this year, I embarked on an exciting new chapter within the Central Data Science Team.</p>"},{"location":"articles/2025/06/12/interview_with_adam_hollings/#what-are-you-currently-working-on-are-there-any-projects-that-youre-particularly-excited-about-or-that-you-feel-are-making-a-real-difference-what-impact-are-you-having","title":"What are you currently working on? Are there any projects that you're particularly excited about, or that you feel are making a real difference? What impact are you having?","text":"<p>I've recently joined the A&amp;E Forecasting project, which was recently nominated as an AI Exemplar. The ability to accurately forecast A&amp;E attendances and admissions has the potential to significantly aid planning across almost every hospital in the country. Knowing that I'll be contributing to maintaining and improving this crucial model is incredibly exciting. My current focus is on enhancing how weather data is incorporated into the model. It's been fascinating to delve deeper into using the Unified Data Access Layer (UDAL) and learning the intricacies of the Federated Data Platform (FDP), as most of my recent years were spent working with the Data Access Environment (DAE) and the SDE. After learning Chinese and Japanese, I'm certainly not afraid of being thrown into new situations and having to acquire new skills!</p> <p>Beyond this, I also have a keen interest in open-source large language models (LLMs) and have recently been appointed as the team's technical lead in this area. Additionally, I'm deeply invested in causal inference methodology, understanding the potentially disastrous consequences of improperly applied causal analysis.</p> <p>I'm also committed to ensuring our projects achieve the greatest possible impact by actively supporting the Marketing and Communications function team. Given the current organisational landscape, I believe it's especially critical to ensure that our work and its significant benefits are well-understood across both NHS England and the DHSC. Recent Marketing and Comms initiatives I've contributed to include setting up the Data Science Team's SharePoint page, gathering and submitting entries for the Advanced Analytics Analytical Prospectus, and currently working on the Data Science Brochure.</p> <p>If you're looking to enhance the reach and impact of your project, please don't hesitate to get in touch with myself, Amaia or Will!</p>"},{"location":"articles/2025/06/12/interview_with_adam_hollings/#if-you-could-give-someone-just-starting-out-in-data-science-a-few-pieces-of-advice-what-would-they-be-and-what-resources-have-you-found-particularly-helpful-along-the-way-that-you-can-share","title":"If you could give someone just starting out in data science a few pieces of advice, what would they be? And what resources have you found particularly helpful along the way that you can share?","text":"<p>While I consider myself a dedicated team player, I would advise new members to be proactive, honest, and strategically self-aware. By \"strategically self-aware,\" I mean that when you're asked about project preferences or which function team you'd like to join, choose what genuinely aligns with and benefits your career trajectory. Be transparent with your manager about any challenges or if a long-term project isn't personally contributing to your growth. Project allocations are based on both business need and individual preference; leadership can't account for your preferences if you're not open about them.</p> <p>When I say \"be proactive,\" I mean actively participate: ask questions in meetings, confidently present your work, and ensure people are aware of who you are, the impactful things you're working on, and your accomplishments. This could involve consistently marketing your work! It's perfectly fine to let your personality shine through \u2013 I always look forward to Warren's weekly meme post, and if even one person in the team gets a laugh from my regular Teams background rotations, it's worthwhile. These small personal touches help endear people to you, make you memorable, and significantly build team spirit.</p> <p>We hope you found this interview with Adam Hollings inspiring. His journey shows the diverse paths you can take to become a Data Scientist in the NHS.</p> <p>You can also view our previous \u2018Interview with a Data Scientist\u2019 with Sarah Culkin, Blowing the Trumpet for Data Science.</p>"},{"location":"articles/2025/06/17/NQCCHack/","title":"NHS England Use-Cases at the National Quantum Computing Centre annual Hackathon","text":"<p>Quantum computing has the potential to dramatically change many aspects of healthcare analysis.   For our interests this mostly features around increased efficiency in optimisation and hybrid ML-Quantum approaches.  This article briefly describes our use-cases submissions to the annual NQCC Hackathon to showcase the emerging interests for healthcare using this technology.</p>"},{"location":"articles/2025/06/17/NQCCHack/#introduction","title":"Introduction","text":"<p>The National Quantum Computing Centre (NQCC) - UK National Laboratory - was launched in 2020 spanning 5 years with a 93 million pound investment from the Uk Government as part of the national quantum technologies program.   </p> <p>The NQCC\u2019s annual UK Quantum Hackathon aims to demonstrate how quantum computing can solve real-world problems by bringing together talented people from across the value chain, with the objectives to:</p> <ul> <li>Connect the UK quantum ecosystem through networking and collaboration throughout the event</li> <li>Understand the opportunities and limitations of quantum computing by demonstrating the current capabilities of the technology</li> <li>Explore a broad range of use cases for quantum computing</li> <li>Enable aspiring practitioners to develop their quantum computing programming and applications skills</li> <li>Showcase the breadth of technology available currently and enable a broad range of users to access these tools</li> </ul> <p>Our team has been engaging with this event for the past few years through the submission of use-cases in order to connect and understand the possible roles of quantum computing in healthcare. </p>"},{"location":"articles/2025/06/17/NQCCHack/#first-quantum-hackathon-in-2022-bed-optimisation","title":"First Quantum Hackathon in 2022 - Bed Optimisation","text":"<p>At the first of these hackathons in 2022, we challenged the hackers to devise a strategy for allocating patients to beds whilst taking account of various individual and system constraints.  Currently, this is an extremely complex problem that is usually worked out by people on the ground with a lot of domain knowledge.   See the full event article here. </p>"},{"location":"articles/2025/06/17/NQCCHack/#second-quantum-hackathon-in-2023-route-optimisation","title":"Second Quantum Hackathon in 2023 - Route Optimisation","text":"<p>The following year, we continued on the theme of optimisation but this time proposed a route optimisation problem around the transportation of both staff and time-sensitive materials to multiple locations.  The computation of optimal routes is of value and a common classical problem.   Owing to the nature of routing and scheduling problems being NP-complete, the application of quantum computing for problem solving in a healthcare setting is an exciting proposition.  </p> <p>To make the problem tractable, the team used a hybrid method that combines classical compute for clustering (k-means-constrained) with quantum annealing. The clustering technique focuses on finding non-overlapping clusters, constraining the search space, and making the solution viable for existing hardware and at scale. </p> <p>Several clustering methods were attempted, and k-means-constrained proved most useful. Hospitals were clustered before using D-Wave to provide solutions for the optimal route based on the shortest path between clusters. The team had a limited number of qubits to use but were still capable of finding solutions for up to five health centres. See the full report here.</p>"},{"location":"articles/2025/06/17/NQCCHack/#third-quantum-hackathon-in-2024-ae-forecasting","title":"Third Quantum Hackathon in 2024 - A&amp;E Forecasting","text":"<p>In 2024, we proposed quantum modelling for NHS forecasting as the use-case.  This built upon our current A&amp;E forecasting tool which uses a Hierarchical Bayesian model trained using a particular kind of Markov-chain Monte Carlo (MCMC) algorithm.  Whilst this approach allows information to be shared between hospitals during training (helping improve quality and missingness of the data), this is computationally expensive.   By incorporating quantum random sampling algorithms into the model it was hoped that better training speed and efficiency could be achieved as well as increasing performance and reducing costs. A quantum-MCMC was attempted but proved to be unsuitable.  Instead a variational quantum circuit was implemented for the time-series forecasting with linear trends in the data detected using just 25 qubits.  See the full report here.</p>"},{"location":"articles/2025/06/17/NQCCHack/#whats-next","title":"What's next?","text":"<p>We're aiming to submit a fourth use-case for the 2025 hack taking place in July.  Watch this space!.</p> <p>The 2025 hackathon details can be found here.  </p>"},{"location":"articles/2025/06/27/hsma-waiting-lists/","title":"HSMA Course Project: Waiting List Data Science Proofs of Concept","text":"<p>Sean and Amaia have spent the last 15 months on the Health Service Modelling Associates (HSMA) Programme, which is soon coming to an end. This program consists of 6 months of weekly lectures on a range of data science, operational research, coding, and web development, followed by a 9 month project, run by the University of Exeter PenChord team and is available for free for anyone working in public healthcare or policing.</p> <p>During the course, we opted for the project to be a set of proof of concepts using the Waiting List Minimum Data set (WLMDS) to identify data science techniques that may provide novel insights into this dataset. Read more about the project itself here. This blog post is a summary of our project, as well a reflection on the past 15 months of the course.</p> <p></p>"},{"location":"articles/2025/06/27/hsma-waiting-lists/#why-did-you-decide-to-attend-the-hsma-programme","title":"Why did you decide to attend the HSMA programme?","text":"<p>Amaia: Having joined the data science team at the end of my NHS Digital graduate scheme, I'd ended up getting stuck into my projects directly, and focusing on the skills needed for my specific projects, which have been the Probabilistic Data Linkage Model and Developing Artificial Primary Care Records  and so I came to the realisation that my wider data science knowledge and formal training was quite lacking, and I wanted to get insight into a wider range of techniques.</p> <p>Sean: In my role within the data science team, I\u2019ve mostly focused on data pipeline development and data linkage, particularly on the CVD Pathways project. I wanted to improve my ability to generate actionable insights from data, not just process and transform it. The HSMA course also looked like a great way to gain formal training in a broader range of data science and analytical techniques, and to explore topics I don\u2019t encounter much in my day-to-day work, like geographical modelling and discrete event simulation.</p>"},{"location":"articles/2025/06/27/hsma-waiting-lists/#how-did-you-find-the-course","title":"How did you find the course?","text":"<p>Sean: Overall, I found the course really valuable. The first section focused on Python coding skills, which was a helpful refresher. Once the course moved into more advanced areas, such as machine learning and event simulation, I found it really engaging and it provided a great overview of what these tools are and how they can be used. Having this formal background and structured walkthrough has definitely improved both my knowledge and my confidence in applying the right data science solutions to different problems. The project phase was particularly interesting! I appreciated and enjoyed the opportunity to delve into a problem that was outside of my normal project work, and apply techniques we had learnt from the taught part of the course. I also want to give a big shout out to Dan and Sammi who organise and run the course - their help has been invaluable, especially when it came to asking questions and trying to understand the technical concepts.</p> <p>Amaia: Whilst the beginning of the course started quite slow, as it's designed to teach clinicians or other non technical staff how to code in Python, it quickly ramped up and I found myself learning about such a massive range of techniques every single week. I now feel like I have a comprehensive set of notes I can refer to if I'm ever in doubt about which technique is most appropriate, or how to improve a particular model. The project phase I found really challenging, particularly at the beginning, because I really struggled to figure out how to come up with a project that would be impactful, reusable, and allow me to practice as many of my new skills as possible. However, once we found our feet, I really enjoyed being able to apply my skills, and getting that committed time each week to work on it. Dan and Sammi who ran the course are also great, and always willing to help if you got stuck!</p>"},{"location":"articles/2025/06/27/hsma-waiting-lists/#who-would-you-recommend-the-course-to","title":"Who would you recommend the course to?","text":"<p>Amaia: Whilst I feel like I gained a lot from being on the course, observing my course mates, those that I think found the most value from it were technically savvy clinicians, who hadn't necessarily coded before, but had some technical knowledge, and I think those are the people who stand to gain the most from this opportunity!</p> <p>Sean: I\u2019d recommend this course to anyone without a formal data science background, or anyone coming from a different field, such as clinical work. There\u2019s a lot of content, but the course builds a solid foundation and is a great starting point for using data science more effectively in your work.</p>"},{"location":"articles/2025/06/27/hsma-waiting-lists/#what-was-your-favorite-part-of-the-course","title":"What was your favorite part of the course?","text":"<p>Amaia: I absolutely loved going into the nitty gritty of Discrete Event Simulations, as it's not something I had much knowledge on at all, and I feel like I learnt such a huge amount. I also really enjoyed the web development (streamlit) bits of the course as I'd never properly used it, and I can see it being applicable to so many of my projects.</p> <p>Sean: I really enjoyed learning about geographical modelling and deploying web apps with Streamlit. Understanding how to use geographic data to explore things like patient journey times or the closest clinic locations gave me lots of ideas to apply in my own projects. Learning Streamlit was great too, and I can see its potential for sharing insights quickly and making information more accessible to others.</p>"},{"location":"articles/2025/06/27/hsma-waiting-lists/#whats-something-that-youve-learned-that-will-feed-into-your-day-job-or-projects","title":"What's something that you've learned that will feed into your day job or projects?","text":"<p>Sean: Two things stand out. First, I\u2019ve learned a lot about how to choose/select analytical or machine learning approaches for a specific question, which I plan to use in future work on the CVD Pathways project (especially for understanding and predicting patient outcomes). Second, I\u2019ve learned a lot about scoping and defining analytical problems, which is a valuable skill for my current role and for projects down the line. It\u2019s really helped me understand the process of turning a stakeholder question of \u201cwe want to see this\u201d into an actionable, data science plan.</p> <p>Amaia: This is actually something that's already happened! I have used streamlit for showcasing some of the work on Developing Artificial Primary Care Records to colleagues and stakeholders with great success. I am also hoping that the work Sean and I did on WLMDS can be used by the Elective Recovery team to feed into future work, as they expressed wanting to implement data science techniques in the past but never having the capacity to. Our codebase should offer a reusable solution for a range of options of modelling, which should speed their process up. Through working with Sean, who despite being in my team I had never worked on a project with, I learnt a lot about object-oriented programming which I will carry forward, and this is an opportunity I would not have had if I had not done HSMA.</p> <p>Check out the HSMA website for more information, and look out for the announcement of next years course (which they think will start in about April 2026).</p>"},{"location":"articles/2025/07/03/interview_with_aib/","title":"Interview with a Data Scientist - The Data Scientist Who Just Wanted to Make Plots","text":"<p>Welcome to another insightful installment of our \"Interview with a Data Scientist\" series. Here, we delve into the diverse career paths and impactful work of the talented individuals who make up the NHS England Data Science team. Our goal is to shine a light on the fantastic people driving the NHS Data Science Profession forward and to offer valuable perspectives for anyone considering a career in healthcare data science.</p> <p>This week our interviewee is Amaia Imaz Blanco, a Data Scientist in the Data Science Team whose love of making graphs led her to the NHS Digital Graduate Scheme and eventually into probabilistic data linkage!</p> <p></p>"},{"location":"articles/2025/07/03/interview_with_aib/#how-did-you-end-up-in-data-science-at-the-nhs-what-did-you-do-before-and-what-really-sparked-your-interest-in-this-field","title":"How did you end up in data science at the NHS? What did you do before, and what really sparked your interest in this field?","text":"<p>I wouldn\u2019t say I had a grand plan to get into data science \u2014 but looking back, it probably isn\u2019t a huge surprise. When I was asked what I wanted to be at age 11 I said \u201ca maths teacher\u201d, which felt a bit out of step with the dreams of astronauts and footballers around me. That early love for numbers truly set the stage for my academic and professional life.</p> <p>Fast forward to being 16, and I stumbled upon the existence of an Astrophysics degree. From that moment, that became the goal. I've always loved to describe astrophysics as \"just maths with pretty pictures\", and honestly, what's not to love about that combination? In 2018, fresh out of school, I embarked on a Physics, Astrophysics, and Cosmology MPhys at Lancaster University. My Masters project involved a deep dive into a dataset of 40,000 galaxies, performing extensive classification and statistical analysis. However, my very first taste of what I now recognise as data science techniques came during an internship at The University of Cambridge's Institute of Astronomy. There, I used Markov-Chain Monte Carlo modelling to understand the shapes and densities of debris and dust around exoplanets, which even led to this published paper.</p> <p></p> <p>Nearing the end of my university studies, a significant realisation dawned on me: despite my enthusiasm for my degree and for astrophysics as a whole, I just wanted to contribute to something that had a tangible impact in the \"real world\". So, I dove into the job market, applying to almost every role that claimed to welcome physics graduates, though with limited success. It was during a moment of frustration that my mum suggested: \"Amaia, you love understanding complicated topics through reading and making graphs, why don't you look for jobs where you can do that?\"</p> <p>That simple question was a revelation. It completely shifted my perspective, and I began searching for data-focused roles. That's how I discovered the NHS Digital graduate scheme. It felt like the perfect alignment \u2013 an opportunity to explore a data career within a broader technical scheme, which meant I wasn't locked into a single path. Crucially, it offered the chance to make a real difference for real people, within an organisation I deeply believe in.</p>"},{"location":"articles/2025/07/03/interview_with_aib/#once-you-joined-the-nhs-what-was-that-experience-like-what-different-roles-and-teams-have-you-been-a-part-of-and-how-have-they-shaped-your-career","title":"Once you joined the NHS, what was that experience like? What different roles and teams have you been a part of, and how have they shaped your career?","text":"<p>I've now been with the NHS for nearly three years, beginning my journey at NHS Digital before its merger with NHS England. Over half of this time has been dedicated to data science roles. My initial placement at NHS Digital was within an analytics team in the Secondary Care area, where I primarily worked with Patient Reported Outcome Measures (PROMs) data.</p> <p>For my second placement, I decided to venture into something completely different, joining the Run &amp; Maintain team in the NHS App as a software developer. It quickly became clear that this wasn't my forte; I wasn't very good at it, nor did I truly enjoy it. After spending a year at the NHS, observing my fellow grad, Will, creating \"cool plots\" for the CVD Pathways project, I decided that the team that he was in might be a good match. For my final placement, I decided to join the Data Science Team myself. I've since successfully graduated from the grad scheme and am now a permanent member of the Data Science and Applied AI Team.</p>"},{"location":"articles/2025/07/03/interview_with_aib/#what-are-you-currently-working-on-are-there-any-projects-that-youre-particularly-excited-about-or-that-you-feel-are-making-a-real-difference-what-impact-are-you-having","title":"What are you currently working on? Are there any projects that you're particularly excited about, or that you feel are making a real difference? What impact are you having?","text":"<p>Currently, I'm deeply involved with the Data Linkage Hub, specifically focusing on the probabilistic data linkage pipeline. This project utilises a probabilistic model designed to accurately identify records belonging to the same patient, using their demographic information. The reason this project excites me so much is its foundational importance to almost all data work within our organisation. We handle immense volumes of data, and without the ability to reliably link records to individual patients, combining datasets to understand patient pathways would be fundamentally impossible. Accurate data linkage underpins the precision of our publications, the robustness of research studies, and ultimately, the efficacy of direct patient care. This is why I've been dedicated to this project since joining the team, and why I continue to be enthusiastic about its impact.</p> <p>I also lead our Marketing &amp; Comms function team, where we focus on making sure people know about the work we\u2019re doing, the impact it\u2019s having, and the wide range of skills across the team. It\u2019s a part of the role I really enjoy: I get to shout about work I genuinely believe in. You can get a flavour of it from the data science website I help run here.</p>"},{"location":"articles/2025/07/03/interview_with_aib/#if-you-could-give-someone-just-starting-out-in-data-science-a-few-pieces-of-advice-what-would-they-be-and-what-resources-have-you-found-particularly-helpful-along-the-way-that-you-can-share","title":"If you could give someone just starting out in data science a few pieces of advice, what would they be? And what resources have you found particularly helpful along the way that you can share?","text":"<p>I believe that, more so now than even a couple of years ago when I started, there's a strong temptation for people to apply AI and machine learning to every single problem. And as ironic as it might sound coming from someone whose job revolves around developing these products, one of the most invaluable skills I've got is the ability to critically assess a situation and determine when data science techniques are, in fact, not the optimal solution. Being able to think critically about the ethical implications and the practical usefulness \u2013 or lack thereof \u2013 of these products, in my opinion, makes me a more valuable team member and a more effective data scientist. Therefore, I would always advise anyone starting out to not just read about the benefits of certain techniques, but to dedicate significant time to understanding their pitfalls and limitations first.</p> <p>My second piece of advice is that genuine enthusiasm and enjoyment in your work are the most powerful ways to succeed. Don't pursue data science simply because it's a popular or trendy field with growing job opportunities. Instead, figure out what truly brings you joy, be it coding, maths, creating visualisations, or anything else, and then explore how that passion aligns with the role of a data scientist. By doing so, you'll naturally gravitate towards a niche within the vast landscape of data science, which in turn will make you a more attractive and unique candidate in the job market.</p> <p>We hope you found this interview with Amaia Imaz Blanco inspiring. Her journey shows how following your enthusiasm and passions can lead you to doing work you love. You can also view our previous \u2018Interview with a Data Scientist\u2019 with Adam Hollings, Dim Sum and Data Science, on the NHS England Data Science Website.</p>"},{"location":"articles/2025/07/07/volunteering-day-2025/","title":"\ud83c\udf31 NHSE Data Science and Applied AI Team Gets Hands Dirty Volunteering at Seacroft Forest Garden \ud83c\udf31","text":"<p>At NHS England, employees get a handful of days each year to volunteer - helping out with causes that align with key NHS goals like helping people live healthier lives, improving quality of life, supporting recovery, and promoting equality in health.</p> <p>The Data Science and Applied AI team recently grabbed this opportunity to give back, with a day spent volunteering at Seacroft Forest Garden, organised by the Wellbeing and Inclusion Function Team.</p> <p></p>"},{"location":"articles/2025/07/07/volunteering-day-2025/#whats-seacroft-forest-garden","title":"\ud83e\udeb4 What\u2019s Seacroft Forest Garden? \ud83e\udeb4","text":"<p>Seacroft Forest Garden started as a local climate action project. It\u2019s built on what used to be a neglected patch of land owned by Leeds City Council Housing. The garden follows permaculture principles, meaning everything planted is either edible or useful, and it\u2019s designed to benefit not just people, but wildlife and the environment too.</p> <p>Local folks get to enjoy fresh fruit, vegetables, herbs, and a peaceful green space to unwind. Plus, the trees planted help store carbon and protect wildlife - so it\u2019s a win-win for the community and nature.</p>"},{"location":"articles/2025/07/07/volunteering-day-2025/#how-the-day-went","title":"\ud83c\udf3f How the Day Went \ud83c\udf3f","text":"<p>We arrived bright and early at 10am and after a quick briefing on the jobs and tools available we got stuck in straight away. The garden team provided all the gear we needed - from gloves to drills.</p> <p>There were lots of different tasks for all skill levels. Many of us started by weeding, carefully pulling out nettles by the roots so the other plants could thrive. The elbow-length gloves were a lifesaver, protecting us from countless nettle stings on our forearms (though a few still managed to sneak through!). We joked that we should have taken a \u201cbefore\u201d photo, because the difference was so striking. It was rewarding to see the impact of our efforts - clearing space, creating a safer environment, and making the garden look so much more inviting.</p> <p>Others wheeled barrows of mulch across the site, spreading it over the beds to lock in moisture and suppress weeds. A few volunteers even pitched in to build a new garden path, giving the area a more accessible and welcoming feel for future visitors.</p> <p>After lunch, the garden leaders taught us how to weave hazel fences\u2014a traditional technique none of us had tried before. We planted stakes into the ground and worked together to weave each hazel rod around them, gradually creating a sturdy natural barrier for the sensory garden. The finished fence not only helped keep out small animals but also gave the space a sense of privacy and calm. It was a fun, hands-on new skill for many of us, and a great teamworking exercise!</p> <p>By the end of the day, we were tired but very proud of the difference we\u2019d made in the garden. We wrapped up with a well-deserved trip to the pub to relax over some food and drinks!</p>"},{"location":"articles/2025/07/07/volunteering-day-2025/#what-the-team-thought","title":"\ud83d\udc69\ud83c\udffc\u200d\ud83c\udf3e \ud83e\uddd1\ud83c\udffb\u200d\ud83c\udf3e What the Team Thought \ud83e\uddd1\ud83c\udffb\u200d\ud83c\udf3e \ud83d\udc69\ud83c\udffc\u200d\ud83c\udf3e","text":"<p>\u201cIt was a really nice thing to do for the community and was great meeting the people who work there and learning about them. It was refreshing to not be looking at a screen all day, getting some fresh air and building team connections in a more casual environment. I think it's really important for us to remember that doing good for the community is a key part of our job, and volunteering plays a big role in that.\u201d - Scarlett Kynoch (Senior Data Scientist)</p> <p>\u201cThis has been fantastic. I feel like I\u2019ve learned something about every single person I worked with.\u201d \u2013 Adam Hollings (Principal Data Scientist)</p> <p>\u201cIt\u2019s amazing what a difference a few hours and 15 people can make to a place like this. I really enjoyed the fence weaving - it was something new and so satisfying to see what we created by the end of the day.\u201d \u2013 Amaia Imaz-Blanco (Senior Data Scientist)</p> <p></p>"},{"location":"articles/2025/07/07/volunteering-day-2025/#want-to-volunteer","title":"\ud83c\udf43 Want to Volunteer? \ud83c\udf43","text":"<p>If you work for NHSE and want to use your volunteering allowance, speak with your line manager and check out the policy on the One Stop Shop.</p> <p>If you want to get involved with Seacroft Forest Garden directly, they\u2019re always happy to welcome helpers on Sundays from 10am to 12pm and Wednesdays from 11am to 2pm.</p>"},{"location":"our_work/","title":"Our Work","text":"Explore our comprehensive portfolio of ongoing and completed projects that harness the power of data to drive insight. <p>Take me to a random project  Explore our projects by categories &amp; tags </p> <p> </p> <p>From predictive modelling to data-driven decision support, we work on a number of dynamic challenges in healthcare.</p> <p>Explore our project pages to:</p> <ul> <li>Understand the problems we're looking into and how we've approached them, ranging from optimising patient outcomes and resource allocation to enhancing operational efficiency across the healthcare ecosystem.</li> <li>Learn about the key results of our work and what impact we've had.</li> <li>Find links to further resources which came out of these projects, such as codebases, tools and reports.</li> </ul>"},{"location":"our_work/Papi_Webpage/","title":"Risk stratification models for Population and Person Insights (PaPI)","text":"","tags":["POPULATION HEALTH","MACHINE LEARNING","MODELLING","STRUCTURED DATA","PYTHON"]},{"location":"our_work/Papi_Webpage/#about-papi","title":"About PaPI","text":"<p>The PaPI (Population and Person Insights) project aims to review, improve, and update the risk stratification models used in PaPI dashboards. These dashboards provide critical insights into health and care services by analysing person-level linked data, including the National Bridges to Health Segmentation Dataset. The project specifically focuses on the \"risk stratification\" view - predicting the likelihood of:</p> <ul> <li>A&amp;E attendance (Type 1 departments) within 12 months</li> <li>Emergency readmission within 12 months</li> <li>Emergency readmission within 30 days</li> </ul> <p>The goal is to improve the accuracy and relevance of these risk predictions, which are currently based on pre-COVID data from 2019, by retraining the models and data pipelines following a platform migration (NCDR to UDAL). This will enable national, regional, and local health and care systems to better understand and manage population health risks.</p> Papi Dashboard","tags":["POPULATION HEALTH","MACHINE LEARNING","MODELLING","STRUCTURED DATA","PYTHON"]},{"location":"our_work/Papi_Webpage/#why-papi","title":"Why PaPI?","text":"<p>PaPI offers a modern, data-driven approach to understanding population health data, enabling national, regional, and local systems to:</p> <ul> <li>Predict and Prevent Risks: Identify high-risk individuals and cohorts to reduce avoidable admissions and improve outcomes.</li> <li>Support Evidence-Based Decisions: Equip clinicians, senior leaders, and policymakers with insights tailored to their needs, ensuring informed and effective decisions.</li> <li>Enhance Resource Efficiency: Optimise financial and operational planning through accurate forecasting and stratification models.</li> <li>Align with NHS Priorities: Drive preventive care initiatives, service recovery efforts, and trust-level benchmarking to meet national healthcare objectives.</li> </ul> <p>This project puts patients at the centre to ensure they receive the best care and outcomes, which is our major goal in the NHS as everything we do is for our patients. By predicting the likelihood of a patient (or multiple patients) attending A&amp;E, we can prepare adequately to provide thereby improving resource allocation. Our risk stratification approach aids in planning and prioritizing resources to meet patients\u2019 needs. Doing Risk Stratification aligns with the NHS\u2019s preventive care objectives.</p>","tags":["POPULATION HEALTH","MACHINE LEARNING","MODELLING","STRUCTURED DATA","PYTHON"]},{"location":"our_work/Papi_Webpage/#what-we-did-risk-stratification-modelling","title":"What we did \u2013 Risk stratification modelling","text":"Risk Stratification Modelling","tags":["POPULATION HEALTH","MACHINE LEARNING","MODELLING","STRUCTURED DATA","PYTHON"]},{"location":"our_work/Papi_Webpage/#data","title":"Data","text":"<p>The data comes from two datasets, SUS and OBH Segmentation. This is pseudonymised person-level data, covering multiple sources (SUS, APC, outpatient, A&amp;E, ECDS, community services, and mental health) from 2008 to 2023. The segmentation dataset splits the population into eight main groups (i.e., healthy, long-term conditions, serious disability, incurable cancer, organ failure, frailty/dementia, maternal and infant health and acutely ill).</p> <p>For the modelling we are using features like demographic, details like age, gender, and ethnicity are included, alongside membership in the Bridges to Health (B2H) segment. We also factor in over 50 medical conditions (for instance, Asthma, Atrial_Fibrillation, COPD, and Diabetes) and information on previous healthcare activity, such as the number of A&amp;E attendances.</p>","tags":["POPULATION HEALTH","MACHINE LEARNING","MODELLING","STRUCTURED DATA","PYTHON"]},{"location":"our_work/Papi_Webpage/#methods","title":"Methods","text":"<ul> <li> <p>The initial phase focuses on establishing baseline performance metrics. The entire dataset, combining all segments, is utilized with the original set of features. Four machine learning models\u2014Logistic Regression (LR), Support Vector Machine (SVM), Random Forest (RF), and XGBoost (XGB)\u2014are tested using basic class balancing strategies (1:1 and 1:2) with no hyperparameter optimization applied</p> </li> <li> <p>We then modelled for the six population segments as to assess performance differences across various subgroups. A minimal feature set is used, and the same four algorithms (LR, SVM, RF, XGB) are applied using the previously established balancing strategies (1:1 and 1:2). Stratified cross-validation ensures robust evaluation. Similar to Phase 1, no hyperparameter optimization is performed.</p> </li> <li> <p>In phase 3 we aim to refine the feature set for improved model performance. For poorly performing segments, tailored features may be added, and irrelevant features identified through segment overlap analysis or low importance rankings (e.g., using RF and XGB feature importance) may be removed. Data quality insights, such as highly correlated features, are also considered. New features identified during data exploration are integrated, and techniques such as Principal Component Analysis (PCA) may be employed for dimensionality reduction.</p> </li> <li> <p>The phase 4 focuses on fine-tuning the top two performing models from Phase 2 to achieve optimal performance. Hyperparameter optimization is conducted using stratified cross-validation and the best class balancing ratio (1:1 or 1:2) identified in earlier phases. The goal is to evaluate and quantify improvements in model performance due to optimization, ensuring the models are both accurate and efficient.</p> </li> <li> <p>The final Phase emphasizes model interpretability and explainability. SHAP (Shapley Additive Explanations) values are used to understand how features influence predictions, providing actionable insights. The best-performing model for each segment is selected based on performance metrics and explainability. This phase ensures that the models are not only accurate but also transparent, enabling informed decision-making.</p> </li> </ul>","tags":["POPULATION HEALTH","MACHINE LEARNING","MODELLING","STRUCTURED DATA","PYTHON"]},{"location":"our_work/Papi_Webpage/#key-exploitable-results-kers-benefits","title":"Key Exploitable Results (KERs) &amp; Benefits","text":"<p>The Key Exploitable results of a project is an identified main interesting result, which has been selected and prioritized due to its high potential to be \u201cexploited\u201d. </p> <p>We have identified various KERS which can be useful to other teams, they are listed below:</p> <ul> <li>PaPI Ways of working: (agile methods, environment, ethics, quality processes): Demonstrates how applying the Quality Assurance Framework and other project management/coding best practices can help organize a project effectively.</li> <li>Code Carbon Library Integration: Tracks CO\u2082 emissions when training models, serving as a benchmark for monitoring environmental impact. The code is publicly available on GitHub and can be replicated by others. </li> <li>Risk Stratification Data Asset: helps to understand and manage population health risks, contributing to Population Health Management initiatives</li> </ul>","tags":["POPULATION HEALTH","MACHINE LEARNING","MODELLING","STRUCTURED DATA","PYTHON"]},{"location":"our_work/Papi_Webpage/#ethical-considerations-compliance","title":"Ethical Considerations &amp; Compliance","text":"<p>We have considered ethical requirements and ensured compliance with NHS policies, prioritising fairness and transparency. </p>","tags":["POPULATION HEALTH","MACHINE LEARNING","MODELLING","STRUCTURED DATA","PYTHON"]},{"location":"our_work/Papi_Webpage/#whats-next","title":"What\u2019s Next?","text":"<p>The Papi Project is currently in the phase 4. We have refined the feature set for improved model performance. For poorly performing segments, new features are being added, and irrelevant features identified through segment overlap analysis or low importance rankings (e.g., XGBoost feature importance \u2018gains\u2019) are being removed. Data quality insights, such as highly correlated features, were also considered.</p> <p>Project summary last updated 12/02/2025 by Michelle Nwachukwu </p>","tags":["POPULATION HEALTH","MACHINE LEARNING","MODELLING","STRUCTURED DATA","PYTHON"]},{"location":"our_work/Papi_Webpage/#_1","title":"Risk stratification models for Population and Person Insights (PaPI)","text":"","tags":["POPULATION HEALTH","MACHINE LEARNING","MODELLING","STRUCTURED DATA","PYTHON"]},{"location":"our_work/Publications/","title":"Connected Publications","text":"<p>List of pre-releases and publications connected to our work</p> <p>[10] https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/data-sharing/privacy-enhancing-technologies/case-studies/g7-dpas-emerging-technologies-working-group-use-case-study-on-privacy-enhancing-technologies/</p> <p>G7 DPAs' Emerging Technologies Working Group use case study on privacy enhancing technologies</p> <p>The IEEE Synthetic Data working group, Professor Khaled El Emam, University of Ottawa, Canada, Dr Jonathan Pearson, NHS England Digital Analytics and Research, UK, Professor Pierre-Antoine Gourraud, Universit\u00e9 de Nantes, France, Dr Aur\u00e9lien Bellet, National Institute for Research in Digital Science and Technology (Inria), France</p> <p>This hypothetical use case aims to demonstrate how synthetic data, generated from a real medical prescription dataset, can be used as part of a testing strategy for the development of a healthcare planning and resource allocation system, without jeopardising the privacy of real patients.</p> <p>[9] https://www.researchsquare.com/article/rs-5701032/v1 </p> <p>Constructing multicancer risk cohorts using national data from medical helplines and secondary care</p> <p>Hadi Modarres, Dimitris Pipinis, Divya Balasubramanian, Rupert Chaplin, Scarlett Kynoch, Achut Manandhar, Gursimran Thandi, Rebecca Cavilla, Emma Hirst-Williams</p> <p>Identification of cohorts at higher risk of cancer can enable earlier diagnosis of the disease, which significantly improves patient outcomes. In this study, we use machine learning to predict cancer diagnosis in the next year. We select nine cancer sites with high incidence of late-stage diagnosis or worsening survival rates, and where there are currently no national screening programmes. We use National Health Service (NHS) data from medical helplines (NHS 111) and secondary care appointments from all hospitals in England. We show that features based on information captured in NHS 111 calls are among the most influential in driving predictions of a future cancer diagnosis. Our predictive models exhibit good discrimination (AUC \u2013 0.78 \u2013 SD 0.04), ranging from 0.69 (ovarian cancer) to 0.83 (oesophageal cancer). While our predictive modelling provides patient level risk predictions, our emphasis is on constructing cohorts of patients who may be at risk of cancer rather than individual risk scores. We present an approach of constructing cohorts at higher risk of cancer based on feature importance and considering possible bias in model results. These outputs can be used to develop highly targeted case finding services, which could help increase earlier detection rates and reduce health disparities. This approach is flexible and can be tailored based on the group the intervention targets (i.e. symptomatic or asymptomatic patients) and the data available to those charged with administering the intervention.</p> <p>[8] https://arxiv.org/abs/2311.13978</p> <p>Medisure: Towards Assuring Machine Learning-Based Medical Image Classifiers Using Mixup Boundary Analysis</p> <p>Adam Byfield, William Poulett, Ben Wallace, Anusha Jose, Shatakshi Tyagi, Smita Shembekar, Adnan Qayyum, Junaid Qadir, Muhammad Bilal</p> <p>Machine learning (ML) models are becoming integral in healthcare technologies, necessitating formal assurance methods to ensure their safety, fairness, robustness, and trustworthiness. However, these models are inherently error-prone, posing risks to patient health and potentially causing irreparable harm when deployed in clinics. Traditional software assurance techniques, designed for fixed code, are not directly applicable to ML models, which adapt and learn from curated datasets during training. Thus, there is an urgent need to adapt established software assurance principles such as boundary testing with synthetic data. To bridge this gap and enable objective assessment of ML models in real-world clinical settings, we propose Mix-Up Boundary Analysis (MUBA), a novel technique facilitating the evaluation of image classifiers in terms of prediction fairness. We evaluated MUBA using brain tumour and breast cancer classification tasks and achieved promising results. This research underscores the importance of adapting traditional assurance principles to assess ML models, ultimately enhancing the safety and reliability of healthcare technologies. Our code is available at https://github.com/willpoulett/MUBA_pipeline.</p> <p>[7] https://publichealth.jmir.org/2024/1/e46485</p> <p>The Use of Online Consultation Systems or Remote Consulting in England Characterized Through the Primary Care Health Records of 53 Million People in the OpenSAFELY Platform: Retrospective Cohort Study</p> <p>Martina Fonseca, Brian MacKenna,  Amir Mehrkar, The OpenSAFELY Collaborative, Caroline E Walters, George Hickman,  Jonathan Pearson,  Louis Fisher,  Peter Inglesby,  Seb Bacon,  Simon Davy, William Hulme,  Ben Goldacre,  Ofra Koffman,  Minal Bakhai</p> <p>We aimed to explore general practice coding activity associated with the use of Online Consultations (OC) systems in terms of trends, COVID-19 effect, variation, and quality.  The OpenSAFELY platform was used to query and analyze the in situ electronic health records of suppliers The Phoenix Partnership (TPP) and Egton Medical Information Systems, covering &gt;53 million patients in &gt;6400 practices, mainly in 2019-2020.  We successfully queried general practice coding activity relevant to the use of OC systems, showing increased adoption and key areas of variation during the pandemic at both sociodemographic and clinical levels. The work can be expanded to support monitoring of coding quality and underlying activity. This study suggests that large-scale impact evaluation studies can be implemented within the OpenSAFELY platform, namely looking at patient outcomes.</p> <p>[6] https://arxiv.org/abs/2403.19802</p> <p>Developing Healthcare Language Model Embedding Spaces</p> <p>Niall Taylor, Dan Schofield, Andrey Kormilitzin, Dan W Joyce, Alejo Nevado-Holgado</p> <p>Pre-trained Large Language Models (LLMs) often struggle on out-of-domain datasets like healthcare focused text. We explore specialized pre-training to adapt smaller LLMs to different healthcare datasets. Three methods are assessed: traditional masked language modeling, Deep Contrastive Learning for Unsupervised Textual Representations (DeCLUTR), and a novel pre-training objective utilizing metadata categories from the healthcare settings. These schemes are evaluated on downstream document classification tasks for each dataset, with additional analysis of the resultant embedding spaces. Contrastively trained models outperform other approaches on the classification tasks, delivering strong performance from limited labeled data and with fewer model parameter updates required. While metadata-based pre-training does not further improve classifications across the datasets, it yields interesting embedding cluster separability. All domain adapted LLMs outperform their publicly available general base LLM, validating the importance of domain-specialization. This research illustrates efficient approaches to instill healthcare competency in compact LLMs even under tight computational budgets, an essential capability for responsible and sustainable deployment in local healthcare settings. We provide pre-training guidelines for specialized healthcare LLMs, motivate continued inquiry into contrastive objectives, and demonstrates adaptation techniques to align small LLMs with privacy-sensitive medical tasks.</p> <p>[5] https://link.springer.com/chapter/10.1007/978-3-031-56107-8_21 - Conference Paper</p> <p>Hypergraphs for Frailty Analysis Research Paper</p> <p>Zoe Hancox, Samuel D. Relton, Andrew Clegg, Philip G. Conaghan, and Daniel Schofield</p> <p>Inclusion of mortality to hypergraphs alongside the most prevalent combinations of frailty conditions.  This paper demonstrates that this technique enables us to determine the probability of acquiring another condition as well as understanding the connection and sequencing of acquiring comorbidities.</p> <p>[4] https://doi.org/10.1101/2023.08.31.23294903 - (Pre-Print)</p> <p>Representing Multimorbid Disease Progressions using directed hypergraphs</p> <p>Jamie Burke, Ashley Akbari, Rowena Bailey, Kevin Fasusi, Ronan A.Lyons, Jonathan Pearson, James Rafferty, and Daniel Schofield</p> <p>To introduce directed hypergraphs as a novel tool for assessing the temporal relationships between coincident diseases,addressing the need for a more accurate representation of multimorbidity and leveraging the growing availability of electronic healthcare databases and improved computational resources.</p> <p>[3] https://doi.org/10.1016/j.epidem.2022.100662</p> <p>Large-scale calibration and simulation of COVID-19 epidemiologic scenarios to support healthcare planning</p> <p>Nick Groves-Kirkby, Ewan Wakeman, Seema Patel, Robert Hinch, Tineke Poot, Jonathan Pearson, Lily Tang, Edward Kendall, Ming Tang, Kim Moore, Scott Stevenson, Bryn Mathias, Ilya Feige, Simon Nakach, Laura Stevenson, Paul O'Dwyer, William Probert, Jasmina Panovska-Griffiths, Christophe Fraser</p> <p>... We adapted an agent-based model of COVID-19 to inform planning and decision-making within a healthcare setting, and created a software framework that automates processes for calibrating the model parameters to health data and allows the model to be run at national population scale on National Health Service (NHS) infrastructure. ... These simulations were used to support operational planning in the NHS in England, and we present the example of the use of these simulations in projecting future clinical demand during the rollout of the national COVID-19 vaccination programme. ...</p> <p>[2] https://doi.org/10.1101/2023.01.25.23284428</p> <p>Primary care coding activity related to the use of online consultation systems or remote consulting: an analysis of 53 million peoples\u2019 health records using OpenSAFELY</p> <p>Martina Fonseca, Brian MacKenna, Amir Mehrkar, The OpenSAFELY Collaborative, Caroline E Walters, George Hickman, Jonathan Pearson, Louis Fisher, Peter Inglesby, Seb Bacon, Simon Davy, William Hulme, Ben Goldacre, Ofra Koffman, Minal Bakhai</p> <p>We aimed to explore general practice coding activity associated with the use of online consultation systems in terms of trends, COVID-19 effect, variation and quality.</p> <p>[1] https://doi.org/10.21203/rs.3.rs-2226531/v1</p> <p>Assessing the value of integrating national longitudinal shopping data into respiratory disease forecasting models</p> <p>Elizabeth Dolan, James Goulding, Harry Marshall, Gavin Smith, Gavin Ling, Laila Tata</p> <p>... We investigated the value of integrating sales of non-prescription medications commonly bought for managing respiratory symptoms, to improve forecasting of weekly registered deaths from respiratory disease at local levels across England, by using over 2 billion transactions logged by a UK high street retailer from March 2016 to March 2020. We report the results from the novel AI explainability variable importance tool Model Class Reliance implemented on the PADRUS model. ...</p>","tags":["PUBLICATIONS"]},{"location":"our_work/Publications/#_1","title":"Connected Publications","text":"","tags":["PUBLICATIONS"]},{"location":"our_work/a_and_e_forecasting_tool/","title":"Accident and Emergency (A&E) Forecasting Tool","text":"<p>The A&amp;E forecasting tool models historical data from A&amp;E departments to forecast likely admissions numbers over the coming days up to three weeks in advance. This is a probabilistic model, which provides confidence intervals within which a given location's admissions numbers are likely to fall. Breakdowns are provided at the Country, Region, ICS, Trust, and Site level as well as broad age groups. The tool is intended to be used alongside local information to help trusts and ICSs plan and prepare for the demand placed on their services. </p> <p>Having this information can help departments, trusts, and ICS' plan and prepare for the demand placed on their services. Breakdowns are provided at the Country, Region, ICS, Trust, and Site level. Weekly trends are provided. </p> <p></p> <p>Forecasts are created at the chosen level of granularity (site, region, and so on); and these show expected admissions numbers for the following three weeks. These forecasts come with different levels of specificity, and differing levels of confidence. Users can view different forecasts in this way. For example, the 95% confidence interval forecast makes a less specific prediction than the 50% CI forecast, but as such the predictions made here are more likely to be correct. </p> <p>If looking at a higher level of granularity, such as region, a breakdown of constituent locations is shown comparing their forecast admission rates<sup>1</sup>. </p> <p>The tool uses a Bayesian hierarchical time series forecasting model that is inspired by Facebook's 'prophet' model and uses Monte Carlo Markov Chain (MCMC) technique to infer model parameters. Since the model can be decomposed into components, these component-level breakdowns are also provided to users in the dashboard to help explain the contribution of each component to forecasted admission numbers.</p> <p>We provide links to relevant guides, documentation, and tools below.</p>","tags":["SECONDARY CARE","FORECASTING","MODELLING","TIME SERIES","STRUCTURED DATA","PYTHON","WIP","DEPLOYED"]},{"location":"our_work/a_and_e_forecasting_tool/#links","title":"Links","text":"<p>Note that some of the following links are hosted on the Future NHS website, which requires an account and is not open to the general public.</p> Links Accessible By User Guide NHS Staff Product NHS Staff Prophet Forecasting Repo Public Accessible lecture on MCMC modelling methods Public <ol> <li> <p>Here the different lines correspond to different ICBs within a region. We have intentionally omitted the legend from this example, since the we are aiming to illustrate the functionality of the tool, and not to provide information about particular forecasts at particular site.\u00a0\u21a9</p> </li> </ol>","tags":["SECONDARY CARE","FORECASTING","MODELLING","TIME SERIES","STRUCTURED DATA","PYTHON","WIP","DEPLOYED"]},{"location":"our_work/adrenal-lesions/","title":"Using deep learning to detect adrenal lesions in CT scans","text":"<p>Many cases of adrenal lesions, known as adrenal incidentalomas, are discovered incidentally on CT scans performed for other medical conditions. These lesions can be malignant, and so early detection is crucial for patients to receive the correct treatment and allow the public health system to target resources efficiently. Traditionally, the detection of adrenal lesions on CT scans relies on manual analysis by radiologists, which can be time-consuming and unsystematic.</p> <p>The main aim of this study was to examine whether or not using AI can improve the detection of adrenal incidentalomas in CT scans. Previous studies have suggested that AI has the potential in distinguishing different types of adrenal lesions. In this study, we specifically focused on detecting the presence of any type of adrenal lesion in CT scans. To demonstrate this proof-of-concept, we investigated the potential of applying deep learning techniques to predict the likelihood of a CT abdominal scan presenting as \u2018normal\u2019 or \u2018abnormal\u2019, the latter implying the presence of an adrenal lesion.</p>","tags":["DISEASES","CLASSIFICATION","COMPUTER VISION","MODELLING","VISUAL DATA","PYTHON","COMPLETE"]},{"location":"our_work/adrenal-lesions/#case-study","title":"Case Study","text":"<p>This is a backup of the case study published here on the NHS England Transformation Directorate website.</p> <p>Many cases of adrenal lesions, known as adrenal incidentalomas, are discovered incidentally on CT scans performed for other medical conditions. These lesions can be malignant, and so early detection is crucial for patients to receive the correct treatment and allow the public health system to target resources efficiently. Traditionally, the detection of adrenal lesions on CT scans relies on manual analysis by radiologists, which can be time-consuming and unsystematic.</p> <p>The challenge Can applying AI and deep learning augment the detection of adrenal incidentalomas in patients\u2019 CT scans?</p>","tags":["DISEASES","CLASSIFICATION","COMPUTER VISION","MODELLING","VISUAL DATA","PYTHON","COMPLETE"]},{"location":"our_work/adrenal-lesions/#overview","title":"Overview","text":"<p>Autopsy studies reveal a statistic that as many as 6% of all natural deaths displayed a previously undiagnosed adrenal lesion. Such lesions are also found incidentally (and are therefore referred to as adrenal incidentalomas) in approximately 1% of chest or abdominal CT scans. These lesions affect approximately 50,000 patients annually in the United Kingdom, with significant impact on patient health, including 10% to 15% of cases of excess hormone production, or 1% to 5% of cases of cancer.</p> <p>It is a significant challenge for the health care system to, in a standardised way, promptly reassure the majority of patients, who have no abnormalities, whilst effectively focusing on those with hormone excess or cancers. Issues include over-reporting (false positives), causing patient anxiety and unnecessary investigations (wasting resources of the health care system), and under-reporting (missed cases), with potentially fatal outcomes. This has major impacts on patient well-being and clinical outcomes, as well as cost-effectiveness.</p> <p>The main aim of this study was to examine whether or not using Artificial Intelligence (AI) can improve the detection of adrenal incidentalomas in CT scans. Previous studies have suggested that AI has the potential in distinguishing different types of adrenal lesions. In this study, we specifically focused on detecting the presence of any type of adrenal lesion in CT scans. To demonstrate this proof-of-concept, we investigated the potential of applying deep learning techniques to predict the likelihood of a CT abdominal scan presenting as \u2018normal\u2019 or \u2018abnormal\u2019, the latter implying the presence of an adrenal lesion.</p>","tags":["DISEASES","CLASSIFICATION","COMPUTER VISION","MODELLING","VISUAL DATA","PYTHON","COMPLETE"]},{"location":"our_work/adrenal-lesions/#what-we-did","title":"What we did","text":"<p>Using the data provided by University Hospitals of North Midlands NHS Trust, we developed a 2.5D deep learning model to perform detection of adrenal lesions in patients\u2019 CT scans (binary classification of normal and abnormal adrenal glands). The entire dataset is completely anonymised and does not contain any personal or identifiable information of patients. The only clinical information taken were the binary labels for adrenal lesions (\u2018normal\u2019 or \u2018abnormal\u2019) for the pseudo-labelled patients and their CT scans.</p>","tags":["DISEASES","CLASSIFICATION","COMPUTER VISION","MODELLING","VISUAL DATA","PYTHON","COMPLETE"]},{"location":"our_work/adrenal-lesions/#25d-images","title":"2.5D images","text":"<p>A 2.5D image is a type of image that lies between a typical 2D and 3D image. It can retain some level of 3D features and can potentially be processed as a 2D image by deep learning models. A greyscale 2D image is two dimensional with a size of x \u00d7 y, where x and y are the length and width of the 2D image. For a greyscale 3D image (e.g., a CT scan), with a size of x \u00d7 y \u00d7 n, it can be considered as a combination of a stack of n number of greyscale 2D images. In other words, a CT scan is a 3D image consisting of multiple 2D images layered on top of each other. The size of a 2.5D image is x \u00d7 y \u00d7 3, and it represents a stack of 3 greyscale 2D images.</p> <p>Typically, an extra dimension of pixel information is required to record and display 2D colour images in electronic systems, such as the three RGB (red, green, and blue) colour channels. This increases the size of a 2D image to x \u00d7 y \u00d7 3, where the 3 represents the three RGB channels. Many commonly used families of 2D deep learning algorithms (e.g., VGG, ResNet, and EfficientNet) have taken colour images into account and have the ability to process images with the extra three channels. Taking the advantage of the fact that pixel volumes have the same size between 2D colour images and 2.5D images, converting our 3 dimensional CT scan data to 2.5D images can allow us to apply 2D deep learning models on our images.</p>","tags":["DISEASES","CLASSIFICATION","COMPUTER VISION","MODELLING","VISUAL DATA","PYTHON","COMPLETE"]},{"location":"our_work/adrenal-lesions/#why-using-a-25d-model","title":"Why using a 2.5D model","text":"<p>Due to the intrinsic nature of CT scans (e.g., a high operating cost, limited number of available CT scanners, and patients\u2019 exposure to radiation), the acquisition of a sufficient amount of CT scans for 3D deep learning models training is challenging. In many cases, the performance of 3D deep learning models is limited by the small and non-diversified dataset. Training, validating, and testing the model with a small dataset can lead to many disadvantages, for example, a high risk of overfitting the training-validation set (low prediction ability on an unseen test set), and evaluating the model performance within the ambit of a small number statistic (under-represented test set results in the test accuracy much lower/higher than the underlying model performance).</p> <p>To overcome some of the disadvantage of training a 3D deep learning model, we took a 2.5D deep learning model approach in this case study. Training the model using 2.5D images enables our deep learning model to still learn from the 3D features of the CT scans, while increasing the number of training and testing data points in this study. Moreover, we can apply 2D deep learning models to the set of 2.5D images, which allow us to apply transfer learning to train our own model further based on the knowledge learned by other deep learning applications (e.g., ImageNet, and the NHS AI Lab\u2019s National COVID-19 Chest Imaging Database).</p> <p></p>","tags":["DISEASES","CLASSIFICATION","COMPUTER VISION","MODELLING","VISUAL DATA","PYTHON","COMPLETE"]},{"location":"our_work/adrenal-lesions/#classification-of-3d-ct-scans","title":"Classification of 3D CT scans","text":"<p>To perform the binary classification on the overall CT scans (instead of a single 2.5D image), the classification results from each individual 2.5D image that make up a CT scan are considered.</p> <p>To connect the classification prediction results from the 2.5D images to the CT scan, we introduce an operating value for our model to provide the final classification. The CT scans are classified as normal if the number of abnormal 2.5D images is lower than the threshold operating value. For example, if the operating value is defined to be X, a CT scan will be considered as normal if there are more than X of its 2.5D images classified as normal by our model.</p>","tags":["DISEASES","CLASSIFICATION","COMPUTER VISION","MODELLING","VISUAL DATA","PYTHON","COMPLETE"]},{"location":"our_work/adrenal-lesions/#processing-the-ct-scans-to-focus-on-the-adrenal-glands","title":"Processing the CT scans to focus on the adrenal glands","text":"<p>To prepare the CT scans for this case study (region of interest focus on the adrenal grands), we also developed a manual 3D cropping tool for CT scans. This cropping applied to all three dimensions, including a 1D cropping to select the appropriate axial slices and a 2D cropping on each axial slice. The final cropped 3D image covered the whole adrenal gland on both sides with some extra margin on each side.</p> <p></p>","tags":["DISEASES","CLASSIFICATION","COMPUTER VISION","MODELLING","VISUAL DATA","PYTHON","COMPLETE"]},{"location":"our_work/adrenal-lesions/#outcomes-and-lessons-learned","title":"Outcomes and lessons learned","text":"<p>The resulting code, released as open source on our\u00a0Github (available to anyone to re-use), enables users to:</p> <ul> <li>Process CT scans to focus on the region of interest (e.g., adrenal glands),</li> <li>Transform 3D CT scans to sets of 2.5D images,</li> <li>Train a deep learning model with the 2.5D images for adrenal lesion detection (classification: normal vs. abnormal),</li> <li>Evaluate the trained deep learning model on an independent test set.</li> </ul> <p>This proof-of-concept model demonstrates the ability and potential of applying such deep learning techniques in the detection of adrenal lesions on CT scans. It also shows an opportunity to detect adrenal incidentalomas using deep learning.</p> <p>An AI solution will allow for lesions to be detected more systematically and flagged for the reporting radiologist. In addition to enhanced patient safety, through minimising missed cases and variability in reporting, this is likely to be a cost-effective solution, saving clinician time. \u2013 Professor Fahmy Hanna, Professor of Endocrinology and Metabolism, Keele Medical School and University Hospitals of North Midlands NHS Trust</p>","tags":["DISEASES","CLASSIFICATION","COMPUTER VISION","MODELLING","VISUAL DATA","PYTHON","COMPLETE"]},{"location":"our_work/adrenal-lesions/#who-was-involved","title":"Who was involved?","text":"<p>This project was a collaboration between the NHS AI Lab Skunkworks, within the Transformation Directorate at NHS England and NHS Improvement, and University Hospitals of North Midlands NHS Trust.</p>","tags":["DISEASES","CLASSIFICATION","COMPUTER VISION","MODELLING","VISUAL DATA","PYTHON","COMPLETE"]},{"location":"our_work/adrenal-lesions/#links","title":"Links","text":"Output Link Open Source Code &amp; Documentation Github Case Study Case Study Technical report medRxiv","tags":["DISEASES","CLASSIFICATION","COMPUTER VISION","MODELLING","VISUAL DATA","PYTHON","COMPLETE"]},{"location":"our_work/adrenal-lesions/#_1","title":"Using deep learning to detect adrenal lesions in CT scans","text":"","tags":["DISEASES","CLASSIFICATION","COMPUTER VISION","MODELLING","VISUAL DATA","PYTHON","COMPLETE"]},{"location":"our_work/ai-deep-dive/","title":"AI Deep Dive Workshops","text":"","tags":["WORKFORCE","BEST PRACTICE","OPEN DATA","COMPLETE"]},{"location":"our_work/ai-deep-dive/#info","title":"Info","text":"<p>This is a backup of the case study published here on the NHS England Transformation Directorate website.</p>","tags":["WORKFORCE","BEST PRACTICE","OPEN DATA","COMPLETE"]},{"location":"our_work/ai-deep-dive/#case-study-overview","title":"Case Study Overview","text":"<p>The NHS AI Lab Skunkworks team provides public sector health and social care organisations with artificial intelligence (AI) support and technical expertise. The team of data scientists and AI project specialists has been helping others with their explorations with AI solutions for a range of problems, from supporting hospital bed allocation to detecting CT scan anomalies.</p> <p>It became clear from engaging with organisations in these projects that the NHS AI Lab has an important role to play in increasing the trust and confidence of healthcare staff with AI tools. Both in their creation and in their everyday use. By exploring the possibilities for AI together with the organisations who will use them, the AI Skunkworks team aims to bring some clarity to the potential of AI and diminish some of the hype.</p>","tags":["WORKFORCE","BEST PRACTICE","OPEN DATA","COMPLETE"]},{"location":"our_work/ai-deep-dive/#the-challenge","title":"The challenge","text":"<p>Despite increasing interest in the use of AI technologies within the NHS, it is difficult for busy teams to develop the skills and experience necessary to start new experimentation with AI or manage a successful AI project. Even when a potential use for AI is identified, ideas are often thwarted by the complexity of using AI technologies, lack of suitable data, concerns about patient data security or the burden of achieving regulatory approvals of AI as a medical device.</p> <p>The Digital team at University Hospital Southampton (UHS) approached us with a request to explore the ethical and safety considerations of applying AI in their work. It is especially critical with AI in health and care that the people affected by its use are confident the tools are robust, and any support for decisions is fair for all patients.</p> <p>UHS Digital has been exploring different applications of AI in healthcare for some time. Previously, UHS had applied to the AI Lab Skunkworks Team to see if AI could assist in prioritising patients for endoscopy procedures. With a clear interest in the topic, UHS wanted to learn more about AI key terms and fundamentals. They also wanted to increase confidence in the organisation with regard to identifying the kind of problems that AI could support, and confirm the practicalities and considerations when launching and running an AI experiment.</p> <p>The Skunkworks programme aims to test the development and, when appropriate, adopt AI technologies into all areas of health and care. In addition to supporting the development of proof-of concept AI solutions, and providing open source code on Github for others to re-purpose, we are also providing teams like UHS with a series of AI deep dive workshops.</p> <p>This was a great opportunity to get a frontline NHS IT team thinking about applied AI inside the system. It has certainly served to inspire the team to try new things. \u2013 Matt Stammers, Clinical Lead, University Hospital Southampton Data Science</p>","tags":["WORKFORCE","BEST PRACTICE","OPEN DATA","COMPLETE"]},{"location":"our_work/ai-deep-dive/#ai-deep-dive-workshops","title":"AI deep dive workshops","text":"<p>The workshops provide organisations with the relevant knowledge and tools to understand how to safely launch an AI experiment in healthcare. We provide guidance on identifying a potential real application of AI and use this idea to create a problem statement and identify AI solutions. We also consider the practicalities of running the experiment and the ethical and information governance considerations that are so vital for producing safe and effective technologies.</p> <p>No previous experience or knowledge of AI is necessary as the series of workshops provide an introduction to the key terms, types and applications of AI in healthcare. Hence, this workshop series is open to anyone who is interested in how AI can support their organisation, and this can include clinicians, technology teams, operations teams and senior stakeholders.</p> <p>The 5-part series of virtual and interactive workshops covers:</p> <ul> <li>an introduction to AI and healthcare case studies</li> <li>how to identify potential applications of AI and write up a patient and user focused problem statement</li> <li>practicalities when starting and running an AI experiment, including who needs to be involved in an AI experiment for example ethics, information governance and medical device regulation</li> <li>agile ways of working to ensure the problem and the solution is always patient and user focused.</li> <li>innovation methodologies, for example the Amazon \u2018working backward\u2019 press release.</li> </ul>","tags":["WORKFORCE","BEST PRACTICE","OPEN DATA","COMPLETE"]},{"location":"our_work/ai-deep-dive/#what-we-did","title":"What we did","text":"<p>Having piloted the deep dive process with colleagues across the NHS Transformation Directorate, we arranged calls with the Southampton team to understand their needs.</p> <p>We began by establishing a workshop group of up to 12 participants who would reflect the likely members of staff to be involved in running a data-driven digital transformation initiative. UHS provided a diverse group of participants from teams including electronic patient record (EPR), business intelligence, database/IT, APEX development, clinicians and research data science.</p> <p>In particular, the group wanted support with \u2026.</p> <ul> <li>being more confident in discussion about AI in healthcare</li> <li>embracing the idea of experimentation with AI in healthcare</li> <li>understanding the practical steps required to start experimenting</li> <li>creating a detailed plan for an AI project.</li> </ul> <p>We set up weekly workshops, delivered online over a period of 5 weeks. The workshops looked to identify one problem that was worked on through the series.</p> <p>The running order for this weekly series was:</p> <ul> <li>Workshop 1: AI fundamentals - establish a baseline understanding of AI and the art of the possible.</li> <li>Workshop 2: Problem Discovery - develop skills to identify and communicate problems.</li> <li>Workshop 3: Solution Discovery - identify solutions and potential AI technologies to solve problems.</li> <li>Workshop 4: Practicalities - understand the practical aspects of AI projects.</li> <li>Workshop 5: Launching your AI experiment - understand the next steps in launching an AI project.</li> </ul> <p>You can read more about the deep dive workshop agenda on our Github website.</p> <p>We involved the group in interactive elements using tools such as Mentimeter and Google Jamboard, allowing the groups to collaborate, share ideas and aid discussions.</p> <p>We included a number of innovation approaches such as the Amazon \u2018working backward\u2019 press release product development approach, which helps to imagine what the desired end result will look like. We also introduced the lean canvas method to clearly capture what the problem and potential solution could be, including identifying alternative solutions that may already exist.</p>","tags":["WORKFORCE","BEST PRACTICE","OPEN DATA","COMPLETE"]},{"location":"our_work/ai-deep-dive/#outcomes-and-lessons-learned","title":"Outcomes and lessons learned","text":"<p>The workshops provided valuable insights to the NHS AI Lab Skunkworks team about the importance of group engagement. Having a diverse AI project team that includes people from technical, governance and frontline backgrounds is important to ensure you fully understand the problem you\u2019re trying to solve.</p> <p>The experience also demonstrated the value of discussing topics such as \u201cbuild or buy?\u201d The team at UHS were keen to invest wisely in any AI developments and to learn how to find out about existing tools. With so many AI applications already in existence, there may be tools you can use \u201coff the shelf\u201d or valuable lessons to learn from previous investigations into similar issues.</p> <p>The workshops gave us a good opportunity to stress the importance of using AI safely and ethically. The data you use and the testing and governance processes you apply must all result in AI that benefits all patients safely and ethically.</p> <p>As a result of the deep dive sessions:</p> <ul> <li>67% of participants felt more confident in their baseline understanding of AI and Machine Learning.</li> <li>71% of participants felt more confident in identifying potential solutions.</li> <li>60% of participants felt more confident in identifying the data needs of an AI project.</li> <li>75% of participants felt more confident conducting business and technical due diligence.</li> </ul> <p>The team at University Hospital Southampton also reported:</p> <ul> <li>a need for additional support when identifying and launching AI experiments</li> <li>the importance of diverse groups who represent different roles and teams in order to help the group explore the problem from different perspectives.</li> </ul>","tags":["WORKFORCE","BEST PRACTICE","OPEN DATA","COMPLETE"]},{"location":"our_work/ai-deep-dive/#_1","title":"AI Deep Dive Workshops","text":"","tags":["WORKFORCE","BEST PRACTICE","OPEN DATA","COMPLETE"]},{"location":"our_work/ai-dictionary/","title":"AI Dictionary","text":"<p>AI is full of acronyms and a common understanding of technical terms is often lacking.</p> <p>We decided to create a simple, open source, AI dictionary of terms with a health and care context to help level up those working in the field.</p>","tags":["WORKFORCE","DOCUMENTATION","WEBDEV","COMPLETE"]},{"location":"our_work/ai-dictionary/#results","title":"Results","text":"<p>A front-end website: https://nhsx.github.io/ai-dictionary written in HTML/CSS/JavaScript (frontend) with a JSON schema driven database of terms.</p> Output Link Open Source Code &amp; Documentation Github Case Study N/A Technical report N/A Algorithmic Impact Assessment N/A","tags":["WORKFORCE","DOCUMENTATION","WEBDEV","COMPLETE"]},{"location":"our_work/ai-dictionary/#_1","title":"AI Dictionary","text":"","tags":["WORKFORCE","DOCUMENTATION","WEBDEV","COMPLETE"]},{"location":"our_work/ai-ethics/","title":"AI Ethics in Practice at NHS England","text":"<p>Warning</p> <p>This project is currently in development, and as such the following is subject to change.</p>","tags":["ETHICS","BEST PRACTICE","DOCUMENTATION","RESEARCH","BEST PRACTICE","WIP"]},{"location":"our_work/ai-ethics/#background","title":"Background","text":"<p>AI presents transformative opportunities in healthcare, but the technology brings with it risks to people, the natural environment and society at large. NHS England has a responsibility to effectively harness AI to support the provision of better care to all patients.</p> <p>To do this, data scientists (among others) need to ensure ethical considerations are embedded in the development, evaluation and implementation of AI and data science projects more generally.</p> <p>However, there are few practical examples of what ethical AI in health looks like for those involved in its design and implementation. This is despite the proliferation of guidance from research institutions and other organisations.</p>","tags":["ETHICS","BEST PRACTICE","DOCUMENTATION","RESEARCH","BEST PRACTICE","WIP"]},{"location":"our_work/ai-ethics/#aim","title":"Aim","text":"<p>We have written a White Paper in which we identify the characteristics of ethical AI that data scientists should be concerned with at the NHS. This takes into account other actors in AI development at the NHS, including cybersecurity, assurance and governance colleagues.</p> <p>This paper proposes that data scientists specifically focus on trying to ensure four characteristics of AI: fair, transparent, value-adding and reliable.</p> <p>Our practical suggestions of how we can work towards embedding these characteristics in working practices include:</p> <ul> <li>Trialling tools and frameworks on live and emerging projects and to share learnings with the wider community. This will eventually constitute a portfolio of real examples that can inform future projects, similar to the use cases features in the OECD's Catalogue of Tools &amp; Metrics for Trustworthy AI.</li> <li>Coordinating a series of interactive workshops to build awareness of ethical risks, and to create and sustain a shared vocabulary to document and help mitigate these risks effectively along each project\u2019s lifecycle.</li> <li>Developing standardised resources that can be flexible to different types of data science projects, but ensure a minimum level of consideration and proportionate action.</li> <li>Mapping the development processes and involved actors of AI at the NHS and growing a platform to share knowledge and experiences. This will help us to identify how to incorporate ethical considerations into the data science lifecycle.</li> </ul>","tags":["ETHICS","BEST PRACTICE","DOCUMENTATION","RESEARCH","BEST PRACTICE","WIP"]},{"location":"our_work/ai-ethics/#outputs","title":"Outputs","text":"<p>We have:</p> <ul> <li>Developed a Model Card Template.</li> <li>Coordinated the publication of a record for the Automatic Moderation of Ratings &amp; Reviews project on the government's Algorithmic Transparency Recording Standard.</li> <li>Written a (currently internal) White Paper defining the scope of operationalising AI Ethics in NHS England.</li> <li>Applied the Data Hazards project to multiple internal projects to communicate potential harms of our work.</li> </ul>","tags":["ETHICS","BEST PRACTICE","DOCUMENTATION","RESEARCH","BEST PRACTICE","WIP"]},{"location":"our_work/ai-ethics/#in-progress","title":"In progress","text":"<p>We are currently working on:</p> <ul> <li>Completing the Turing Experts in Residence programme</li> <li>Continuing to use the data hazards to guide ethical considerations within team projects</li> </ul>","tags":["ETHICS","BEST PRACTICE","DOCUMENTATION","RESEARCH","BEST PRACTICE","WIP"]},{"location":"our_work/ai-ethics/#_1","title":"AI Ethics in Practice at NHS England","text":"","tags":["ETHICS","BEST PRACTICE","DOCUMENTATION","RESEARCH","BEST PRACTICE","WIP"]},{"location":"our_work/ai-skunkworks/","title":"NHS AI Lab Skunkworks","text":"<p>The AI Skunkworks programme was part of the NHS AI Lab. It finds new ways to use AI for driving forward the early adoption of technology to support health, in both clinical and business contexts. The team provides free short-term expertise and resources to public sector health and social care organisations to support AI projects and develop capability.</p> <p>The programme was closed to new projects at the end of 2022. This website captures the findings, source code, and lessons of its activities.</p>","tags":["POPULATION HEALTH","CLASSIFICATION","RESEARCH","OPEN DATA","PYTHON","COMPLETE"]},{"location":"our_work/ai-skunkworks/#how-we-work","title":"How we work","text":"<p>The NHS AI Lab Skunkworks team was built around the idea of short-term, rapid projects, aiming to investigate the use of AI for improving efficiency and accuracy in health and care.</p> <p>The programme aimed to facilitate a robust conversation around uses of AI in health and care, encouraging the community of healthcare AI practitioners share and discuss their experiences, documenting the findings and releasing any open source code produced.</p> <p>The Skunkworks' vision is that organisations in the health and care system will be able, through practical experience, to understand, build, buy, deploy, support, and challenge AI solutions. In order to achieve this, we have mostly ran projects in three ways, all centred around the idea of co-production:</p>","tags":["POPULATION HEALTH","CLASSIFICATION","RESEARCH","OPEN DATA","PYTHON","COMPLETE"]},{"location":"our_work/ai-skunkworks/#1-internal","title":"1. Internal","text":"<p>Utilising our internal team of data scientists and data / technology leads, we are able to run small agile projects that may involve one-to-one coaching in the use of python and machine learning frameworks, through to data discoveries to help assess whether the data your organisation possesses is suitable for an AI approach.</p>","tags":["POPULATION HEALTH","CLASSIFICATION","RESEARCH","OPEN DATA","PYTHON","COMPLETE"]},{"location":"our_work/ai-skunkworks/#2-short-term-resource","title":"2. Short term resource","text":"<p>We are able to sponsor individual contractors provided through the Public Sector Resourcing framework to assist in the conception or implementation of AI solutions for your organisation.</p>","tags":["POPULATION HEALTH","CLASSIFICATION","RESEARCH","OPEN DATA","PYTHON","COMPLETE"]},{"location":"our_work/ai-skunkworks/#3-with-a-supplier","title":"3. With a supplier","text":"<p>Partnering with the Home Office's Accelerated Capability Environment (ACE), we are able to sponsor 12 week agile projects using a pool of cutting edge AI suppliers.</p> <p>Health and care organisations can submit their proposed AI problem, and we will work with ACE to select and manage a supplier to deliver an AI proof of concept.</p> <p>These projects culminate with:</p> <ul> <li>Working code, published under an Open Source license on Github</li> <li>A technical report, detailing methodology and findings</li> </ul> <p>Co-production means that regardless of the project and the presence or absence of external contractors and partners, we would always work in a collaborative way, by bringing everyone involved around the table: data scientists, data engineers, experts of ML techniques, ethicists, regulation advisers, clinical and non-clinical NHS colleagues, and so on. The in-house AI Skunkworks team would always provide the steering and technical scrutiny to the project.</p>","tags":["POPULATION HEALTH","CLASSIFICATION","RESEARCH","OPEN DATA","PYTHON","COMPLETE"]},{"location":"our_work/ai-skunkworks/#capability-building","title":"Capability building","text":"<p>We also provide ad-hoc support, advice and education through initiatives such as our AI Deep Dive workshops, bringing an organisation through the opportunities and challenges of using AI, with an open mind about its implications and issues.</p>","tags":["POPULATION HEALTH","CLASSIFICATION","RESEARCH","OPEN DATA","PYTHON","COMPLETE"]},{"location":"our_work/ai-skunkworks/#get-in-touch","title":"Get in touch","text":"<p>As of 2023, you can still reach us via email at england.aiskunkworks@nhs.net</p>","tags":["POPULATION HEALTH","CLASSIFICATION","RESEARCH","OPEN DATA","PYTHON","COMPLETE"]},{"location":"our_work/ai-skunkworks/#_1","title":"NHS AI Lab Skunkworks","text":"","tags":["POPULATION HEALTH","CLASSIFICATION","RESEARCH","OPEN DATA","PYTHON","COMPLETE"]},{"location":"our_work/ambulance-delay-predictor/","title":"Ambulance Handover Delay Predictor","text":"<p>Ambulance Handover Delay Predictor was selected as a project in Q2 2022 following a successful pitch to the AI Skunkworks problem-sourcing programme.</p>","tags":["SECONDARY CARE","FORECASTING","MACHINE LEARNING","TIME SERIES","PYTHON"]},{"location":"our_work/ambulance-delay-predictor/#results","title":"Results","text":"<p>A proof-of-concept demonstrator written in Python (machine learning model, Jupyter Notebooks).</p> Output Link Open Source Code &amp; Documentation Github Technical report PDF","tags":["SECONDARY CARE","FORECASTING","MACHINE LEARNING","TIME SERIES","PYTHON"]},{"location":"our_work/ambulance-delay-predictor/#_1","title":"Ambulance Handover Delay Predictor","text":"","tags":["SECONDARY CARE","FORECASTING","MACHINE LEARNING","TIME SERIES","PYTHON"]},{"location":"our_work/bed-allocation/","title":"Bed allocation","text":"<p>Bed allocation was identified as a suitable opportunity for the AI Skunkworks programme in May 2021.</p>","tags":["SECONDARY CARE","FORECASTING","PYTHON","WEBDEV","SIMULATION","MODELLING","PSEUDONYMISED","COMPLETE"]},{"location":"our_work/bed-allocation/#results","title":"Results","text":"<p>A proof-of-concept demonstrator written in Python (backend, virtual hospital, models) and HTML/CSS/JavaScript (frontend).</p>","tags":["SECONDARY CARE","FORECASTING","PYTHON","WEBDEV","SIMULATION","MODELLING","PSEUDONYMISED","COMPLETE"]},{"location":"our_work/bed-allocation/#case-study","title":"Case Study","text":"<p>This is a backup of the case study published here on the NHS England Transformation Directorate website.</p> <p>Kettering General Hospital approached the NHS AI Lab Skunkworks team with a request for support exploring artificial intelligence (AI) to improve bed management. Their vision was to use AI to achieve the \u201cright patient, in the right bed, receiving the right care, at the right time.\u201d</p> <p>This 14-week project investigated the AI techniques that could be used to generate options for moving patients in a way that supports the human team to make the best decisions. The project aimed to provide a proof of concept tool that uses historic data to predict demand and make bed allocation suggestions to the bed management team - providing the open source code for further experimentation at the end of the project.</p>","tags":["SECONDARY CARE","FORECASTING","PYTHON","WEBDEV","SIMULATION","MODELLING","PSEUDONYMISED","COMPLETE"]},{"location":"our_work/bed-allocation/#overview","title":"Overview","text":"<p>Admitting patients into hospital is like a game of Tetris, or chess, where the allocation of each patient and bed can have a huge knock-on effect to the smooth-running of admissions and the welfare of patients. This scheduling of beds is managed by a human team who rely on individual expertise to deliver a system not unlike air traffic control, calculating the best arrangement with a continually changing set of demands and numbers of patients.</p> <p>The main challenges for managing hospital admissions were reported to be:</p> <ul> <li>Demand and capacity is complex. Not all patients are the same. Not all beds are the same.</li> <li>Staff are overwhelmed with options. Managing hundreds of beds and people presents too many choices.</li> <li>Expertise of staff, and therefore needs, varies.</li> </ul> <p>The NHS AI Lab Skunkworks funded and supported an AI investigation with the team at Kettering General Hospital, alongside Faculty, an AI specialist supplier provided through the Home Office\u2019s Accelerated Capability Environment (ACE).</p> <p>The work looked at whether AI could support better, faster decision-making, using a tool that would predict patient flow and provide bed allocation options for a human team to consider. The potential benefits being:</p> <ul> <li>high-quality, consistent bed allocation decisions</li> <li>improved patient experience</li> <li>improved workforce efficiency and staff satisfaction</li> <li>a reduction in the average number of patient moves per admission (and after-hours moves)</li> <li>reductions in inpatient length of stay</li> <li>improved problem solving capability within the team.</li> </ul>","tags":["SECONDARY CARE","FORECASTING","PYTHON","WEBDEV","SIMULATION","MODELLING","PSEUDONYMISED","COMPLETE"]},{"location":"our_work/bed-allocation/#what-we-did","title":"What we did","text":"<p>We ran a discovery phase in which the project team sought to understand the problem, the existing process and the constraints. We talked to others who are trying similar projects. The team also researched existing attempts to use AI for demand management and scheduling.</p>","tags":["SECONDARY CARE","FORECASTING","PYTHON","WEBDEV","SIMULATION","MODELLING","PSEUDONYMISED","COMPLETE"]},{"location":"our_work/bed-allocation/#creating-a-virtual-hospital","title":"Creating a virtual hospital","text":"<p>Following a robust information governance process, the team had access to 5 years\u2019 worth of historic pseudonymised data from the patient admission system (PAS) and 1 to 2 years of patient flow data. Pseudonymisation is a technique that separates data from direct identifiers (for example name, surname, NHS number) and replaces them with a pseudonym (for example, a reference number), so that identifying an individual from that data is not possible without additional information.</p> <p>Having assessed the data quality and analysed pre- and post COVID changes, they engineered training and test sets for modelling. This provided a virtual hospital environment with which to explore the use of AI.</p>","tags":["SECONDARY CARE","FORECASTING","PYTHON","WEBDEV","SIMULATION","MODELLING","PSEUDONYMISED","COMPLETE"]},{"location":"our_work/bed-allocation/#choosing-a-technical-approach","title":"Choosing a technical approach","text":"<p>For the forecasting component of the project, the team used a Bayesian modelling approach which used historical data to predict how many patients with specific characteristics would present at the hospital over time.</p> <p>The team then compared three approaches to allocating a bed: greedy allocation, Monte Carlo Tree Search (MCTS) and reinforcement learning.</p> <p>Greedy allocation allocates beds based on the best bed at the point of admission thereby making it a fast and less resource intensive method.</p> <p>The MCTS model operates by considering future events such as the number and nature of patients who are likely to arrive for admission within the next couple of hours, along with constraints of available beds, and then uses that information to allocate the best bed to a patient at the point of admission. This makes the model resource intensive and requires significant computing power to operate.</p> <p>Finally, a reinforcement learning approach was considered, which uses \u201cagents\u201d to maximise a reward over time, but this was not developed within the constraints of this 14 week project.</p>","tags":["SECONDARY CARE","FORECASTING","PYTHON","WEBDEV","SIMULATION","MODELLING","PSEUDONYMISED","COMPLETE"]},{"location":"our_work/bed-allocation/#building-a-user-interface","title":"Building a user interface","text":"<p>In order to provide staff with a usable and understandable front end, the team developed and tested a web-based user interface (UI) and integrated the allocation models.</p> <p>The team implemented the greedy allocation method in the user interface as it was the least resource intensive approach and able to provide an explainable allocation suggestion.</p> <p>The resulting proof of concept was then tested and reviewed by Kettering General Hospital.</p>","tags":["SECONDARY CARE","FORECASTING","PYTHON","WEBDEV","SIMULATION","MODELLING","PSEUDONYMISED","COMPLETE"]},{"location":"our_work/bed-allocation/#outcomes-and-lessons-learned","title":"Outcomes and lessons learned","text":"<p>The result is a proof of concept, created in 14 weeks, with a user interface that provides staff with the following:</p> <ul> <li>The ability to visualise a virtual hospital, showing current occupancy rates and forecasted demand for beds.</li> <li>A demonstration of what a fully developed allocation model could provide, making suggestions to the user along with an explanation.</li> <li>The ability to test the model on a wide range of patients with different attributes and associated constraints and validate the performance.</li> </ul> <p>This tool will help the likes of myself and others by supporting decision making. Support is the key word here, machine learning will support us to make these difficult bed allocation and patient decisions. \u2013 Digital Director, Kettering General Hospital NHS Foundation Trust</p> <p>I regularly hear that a bed is a bed and I know it\u2019s not ... But when you have those front door pressures, you can\u2019t get ambulances offloaded and I have beds in the wrong place - this is the time I need the real support, real time data, an automatic risk assessment that is generated for each patient. \u2013 Member of bed management staff, Kettering General Hospital</p> <p>There have been significant challenges with this project.</p>","tags":["SECONDARY CARE","FORECASTING","PYTHON","WEBDEV","SIMULATION","MODELLING","PSEUDONYMISED","COMPLETE"]},{"location":"our_work/bed-allocation/#data-quality","title":"Data quality","text":"<p>Attempting to get a total view of the trust\u2019s capacity and demand is complicated. In this example with Kettering General Hospital, there is no centralised patient flow information. Admission data would be needed for all specialties across the trust for the allocation algorithm to produce the best results.</p>","tags":["SECONDARY CARE","FORECASTING","PYTHON","WEBDEV","SIMULATION","MODELLING","PSEUDONYMISED","COMPLETE"]},{"location":"our_work/bed-allocation/#complexity-of-patient-needs","title":"Complexity of patient needs","text":"<p>The unique nature of patients\u2019 needs means taking into consideration a large number of complex combinations in order to achieve the best allocation decision.</p>","tags":["SECONDARY CARE","FORECASTING","PYTHON","WEBDEV","SIMULATION","MODELLING","PSEUDONYMISED","COMPLETE"]},{"location":"our_work/bed-allocation/#adapting-quickly-to-change","title":"Adapting quickly to change","text":"<p>In a real-world setting, the technology would need to be easily reconfigured by staff with new information about increased beds, changed ward layouts or flu admission peaks. There is currently limited ability to see the impact of changes like these.</p>","tags":["SECONDARY CARE","FORECASTING","PYTHON","WEBDEV","SIMULATION","MODELLING","PSEUDONYMISED","COMPLETE"]},{"location":"our_work/bed-allocation/#what-next","title":"What next?","text":"<p>Kettering General Hospital will be working with Faculty to bid for further funding to develop and operationalise the bed allocation system. This will aim to build connections to patient data in real-time, refining the algorithm, and understanding how the allocation tool can be integrated into site management practises.</p>","tags":["SECONDARY CARE","FORECASTING","PYTHON","WEBDEV","SIMULATION","MODELLING","PSEUDONYMISED","COMPLETE"]},{"location":"our_work/bed-allocation/#who-was-involved","title":"Who was involved?","text":"<p>This project is a collaboration between NHSX, Kettering General Hospital NHS Trust, Faculty and the Home Office\u2019s Accelerated Capability Environment (ACE). The AI Lab Skunkworks exists within the NHS AI Lab to support the health and care community to rapidly progress ideas from the conceptual stage to a proof of concept.</p> <p>The NHS AI Lab is working with the Home Office programme: Accelerated Capability Environment (ACE) to develop some of its skunkworks projects, providing access to a large pool of talented and experienced suppliers who pitch their own vision for the project.</p> <p>Accelerated Capability Environment (ACE) is part of the Homeland Security Group within the Home Office. It provides access to more than 250 organisations from across industry, academia and the third sector who collaborate to bring the right blend of capabilities to a given challenge. Most of these are small and medium-sized enterprises (SMEs) offering cutting-edge specialist expertise.</p> <p>ACE is designed to bring innovation at pace, accelerating the process from defining a problem to developing a solution and delivering practical impact to just 10 to 12 weeks.</p> <p>Faculty is an applied AI company that helps build and accelerate an organisation's AI capability. They offer a range of software and services solutions. Faculty works with a number of high-profile brands globally as well as government departments and agencies.</p> Output Link Open Source Code &amp; Documentation Github Case Study Case Study Technical report PDF","tags":["SECONDARY CARE","FORECASTING","PYTHON","WEBDEV","SIMULATION","MODELLING","PSEUDONYMISED","COMPLETE"]},{"location":"our_work/bed-allocation/#_1","title":"Bed allocation","text":"","tags":["SECONDARY CARE","FORECASTING","PYTHON","WEBDEV","SIMULATION","MODELLING","PSEUDONYMISED","COMPLETE"]},{"location":"our_work/c245_synpath/","title":"Building the Foundations for a Generic Patient Simulator (SynPath)","text":"Figure 1: Overview of the Synpath data model <p>A data model (\u201cPatient Agent\u201d) was developed for fake patients to be defined in the simulation.  The patient is then assigned a health record (conditions, medications,  ..) with optional additional attributes.</p> <p>Interacting this data model over time with an environment layer (representation of the physical and abstract health system components that the patient can interact with (e.g., GP practice, multidisciplinary team meeting)) creates a patient record with appointment times, updates to health status, and changes in medications prescribed.</p>","tags":["SYNTHETIC DATA","SIMULATION","GENERATION","STRUCTURED DATA","PYTHON","PAUSED"]},{"location":"our_work/c245_synpath/#results","title":"Results","text":"<p>Foundations were built for the data model and environment.</p> <p>During the development it became clear that a key nature to be included for healthcare agent simulations in the NHS is the distinction between active and passive agents in regards to the choice of the next environment interaction point.   The spatial location of services was less important than in a typical agent-based simulation as the timescales made these considerations redundant.</p> <p>Efficient object communication and concurrency were also highlighted needing significant further development.</p> Output Link Open Source Code &amp; Documentation Github Case Study Awaiting Sign-Off Technical report Here","tags":["SYNTHETIC DATA","SIMULATION","GENERATION","STRUCTURED DATA","PYTHON","PAUSED"]},{"location":"our_work/c245_synpath/#_1","title":"Building the Foundations for a Generic Patient Simulator (SynPath)","text":"","tags":["SYNTHETIC DATA","SIMULATION","GENERATION","STRUCTURED DATA","PYTHON","PAUSED"]},{"location":"our_work/c250_nhscorpus/","title":"Considerations for Building a Language Corpus with a Focus on the NHS","text":"Figure 1: Open source tools used in each functional setting <p>We aimed to explore how to build an Open, Representative, Extensible and Useful set of tools to curate, enrich and share sources of healthcare text data in an appropriate manner.</p>","tags":["NATURAL LANGUAGE PROCESSING","RESEARCH","UNSTRUCTURED DATA","TEXT DATA","OPEN DATA","PYTHON","PAUSED","EXPERIMENTAL"]},{"location":"our_work/c250_nhscorpus/#results","title":"Results","text":"<p>Whilst a tool stack was developed which achieved many of our objectives, the key learning points were around the knowledge gaps which need to be addressed at both a data and tooling level before bringing these data together becomes achievable.</p> Output Link Open Source Code &amp; Documentation Github Case Study n/a Blog Here","tags":["NATURAL LANGUAGE PROCESSING","RESEARCH","UNSTRUCTURED DATA","TEXT DATA","OPEN DATA","PYTHON","PAUSED","EXPERIMENTAL"]},{"location":"our_work/c250_nhscorpus/#_1","title":"Considerations for Building a Language Corpus with a Focus on the NHS","text":"","tags":["NATURAL LANGUAGE PROCESSING","RESEARCH","UNSTRUCTURED DATA","TEXT DATA","OPEN DATA","PYTHON","PAUSED","EXPERIMENTAL"]},{"location":"our_work/c338_poud/","title":"How to Assess the Privacy of Unstructured Data","text":"Figure 1: Figure 4 from Al-Fedaghi, Sabah. (2012). Experimentation with Personal Identifiable Information. Showing an example PII sphere from different perspectives (compound, singleton and multitude personal identifiable information). <p>Unstructured data (e.g. text, image, audio) makes up a significant quantity of NHS data but is comparatively underused as an evidence source for analysis. This is often due to the privacy concerns restricting the sharing and use of these data.</p> <p>To our knowledge, there are currently no tools on the market that allows the NHS to robustly ascertain the level of privacy of unstructured data. To have confidence when commissioning tooling for anonymisation purposes the NHS needs an understanding of what private content, health related text data can contain. The tooling put in place to protect the privacy of these data needs to be able to assess the content, evaluate the risk associated with the content, and demonstrate that the tooling functionality has dealt with any privacy concerns appropriately.</p> <p>During this time the three main activities were a literature review, bringing a range of expert and voices together into a workshop, and writing the associated report summarising our understanding of the problem.</p>","tags":["UNSTRUCTURED DATA","ETHICS","PATIENT IDENTIFIABLE DATA","BEST PRACTICE","NATURAL LANGUAGE PROCESSING","RESEARCH","TEXT DATA","OPEN DATA","PYTHON","COMPLETE"]},{"location":"our_work/c338_poud/#results","title":"Results","text":"<p>The main output specified was for a list of key qualities that could feed a tool specification in the future.  The qualities the report highlighted were:</p> <p>Structuring and data handling</p> <ul> <li>Ability to flag and identify with the range of possible data issues prior to de-identification (misspellings, medical terms, acronyms)</li> <li>Connection with a clinical vocabulary in order to match and assist word identification to assist structuring of the data.</li> <li>Ability to flag the data variables required for anonymisation to assist in the risk analysis and disclosure control process</li> <li>Ability to deal with unstructured, semi-structured and structured data</li> <li>Ability to deal with different formats of free-text data e.g. medical notes, patient feedback, survey responses, research papers</li> </ul> <p>Tool Use &amp; Validation</p> <ul> <li>Ease of manual manipulation in order to react to the level of anonymisation required and the key variables to be maintained for data utility</li> <li>Automated auditing of the flagged terms, any data manipulation and tool manipulation that has taken place</li> <li>Ability to demonstrate quality and anonymisation level before and after each stage of the de-identification process for the QC process.</li> <li>Ability to apply manual QC at each step along the process QC (human and automated) or the requirement of human authorisation to move to the next step</li> <li>Clarity around the tool limitations</li> <li>Need to align with the Information Governance (IG) process</li> </ul> <p>Context</p> <ul> <li>Ability to tune into a domain extracting and utilising the appropriate medical dictionary. b. Clarity around individual versus population</li> <li>Ability to define level of anonymisation</li> </ul> <p>Flexibility</p> <ul> <li>Ability to adapt the anonymisation functionality to the risk level assessed</li> <li>Flexibility within the tool programming to adapt to the utility required and hence the purpose of the output data aligning the appropriate level of de-identification</li> <li>Incorporated regular updating and reaction to current \u201cthreats\u201d</li> </ul> Output Link Open Source Code &amp; Documentation n/a Case Study Awaiting Sign-Off Technical report Here","tags":["UNSTRUCTURED DATA","ETHICS","PATIENT IDENTIFIABLE DATA","BEST PRACTICE","NATURAL LANGUAGE PROCESSING","RESEARCH","TEXT DATA","OPEN DATA","PYTHON","COMPLETE"]},{"location":"our_work/c338_poud/#_1","title":"How to Assess the Privacy of Unstructured Data","text":"","tags":["UNSTRUCTURED DATA","ETHICS","PATIENT IDENTIFIABLE DATA","BEST PRACTICE","NATURAL LANGUAGE PROCESSING","RESEARCH","TEXT DATA","OPEN DATA","PYTHON","COMPLETE"]},{"location":"our_work/c339_sas/","title":"Creating a Generic Adversarial Attack for any Synthetic Dataset","text":"<p>Figure 1: Attack diagrams for the currently incorporated scenarios. Scenario 1: Access to the synthetic dataset and a description of the generative model\u2019s architecture and training procedure. Scenario 2: Access to a black box model that can provide unlimited synthetic data, with data realistic of the training distribution gathered by the attacker, which may be an example synthetic dataset released by the researchers.</p> <p>An extensible code was developed to apply a suite of adversarial attacks to synthetically generated single table tabular data in order to assess the likely success of attacks and act as a privacy indicator for the dataset.  Using this information then informs the generation and information governance process to ensure the safety of our data.</p>","tags":["SYNTHETIC DATA","STRUCTURED DATA","ETHICS","DATA VALIDATION","STRUCTURED DATA","PYTHON","COMPLETE"]},{"location":"our_work/c339_sas/#results","title":"Results","text":"<p>The code-base was successfully developed with code injection points for extensibility.  Unfortunately, as the code could be used as an active attack on a dataset, we have decided not not to make the codebase public but instead aiming to both extend the number of attacks and incorporate the code in our synthetic generation process.</p> Output Link Open Source Code &amp; Documentation restricted Case Study Awaiting Sign-Off Technical report Blod","tags":["SYNTHETIC DATA","STRUCTURED DATA","ETHICS","DATA VALIDATION","STRUCTURED DATA","PYTHON","COMPLETE"]},{"location":"our_work/c339_sas/#_1","title":"Creating a Generic Adversarial Attack for any Synthetic Dataset","text":"","tags":["SYNTHETIC DATA","STRUCTURED DATA","ETHICS","DATA VALIDATION","STRUCTURED DATA","PYTHON","COMPLETE"]},{"location":"our_work/c399_privfinger/","title":"Building a Tool to Assess the Privacy Risk of Text Data","text":"<p>This work was undertaken as an external commission aiming to build a pipeline of components which firstly generated unstructured medical notes using a structured output from Synthea and then running these through GPT-3.5 models to transform these into human readable notes.</p> <p>These notes were then processed using named entity recognition to extract pre-defined identifiers and store these in a structured form.  The algorithm pycorrect match was then implemented to give a privacy risk score of reidentification from the identifiers.</p> <p>Shap analysis was then conducted to understand which components of an individual record and of the dataset as a whole had the highest risk of privacy leakage.</p> <p>This pipeline could then be run before and after a de-identification process has taken place to understand the impact of the process on the risk score and to generate confidence that the dataset has been appropriately processed for use.</p>","tags":["LLM","PYTHON","NATURAL LANGUAGE PROCESSING","LLM","RESEARCH","DATA VALIDATION","UNSTRUCTURED DATA","TEXT DATA","PYTHON","PAUSED","SYNTHETIC DATA"]},{"location":"our_work/c399_privfinger/#results","title":"Results","text":"<p>During the 10 week project the end-to-end code was developed, tested and delivered.  However, key components are dependent on commercial offerings and only the first (of three) levels of identifiers was tested in the setup.</p> <p>Future work needs to replace some components with open source versions and a large number of experiments needs to be investigated to understand the limitations and where further development would be useful.</p> <p>This is an ongoing piece of work.</p> Output Link Open Source Code &amp; Documentation Coming Soon Case Study Coming Technical report Coming Soon","tags":["LLM","PYTHON","NATURAL LANGUAGE PROCESSING","LLM","RESEARCH","DATA VALIDATION","UNSTRUCTURED DATA","TEXT DATA","PYTHON","PAUSED","SYNTHETIC DATA"]},{"location":"our_work/c399_privfinger/#_1","title":"Building a Tool to Assess the Privacy Risk of Text Data","text":"","tags":["LLM","PYTHON","NATURAL LANGUAGE PROCESSING","LLM","RESEARCH","DATA VALIDATION","UNSTRUCTURED DATA","TEXT DATA","PYTHON","PAUSED","SYNTHETIC DATA"]},{"location":"our_work/cancer_high_risk_cohorts/","title":"Cancer high-risk cohorts","text":"<p>Identification of cohorts at higher risk of cancer can enable earlier diagnosis of the disease, which significantly improves patient outcomes. In this project, we use machine learning to predict cancer diagnosis in the next year. We select nine cancer sites with high incidence of late-stage diagnosis or worsening survival rates, and where there are currently no national screening programmes. We use National Health Service (NHS) data from medical helplines (NHS 111) and secondary care appointments from all hospitals in England. We present an approach of constructing cohorts at higher risk of cancer based on feature importance and considering possible bias in model results. These outputs can be used to develop highly targeted case finding services, which could help increase earlier detection rates and reduce health disparities.</p>","tags":["POPULATION HEALTH","RAP","MACHINE LEARNING","PYTHON","STRUCTURED DATA","WIP"]},{"location":"our_work/cancer_high_risk_cohorts/#results","title":"Results","text":"<p>A dataset comprising 23.6 million individuals aged between 40 and 74 in England was compiled, integrating various datasets including the National Bridges to Health Segmentation Dataset, Secondary Use Services (SUS) data, Emergency Care Data Set (ECDS), NHS 111 calls data, as well as ONS mortality data. Features capturing healthcare interactions (e.g. number of 111 calls, number of hospital attendances), demographic, socioeconomic, and clinical diagnosis variables were developed. </p>","tags":["POPULATION HEALTH","RAP","MACHINE LEARNING","PYTHON","STRUCTURED DATA","WIP"]},{"location":"our_work/cancer_high_risk_cohorts/#patient-pathway","title":"Patient pathway","text":"<p>Our patient histories cover six years between 2016-2022. We use the first 5 years of the patient histories to predict cancer diagnosis in year 6. We focus on nine cancers (bladder, head and neck, kidney, lymphoma, myeloma, oesophageal, ovarian, pancreatic, and stomach), which are associated with a high proportion of late-stage diagnoses (stage III and IV) or worsening survival rates in England, and don\u2019t currently have screening programmes. </p>","tags":["POPULATION HEALTH","RAP","MACHINE LEARNING","PYTHON","STRUCTURED DATA","WIP"]},{"location":"our_work/cancer_high_risk_cohorts/#feature-importance","title":"Feature importance","text":"<p>For each cancer site we identified which factors are most important for predicting those at high risk of developing cancer. As an example, we show the results for bladder cancer. In the figure below we show the SHAP values for the top 20 features (our models include more than 800 features in total) \u2013 ordered based on the gain metric (the average gain across all splits where the feature is used) for our model. SHAP calculates the contribution of each variable to the model predicted probability output. The red colour indicates higher values for the selected feature, and a positive SHAP value means an increase in the risk of cancer. For example, higher age (red colour) has overwhelmingly positive SHAP values, which means that higher age is predicting higher risk of bladder cancer in the next year. </p> <p></p> <p>We observe that beyond age and gender, several comorbidities appear as relevant predictors of a bladder cancer diagnosis in ways that are consistent with expectations based on the medical literature. For example, the presence of chronic obstructive pulmonary disease (COPD) and urinary infections is associated with the incidence of bladder cancer in previous research.</p> <p>In addition, several features drawn from the NHS 111 calls dataset appear to be good predictors of bladder cancer incidence. For example, higher number of calls to NHS 111 lines reporting cancer related symptoms is one of the features with the highest gain metric value (just below demographics and long-term condition status). In addition, we also see that features capturing specific symptoms that are plausibly related to undiagnosed bladder cancer are also relevant and have the expected direction of effect. Specifically, higher number of calls to NHS 111 lines reporting \u201cpain and frequency of passing urine\u201d or \u201cblood in urine\u201d (during the last year) are both relevant predictors of risk of being diagnosed with bladder cancer in the next year. </p>","tags":["POPULATION HEALTH","RAP","MACHINE LEARNING","PYTHON","STRUCTURED DATA","WIP"]},{"location":"our_work/cancer_high_risk_cohorts/#cohort-construction","title":"Cohort construction","text":"<p>We select relevant features based on feature importance and data availability. The most important features, which were in the top 20 of model gain and SHAP value, were selected. SHAP was also used to identify the direction of the feature. Features which had a positive impact on the model output (i.e. which tended to increase the risk if the feature was present) were selected. This method has been applied to the whole population, and to sub-groups of demographic strata, demonstrating how the approach can be used for targeted screening, as shown in the figure below. </p> <p></p> <p>In the table below, we show  examples of such curated cohorts based on combinations of just two features among those that the model considers as high importance for predicting cancer incidence in the next year. We define a lift value as the ratio of the cancer incidence within the cohort to the baseline cancer incidence. The baseline cancer incidence in our case, refers to those aged between 40-74 with no previous cancer diagnoses.</p> <p>The cohort with highest cancer incidence, and a size of at least 10,000, is constructed based on interactions with the 111-call service and includes a specific bladder cancer related symptom of blood in urine. This cohort has a cancer incidence of 1 in 82, compared to 1 in 3355 in the study population. This means the cohort has a lift value of 41 - i.e. the cancer incidence within this cohort is 41 times higher than the overall incidence in the analysis population. </p> <p>Larger cohorts of high-risk patients are constructed with flags relating to comorbidities of the genitourinary system and other diseases of the urinary system. Applying these flags to the population results in a cohort size of approximately 100,000 individuals, with a cancer rate 6 times higher than in the overall study population. An example of a larger cohort of ~290,000 individuals would be constructed by applying the filter of patients having at least one long-term condition, and a diagnosis relating to symptoms and signs involving the genitourinary system in the last 5 years. This results in a cohort with a lift value of 4.5. </p> Feature combination Population size Incidence in cohort (%) Lift value At least one call reporting cancer related symptoms in last year AND at least one call reporting blood in urine in last year 16,700 1.2% 41 Diagnosis of \u201cSymptoms and signs involving the genitourinary system\u201d (ICD10 R30-R39) in the last 5 years AND Diagnosis of \u201cOther diseases of the urinary system\u201d (ICD10 N30-N39) in last 5 years 98,300 0.18% 6 Has a long-term condition AND Diagnosis of \u201cSymptoms and signs involving the genitourinary system\u201d (ICD10 R30-R39) in the last 5 years 290,000 0.14% 4.5","tags":["POPULATION HEALTH","RAP","MACHINE LEARNING","PYTHON","STRUCTURED DATA","WIP"]},{"location":"our_work/cancer_high_risk_cohorts/#consideration-of-bias-in-results","title":"Consideration of bias in results","text":"<p>Machine learning models may be biased towards predicting higher risk for certain demographic groups, making the high-risk cohort non representative of the actual incidence of bladder cancer in the population. For example, the SHAP feature importance plots show that higher age, male, and white ethnicity all tend to increase the model risk score. This does correspond with higher incidence of bladder cancer in this group, however, given the low counts of bladder cancer among other demographic groups, it is difficult to ensure fair representation of all strata when constructing high-risk cohorts. </p> <p>We sought to mitigate this by segmenting the population into demographic groups to investigate if different sets of features can create higher risk cohorts across demographic strata. The segmentation was based on gender (male/female) and broad ethnicity (White/Non-white), resulting in four groups. Due to the low incidence of bladder cancer, more granular segmentation would have resulted in very small sample sizes. For each population segment, the same methodology as described above was applied, with the cohort with the highest incidence of cancer cases being identified. These decision rules were then applied to the test dataset to evaluate the efficacy of the cohort. The lift value was calculated based on the incidence of cancer for each stratum. </p> Demographic strata Feature combination Cohort size Incidence in cohort (%) Lift value Female \u2013 non white ethnicity At least 1 A&amp;E attendance in the last year AND diagnosis of a long term condition 195000 0.01% 2.9 Female \u2013 white ethnicity At least one call reporting cancer related symptoms in last year AND at least one call reporting blood in urine in last year 6800 0.8% 47.5 Male \u2013 non white ethnicity At least 1 A&amp;E attendance in the last year AND diagnosis of COPD 7000 0.07% 4.9 Male \u2013white ethnicity At least one call reporting cancer related symptoms in last year AND at least one call reporting blood in urine in last year 7600 1.9% 36 <p>For the white ethnicity group, features related to 111 calls are particularly effective in identifying high-risk groups. The specific nature of the symptom information (blood in urine) can result in small cohorts with lift values of 47.5 for white females, and 36 for white males. In contrast, for the non-white ethnic group, more general health factors (e.g. A&amp;E attendance) and comorbidities (e.g. COPD) result in the highest risk groups. These cohorts are still significantly higher in cancer incidence compared to baseline rates for these populations, as shown by the lift values of 2.9 for females, and 4.9 for males. However, they are also significantly lower than the lift values obtained for the white ethnic group. This likely reflects health inequalities in the utilisation of services such as 111 calls. </p>","tags":["POPULATION HEALTH","RAP","MACHINE LEARNING","PYTHON","STRUCTURED DATA","WIP"]},{"location":"our_work/cancer_high_risk_cohorts/#teams","title":"Teams","text":"<p>This project is a collaboration between the NHS England Data Science, Strategic Analysis and Cancer Programme teams.</p>","tags":["POPULATION HEALTH","RAP","MACHINE LEARNING","PYTHON","STRUCTURED DATA","WIP"]},{"location":"our_work/cancer_high_risk_cohorts/#links","title":"Links","text":"Output Link Constructing multicancer risk cohorts using national data from medical helplines and secondary care Pre-print Data Processing pipeline Github Presentation at Health and Care Analytics Conference 2024 YouTube Presentation at RPYSOC Conference 2024 Youtube","tags":["POPULATION HEALTH","RAP","MACHINE LEARNING","PYTHON","STRUCTURED DATA","WIP"]},{"location":"our_work/cancer_high_risk_cohorts/#_1","title":"Cancer high-risk cohorts","text":"","tags":["POPULATION HEALTH","RAP","MACHINE LEARNING","PYTHON","STRUCTURED DATA","WIP"]},{"location":"our_work/casestudy-recruitment-shortlisting/","title":"Examining whether recruitment data can, and should, be used to train AI models for shortlisting interview candidates","text":"","tags":["WORKFORCE","NATURAL LANGUAGE PROCESSING","LLM","NEURAL NETWORKS","RESEARCH","SYNTHETIC DATA","STRUCTURED DATA","PYTHON","COMPLETE"]},{"location":"our_work/casestudy-recruitment-shortlisting/#info","title":"Info","text":"<p>This is a backup of the case study published here on the NHS England Transformation Directorate website.</p>","tags":["WORKFORCE","NATURAL LANGUAGE PROCESSING","LLM","NEURAL NETWORKS","RESEARCH","SYNTHETIC DATA","STRUCTURED DATA","PYTHON","COMPLETE"]},{"location":"our_work/casestudy-recruitment-shortlisting/#case-study","title":"Case Study","text":"<p>Recruitment is a complex process where many factors need to be considered and understood so that the right candidates can be shortlisted for an interview. As well as that, it is a time-consuming process. However, the information provided in a job application has the potential to lead to bias if not handled correctly. This can happen with human shortlisters, but extra care must be taken if a machine (e.g. AI) is involved that is entirely unaware of what is and is not sensitive.</p> <p>The challenge Can we identify where bias has potential to occur when using machine learning for shortlisting interview candidates as part of the NHS England recruitment process? How does this manifest, and how can it be mitigated?</p>","tags":["WORKFORCE","NATURAL LANGUAGE PROCESSING","LLM","NEURAL NETWORKS","RESEARCH","SYNTHETIC DATA","STRUCTURED DATA","PYTHON","COMPLETE"]},{"location":"our_work/casestudy-recruitment-shortlisting/#overview","title":"Overview","text":"<p>Recruitment in the NHS involves identifying the best candidates for a wide array of jobs, where the skills typically vary significantly between roles. The process is very time consuming, with applications, CVs and other supporting documents reviewed to identify the best candidates. These candidates are then \u2018shortlisted\u2019 for an interview, which is the next step in the process. This shortlisting is done by often highly experienced hiring managers, who have a nuanced understanding of what makes a good candidate. However, the fact that humans are involved in this review process means decisions taken at this stage can vary, from person to person as well as between hiring rounds.</p> <p>One way to gain some consistency might be to leverage artificial intelligence (AI), specifically machine learning, to undertake this process. The purpose of this project was to review the feasibility and consistency of this in the context of the NHS.</p> <p>What do we mean by bias? There are many ways to define bias depending on whether it is in relation to the data, design, outcomes or implementation of AI. In this piece of work, bias was investigated in the representativeness of the data set and the results of the predictive model using the distribution and balance of shortlisted candidates across groups of protected characteristics in those who applied.</p> <p>When talking about bias by the predictive model, the model was determined to have shown \u2018bias\u2019 if the errors made in prediction were larger than an accepted error rate (which is defined by the person carrying out the work).</p> <p>Bias can also be identified by looking at integrity of the source data (looking at factors such as the way it was collected) or sufficiency (see here) of the data</p> <p></p> <p>Figure 1: Graph of the counts and proportions of candidates shortlisted or not shortlisted, by grade and ethnicity.</p>","tags":["WORKFORCE","NATURAL LANGUAGE PROCESSING","LLM","NEURAL NETWORKS","RESEARCH","SYNTHETIC DATA","STRUCTURED DATA","PYTHON","COMPLETE"]},{"location":"our_work/casestudy-recruitment-shortlisting/#_1","title":"Examining whether recruitment data can, and should, be used to train AI models for shortlisting interview candidates","text":"","tags":["WORKFORCE","NATURAL LANGUAGE PROCESSING","LLM","NEURAL NETWORKS","RESEARCH","SYNTHETIC DATA","STRUCTURED DATA","PYTHON","COMPLETE"]},{"location":"our_work/casestudy-synthetic-data-pipeline/","title":"Exploring how to create mock patient data (synthetic data) from real patient data","text":"","tags":["SYNTHETIC DATA","GENERATION","NEURAL NETWORKS","STRUCTURED DATA","PATIENT IDENTIFIABLE DATA","SYNTHETIC DATA","PYTHON","COMPLETE"]},{"location":"our_work/casestudy-synthetic-data-pipeline/#info","title":"Info","text":"<p>This is a backup of the case study published here on the NHS England Transformation Directorate website.</p>","tags":["SYNTHETIC DATA","GENERATION","NEURAL NETWORKS","STRUCTURED DATA","PATIENT IDENTIFIABLE DATA","SYNTHETIC DATA","PYTHON","COMPLETE"]},{"location":"our_work/casestudy-synthetic-data-pipeline/#case-study","title":"Case Study","text":"<p>The NHS AI Lab Skunkworks team has been releasing open-source code from their artificial intelligence (AI) projects since 2021. One of the challenges faced with releasing code is that without suitable test data it is not possible to properly demonstrate AI tools, preventing users without data access from being able to see the tool in action.</p> <p>One avenue for enabling this is to provide \u201csynthetic data\u201d, where new \u201cfake\u201d data is generated from real data using a specifically designed model, in a way that maintains several characteristics of the original data. In particular, synthetic data aims to achieve:</p> <ul> <li>Utility - the synthetic data must be fit for its defined use.</li> <li>Quality - it must be a sufficient representation of the real data.</li> <li>Privacy - it mustn\u2019t \u2018leak\u2019 or expose any sensitive information from the real data.</li> </ul> <p>The challenge This project aimed to provide others with a simple, re-usable way of generating safe and effective synthetic data to be used in technologies that improve health and social care.</p>","tags":["SYNTHETIC DATA","GENERATION","NEURAL NETWORKS","STRUCTURED DATA","PATIENT IDENTIFIABLE DATA","SYNTHETIC DATA","PYTHON","COMPLETE"]},{"location":"our_work/casestudy-synthetic-data-pipeline/#the-challenge","title":"The challenge","text":"<p>Using real patient data for research and development carries with it safety and privacy concerns about the anonymity of the people behind the information. Various anonymisation techniques can be used to turn data into a form that does not directly identify individuals and where re-identification is not likely to take place. However, it is very difficult to entirely remove the chance of re-identification. Wide release of anonymised data will always carry some risks. Synthetic data aims to remove the need for such concerns because there is no \u201creal patient\u201d connected with the data.</p> <p>There are many ways to generate synthetic data. One common challenge with synthetic data approaches is that they are usually configured specifically for a dataset. This is a problem because it means a significant amount of work is needed to update them for use with a different data source.</p> <p>Additionally, once data has been produced, it can be difficult to know whether it is actually useful.</p> <p>In a partnership project, the NHS Transformation Directorate\u2019s Analytics Unit and the NHS AI Lab Skunkworks team sought to further improve an existing synthetic data generation model (called SynthVAE) and develop a framework for generating synthetic data that could be shared for others to re-use.</p>","tags":["SYNTHETIC DATA","GENERATION","NEURAL NETWORKS","STRUCTURED DATA","PATIENT IDENTIFIABLE DATA","SYNTHETIC DATA","PYTHON","COMPLETE"]},{"location":"our_work/casestudy-synthetic-data-pipeline/#the-teams-explored-how-synthvae-could-be-used-to-generate-synthetic-data-how-that-data-would-be-evaluated-and-how-the-whole-process-could-be-documented-for-others-to-re-use","title":"The teams explored how SynthVAE could be used to generate synthetic data, how that data would be evaluated and how the whole process could be documented for others to re-use","text":"<p>If you would like greater technical detail about this project, please read the version on the Skunkworks Github website.</p> <p>They sought to:</p> <ul> <li>increase the range of synthetic data types that SynthVAE can generate</li> <li>create a standard series of checks that can be carried out on the data produced, so that people can better understand its characteristics</li> <li>implement a structure to allow users to run the full functionality with a single piece of code.</li> </ul> <p>To be able to increase SynthVAE\u2019s range of capabilities, the teams needed an input dataset containing a number of different data types in order to broaden the range of the data produced.</p> <p>The teams chose to work from a starting dataset that was already in the public domain. This meant people wishing to use the code after release could access and use the same dataset with which the project was developed. MIMIC-III was selected because the size and variety of its data would enable them to produce an input file that would closely match the broad range of typical hospital data.</p> <p>From the raw MIMIC-III files, they produced a single dataset containing treatment provided by a hypothetical set of patients. It looked similar to datasets that might be encountered in a real hospital setting, helping to keep this project as relevant as possible to anyone wishing to explore the use of synthetic data for health and care.</p>","tags":["SYNTHETIC DATA","GENERATION","NEURAL NETWORKS","STRUCTURED DATA","PATIENT IDENTIFIABLE DATA","SYNTHETIC DATA","PYTHON","COMPLETE"]},{"location":"our_work/casestudy-synthetic-data-pipeline/#1-adapting-synthvae","title":"1. Adapting SynthVAE","text":"<p>SynthVAE was originally written primarily to generate synthetic data from both continuous data (data with an infinite number of values) and categorical data (data that can be divided into groups). The inclusion of other data types (like dates) in the new input dataset meant SynthVAE needed to be adapted to take the new set of variables.</p>","tags":["SYNTHETIC DATA","GENERATION","NEURAL NETWORKS","STRUCTURED DATA","PATIENT IDENTIFIABLE DATA","SYNTHETIC DATA","PYTHON","COMPLETE"]},{"location":"our_work/casestudy-synthetic-data-pipeline/#2-producing-synthetic-data","title":"2. Producing synthetic data","text":"<p>Having sourced suitable data and created a useful input file, it was possible to use the input file to train a SynthVAE model that could generate synthetic data. The model was used to generate a synthetic dataset containing several million entries, a substantial increase on volumes previously produced using SynthVAE.</p> <p>This wasn\u2019t without challenges, as SynthVAE hadn\u2019t been substantially tested using dates or large volumes of data. However, SynthVAE was successfully adapted to produce a synthetic version of the input data from MIMIC-III.</p>","tags":["SYNTHETIC DATA","GENERATION","NEURAL NETWORKS","STRUCTURED DATA","PATIENT IDENTIFIABLE DATA","SYNTHETIC DATA","PYTHON","COMPLETE"]},{"location":"our_work/casestudy-synthetic-data-pipeline/#3-creating-a-checking-process","title":"3. Creating a checking process","text":"<p>In order to evaluate the privacy, quality and utility of the synthetic data produced, a set of checks were needed. There is currently no industry standard, so the teams chose a range of evaluation approaches designed to provide the broadest possible assessment of the data.</p> <p>The process aimed to check whether the synthetic data was a good substitute for the real data, without causing a change in performance (also known as the utility). The additional checks that were added aimed to make the evaluation of utility more robust, for example by checking there are no identical records in the synthetic and real datasets, but also to provide visual aids to allow the user to see what differences are present in the data.</p> <p>These checks were combined and their results collected in a web-based report, to allow results to be packaged and shared with any data produced.</p>","tags":["SYNTHETIC DATA","GENERATION","NEURAL NETWORKS","STRUCTURED DATA","PATIENT IDENTIFIABLE DATA","SYNTHETIC DATA","PYTHON","COMPLETE"]},{"location":"our_work/casestudy-synthetic-data-pipeline/#4-creating-a-pipeline","title":"4. Creating a pipeline","text":"<p>Finally, the teams pulled these steps into a single workflow process for others to follow.</p> <p>The input data generation, SynthVAE training, synthetic data production and output checking processes were chained together, creating a single flow to train a model, produce synthetic data and then evaluate the final output.</p> <p>To make the end-to-end process as user-friendly as possible, a pipelining library called QuantumBlack\u2019s Kedro was employed. This allowed each step in the workflow to be linked to the next, meaning users can run all parts of the process with a single command. It also gives users the ability to control the definitions within the pipeline and change it according to their needs.</p>","tags":["SYNTHETIC DATA","GENERATION","NEURAL NETWORKS","STRUCTURED DATA","PATIENT IDENTIFIABLE DATA","SYNTHETIC DATA","PYTHON","COMPLETE"]},{"location":"our_work/casestudy-synthetic-data-pipeline/#outcomes-and-lessons-learned","title":"Outcomes and lessons learned","text":"<p>The resulting code (click here to see the code) enables users to see how:</p> <ul> <li>an input dataset can be constructed from an open-source dataset, MIMIC-III</li> <li>SynthVAE can be adapted to be trained on a new input dataset with mixed data-types</li> <li>SynthVAE can be used to produce synthetic data</li> <li>synthetic data can be evaluated to assess it\u2019s privacy, quality and utility</li> <li>a pipeline can be used to tie together steps in a process for a simpler user experience.</li> </ul> <p>By using the set of evaluation techniques, concerns around the quality of the synthetic data can be directly addressed and measured using the variety of metrics produced as part of the report.</p> <p>The approach outlined here is not intended to demonstrate a perfectly performing synthetic data generation model, but instead to outline a pipeline that enables the generation and evaluation of synthetic data. Things like overfitting to the training data, and the potential for bias will be highlighted by the evaluation metrics but will not be remedied.</p> <p>It\u2019s important to emphasise that concerns around re-identification are reduced by using synthetic data but not completely removed. Looking at privacy metrics for the synthetic dataset will help the user to understand how well privacy has been preserved, but re-identification may still be possible.</p>","tags":["SYNTHETIC DATA","GENERATION","NEURAL NETWORKS","STRUCTURED DATA","PATIENT IDENTIFIABLE DATA","SYNTHETIC DATA","PYTHON","COMPLETE"]},{"location":"our_work/casestudy-synthetic-data-pipeline/#what-next","title":"What next?","text":"<p>The Analytics Unit is continuing to develop and improve SynthVAE, with a focus on improving the model\u2019s ability to produce high quality synthetic data.</p> <p>To better understand the privacy of any patient data used to train a synthetic data generating model, the Analytics Unit has undertaken a project exploring the use of \u2018adversarial attacks\u2019 to prove what information about the original training data might be gained from a model alone. The project focussed on a particular type of adversarial attack, called a \u2018membership attack. It explored how different levels of information would influence what the attacker could learn about the underlying dataset, and therefore the implications to any individuals whose information was used to train a model.</p> <p>Who was involved? This project was a collaboration between the NHS AI Lab Skunkworks and the Analytics Unit within the Transformation Directorate at NHS England and Improvement.</p> <p>The NHS AI Lab Skunkworks is a team of data scientists, engineers and project leaders who support the health and social care community to rapidly progress ideas from the conceptual stage to a proof of concept.</p> <p>The Analytics Unit consists of a team of analysts, economists, data scientists and data engineers who provide leadership to other analysts who are working in the system and raise data analysis up the health and care system agenda.</p>","tags":["SYNTHETIC DATA","GENERATION","NEURAL NETWORKS","STRUCTURED DATA","PATIENT IDENTIFIABLE DATA","SYNTHETIC DATA","PYTHON","COMPLETE"]},{"location":"our_work/casestudy-synthetic-data-pipeline/#_1","title":"Exploring how to create mock patient data (synthetic data) from real patient data","text":"","tags":["SYNTHETIC DATA","GENERATION","NEURAL NETWORKS","STRUCTURED DATA","PATIENT IDENTIFIABLE DATA","SYNTHETIC DATA","PYTHON","COMPLETE"]},{"location":"our_work/clinical_measurement_extractor/","title":"Clinical Measurement Extractor","text":"<p>Warning</p> <p>This project is currently in development, and as such the following is subject to change.</p>","tags":["WIP","MACHINE LEARNING","NATURAL LANGUAGE PROCESSING","LLM","PYTHON","TEXT DATA"]},{"location":"our_work/clinical_measurement_extractor/#aim","title":"Aim","text":"<p>We are building an LLM-based named-entity extraction pipeline to extract clinical measurements from free-text data. This codebase is in Python, developed in SageMaker Notebooks, and is sending request to models hosted on BedRock.</p>","tags":["WIP","MACHINE LEARNING","NATURAL LANGUAGE PROCESSING","LLM","PYTHON","TEXT DATA"]},{"location":"our_work/clinical_measurement_extractor/#methodology","title":"Methodology","text":"Figure 1: High-level of the LLM-based named-entity extraction pipeline <ol> <li>Pre-processing Reports: Applying NLP techniques to reduce and clean free-text input data.</li> <li>Detecting Contradictions:<ul> <li>Prompting an LLM to detect contradictions in free-text data.</li> <li>Using NLP techniques to identify contradictions (STRETCH).</li> </ul> </li> <li>Named-entity Recognition:<ul> <li>Prompting an LLM hosted on Bedrock to extract entities from free-text data:<ul> <li>Providing the model with defined accepted values and proposed JSON structure to guide the response.</li> <li>Using a question-answering approach guided by instructions to shape model behaviour.</li> <li>Exploring the use of zero-shot and few-shot examples in prompts.</li> </ul> </li> <li>Parsing the output into a JSON structure.</li> </ul> </li> <li>Post-Processing: Validating that the keys and values in the JSON structure are as expected, and potentially applying further post-processing steps.</li> </ol>","tags":["WIP","MACHINE LEARNING","NATURAL LANGUAGE PROCESSING","LLM","PYTHON","TEXT DATA"]},{"location":"our_work/clinical_measurement_extractor/#results","title":"Results","text":"","tags":["WIP","MACHINE LEARNING","NATURAL LANGUAGE PROCESSING","LLM","PYTHON","TEXT DATA"]},{"location":"our_work/clinical_measurement_extractor/#evaluation-approach","title":"Evaluation Approach","text":"<p>We aim to evaluate the outputs of this work using multiple methods:</p> <ol> <li>Accuracy, Precision, Recall, and F1 Score for each extracted entity, as well as overall performance: This assesses the correctness of our approach. Ideally, we aim for high accuracy, followed by precision, then recall.</li> <li>Out-of-Distribution Performance Evaluation: We will engineer prompts using a subset of free-text data from a specific time period, then evaluate performance on a holdout set from both the same and a later time period to assess the impact of data drift.</li> <li>Stratified Performance Evaluation: We will analyse how metrics vary across different dimensions of the data to measure and mitigate potential biases introduced by the system.</li> <li>G-Eval: Using an LLM to evaluate hallucinations, grounding of outputs, and to sense-check the correctness of extracted values.</li> </ol>","tags":["WIP","MACHINE LEARNING","NATURAL LANGUAGE PROCESSING","LLM","PYTHON","TEXT DATA"]},{"location":"our_work/clinical_measurement_extractor/#outcome","title":"Outcome","text":"<p>TBD</p>","tags":["WIP","MACHINE LEARNING","NATURAL LANGUAGE PROCESSING","LLM","PYTHON","TEXT DATA"]},{"location":"our_work/clinical_measurement_extractor/#_1","title":"Clinical Measurement Extractor","text":"","tags":["WIP","MACHINE LEARNING","NATURAL LANGUAGE PROCESSING","LLM","PYTHON","TEXT DATA"]},{"location":"our_work/corporate_services_pipeline_rebuild/","title":"Corporate Services: Pipeline Rebuild","text":"<p>Proprietary (owned by a company) software can be valuable for data analysis tasks. It often comes with robust support, beginner-friendly interfaces and useful built-in features. However, it can be expensive, less flexible, and - most importantly - limits our ability to be transparent about our processes. </p> <p>Transparency is a key principle of the Reproducible Analytical Pipeline (RAP) guidelines promoted within NHS England. RAP encourages reproducibility, auditability, and openness in analytical work to build trust and support evidence-based decision-making.</p> <p>In line with these principles, the analytical team supporting the Corporate Services programme engaged the Data Science team to help transition an existing automated process - previously built in proprietary software - into an open and transparent RAP-compliant pipeline.</p> <p>Unlike typical RAP projects, which often involve replacing manual processes, this project focused on migrating a fully automated pipeline into an open-source environment. A key goal was to ensure the team could maintain and develop the pipeline independently going forward.</p>","tags":["FINANCIAL","RAP","PYTHON","WIP","STRUCTURED DATA"]},{"location":"our_work/corporate_services_pipeline_rebuild/#team-upskilling","title":"Team Upskilling","text":"<p>To support this, upskilling the team was a central focus of the project. Group workshops were delivered using materials from the Intro to Python Repository, developed by the Data Science RAP team to provide foundational Python skills. The workshops covered a range of topics, from terminal and Linux commands to loops, functions, and more advanced areas such as unit testing with <code>pytest</code> and editing Excel files using <code>openpyxl</code>.</p> <p>Once confident, the team began work on a pipeline which ingested procurement data and turned it into usable metrics. Tasks were assigned using tickets, and team members worked in pairs to complete them, raising pull requests for peer review and making revisions based on feedback. This practical, hands-on approach allowed the team to apply new skills directly to real data and see immediate results. The pipeline built by the team is now live in production.</p>","tags":["FINANCIAL","RAP","PYTHON","WIP","STRUCTURED DATA"]},{"location":"our_work/corporate_services_pipeline_rebuild/#corporate-services-python-pipeline","title":"Corporate Services Python Pipeline","text":"<p>The Corporate Services data pipeline processes data submitted by over 200 trusts on corporate functions such as HR, IT, and Legal. The data is validated, compiled, and used to calculate key metrics and benchmarks - including national medians and quartile-based opportunity values - based on the income-adjusted distribution of values.</p> <p>Once published, results are made available to trusts via the Model Health System, a benchmarking tool, where outputs are visualised to support easy comparison and insights.</p> <p>By the end of the project, the team will have a fully migrated data pipeline \u2014 covering extraction, validation, and analysis \u2014 and the skills to maintain and update it collaboratively. The reliance on proprietary software will have been removed, and in alignment with RAP principles, the project commits to publishing the pipeline code openly on the NHS England GitHub repository once complete.</p>","tags":["FINANCIAL","RAP","PYTHON","WIP","STRUCTURED DATA"]},{"location":"our_work/corporate_services_pipeline_rebuild/#powerpoint-pack-automation","title":"PowerPoint Pack Automation","text":"<p>To support Regions and ICBs in accessing their data in a clear and consistent format, the team previously produced individual PowerPoint slide packs containing tailored tables and figures. This involved a time-consuming manual process of copying content from Excel to produce outputs for 8 Regions and 42 ICBs.</p> <p>This task was automated using the <code>python-pptx</code> package, paired with a pre-formatted PowerPoint template. The automation fills in each slide with the correct data and formatting, significantly reducing manual workload and saving the team over a week of repetitive effort.</p> Output Link Code and Documentation - private while under development Github","tags":["FINANCIAL","RAP","PYTHON","WIP","STRUCTURED DATA"]},{"location":"our_work/corporate_services_pipeline_rebuild/#_1","title":"Corporate Services: Pipeline Rebuild","text":"","tags":["FINANCIAL","RAP","PYTHON","WIP","STRUCTURED DATA"]},{"location":"our_work/ct-alignment/","title":"CT Alignment and Lesion Detection","text":"<p>As the successful candidate from the AI Skunkworks problem-sourcing programme, CT Alignment and Lesion Detection was first picked as a pilot project for the AI Skunkworks team in April 2021.</p>","tags":["SECONDARY CARE","COMPUTER VISION","MACHINE LEARNING","UNSTRUCTURED DATA","VISUAL DATA","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/ct-alignment/#results","title":"Results","text":"<p>A proof-of-concept demonstrator written in Python (user interface, classical computer vision models, notebooks with machine learning models).</p>","tags":["SECONDARY CARE","COMPUTER VISION","MACHINE LEARNING","UNSTRUCTURED DATA","VISUAL DATA","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/ct-alignment/#case-study","title":"Case Study","text":"<p>This is a backup of the case study published here on the NHS England Transformation Directorate website.</p> <p>George Eliot Hospital approached the NHS AI Lab Skunkworks team with an idea to use AI to speed up the analysis of computerised tomography (CT) scans.</p> <p>CT scans are used in the detection and understanding of disease. Radiologists currently manually compare two CT scans, taken at different dates, to see whether a patient\u2019s disease has improved, deteriorated or remained unchanged.</p> <p>This 12-week project investigated the AI techniques that could be applied to the problem and sought to provide a proof of concept (a feasibility study) about whether it was possible to identify organs and growths, report on any changes and highlight areas of concern to the radiologist.</p>","tags":["SECONDARY CARE","COMPUTER VISION","MACHINE LEARNING","UNSTRUCTURED DATA","VISUAL DATA","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/ct-alignment/#overview","title":"Overview","text":"<p>There are a number of challenges for radiologists reviewing cancer patients by comparing CT scans:</p> <ul> <li>Review is time-consuming. It typically takes 30 to 40 minutes to assess scans for each patient.</li> <li>They must check if there has been growth through multiple dimensions, but can only check one dimension at a time. Growth changes that look minimal in one dimension may be significant if viewed in 3D.</li> <li>The manual alignment of images is not precise because of variations in the position of the patient\u2019s body between scans.</li> <li>It is not easy to see small or developing growths, increasing the possibility of missed detection. In the abdomen, for example, radiologists are reportedly making differing interpretations in up to 37% of cases. (Siewert, 2008)</li> </ul> <p>The NHS AI Lab Skunkworks funded and supported an AI investigation with the team at George Eliot Hospital, alongside Roke, an AI specialist supplier provided through the Home Office\u2019s Accelerated Capability Environment (ACE).</p> <p>The work looked at whether AI could be used to identify features in a CT scan and automatically align images to provide radiologists with a quick, trustworthy support tool that would improve early detection and diagnosis of growths and improve patient outcomes. The team aimed to:</p> <ul> <li>provide fast, automatic overlay of scans to enable tissue growth comparison in 2D and 3D</li> <li>successfully align scans despite changes in body shape (caused by breathing during scanning, or weight gain and loss between scans)</li> <li>identify different parts of the body (bone, organs and tissue growth)</li> <li>automatically measure tissue growth, in 2D and 3D</li> <li>detect anomalies (new growths or changes not present in previous scans)</li> </ul>","tags":["SECONDARY CARE","COMPUTER VISION","MACHINE LEARNING","UNSTRUCTURED DATA","VISUAL DATA","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/ct-alignment/#what-we-did","title":"What we did","text":"","tags":["SECONDARY CARE","COMPUTER VISION","MACHINE LEARNING","UNSTRUCTURED DATA","VISUAL DATA","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/ct-alignment/#tissue-sectioning","title":"Tissue sectioning","text":"<p>Before detecting anomalies, the tool needs to separate the various types of tissue present in a scan (bone, fat, organs). The team first tried the recently released Facebook self-supervised learning method \u201cDINO\u201d, but found that this method was less accurate on CT scans than other approaches and required large amounts of memory to process. Instead, the team proceeded with a texton approach. Textons are micro-structures in images, or \u2018groups\u2019 of pixels, that can be recognised visually before the whole image is.</p> <p>The process involved applying machine learning (where an algorithm gradually improves its accuracy through repeated exposure to new data like scans and images) to the use of textons. This method was able to identify and differentiate between different tissue types accurately and rapidly.</p>","tags":["SECONDARY CARE","COMPUTER VISION","MACHINE LEARNING","UNSTRUCTURED DATA","VISUAL DATA","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/ct-alignment/#anomaly-detection","title":"Anomaly detection","text":"<p>Much like the process humans take in distinguishing between objects, classifying them and sorting them by size, computer vision takes an image input and gives output in the form of information, for example on size or colour. To detect anomalies, the team tried two methods: ellipsoid detection and infill prediction.</p> <p>Ellipsoid detection aims to isolate rounded 3D volumes of tissue, which were expected to correlate with lesions more strongly than other geometric shapes and identify those which differ from their surroundings. Infill prediction aims to learn what the parts of an image should look like in order to recognise anomalies that shouldn\u2019t be there. A combination of the two methods was found to be the most accurate at detecting lesions, while manually ignoring anomalies that aren\u2019t of concern, for example pockets of air inside the body.</p>","tags":["SECONDARY CARE","COMPUTER VISION","MACHINE LEARNING","UNSTRUCTURED DATA","VISUAL DATA","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/ct-alignment/#scan-alignment","title":"Scan alignment","text":"<p>As well as automatically detecting lesions, the project was designed to keep a \u2018human in the loop\u2019 and act as a support for radiologists conducting examinations. To do this, three methods were tested for aligning two scans from the same patient, usually taken months apart, in 2D and 3D: keypoint alignment, phase correlation and coherent point drift. Phase correlation was found to be the most robust but did not deal with body shape changes, while coherent point drift was most effective for aligning scans while taking into account any body shape changes.</p> <p>The tool allows the user to choose which of the three adjustment options provides the best alignment for each case.</p>","tags":["SECONDARY CARE","COMPUTER VISION","MACHINE LEARNING","UNSTRUCTURED DATA","VISUAL DATA","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/ct-alignment/#outcomes-and-lessons-learned","title":"Outcomes and lessons learned","text":"<p>The hoped-for benefits of the project have had some partial successes:</p>","tags":["SECONDARY CARE","COMPUTER VISION","MACHINE LEARNING","UNSTRUCTURED DATA","VISUAL DATA","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/ct-alignment/#scan-alignment_1","title":"Scan alignment","text":"<p>The team achieved both rigid and non-rigid 3D alignment that were an improvement on manual alignment but not perfect. The methods used to deal with difficulties caused by patients inhaling or exhaling during scanning were observed to work in most cases.</p>","tags":["SECONDARY CARE","COMPUTER VISION","MACHINE LEARNING","UNSTRUCTURED DATA","VISUAL DATA","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/ct-alignment/#overlay","title":"Overlay","text":"<p>Precise overlay of scans was achieved in the tool created both for 3D and 2D images, including when zooming, rotating or panning the image.</p>","tags":["SECONDARY CARE","COMPUTER VISION","MACHINE LEARNING","UNSTRUCTURED DATA","VISUAL DATA","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/ct-alignment/#new-tissue-growth-detection","title":"New tissue growth detection","text":"<p>Anomaly sizes were measured in 3D but more work is needed for robust correspondence between existing lesions to measure change.3D to successfully aid the identification of new growths.</p>","tags":["SECONDARY CARE","COMPUTER VISION","MACHINE LEARNING","UNSTRUCTURED DATA","VISUAL DATA","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/ct-alignment/#time-saving","title":"Time saving","text":"<p>Further development and testing is required to establish a reduction in radiologists\u2019 time, but the tool provides a process that is less manual for radiologists.</p> <p>George Eliot Hospital provided anonymised CT scans from 100 patients with tissue growths, and a number of marked-up scans to be used as \u201cground truth\u201d data. In order to progress this work further, greater numbers of scans are needed to provide quantitative metrics of success.</p> <p>Integration of novel image processing techniques with existing scanner software also needs to be explored to minimise friction in the workflow for radiologists.</p> <p>Further exploration is also needed to identify and compare techniques and approaches that have already been tried by the medical imaging community.</p>","tags":["SECONDARY CARE","COMPUTER VISION","MACHINE LEARNING","UNSTRUCTURED DATA","VISUAL DATA","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/ct-alignment/#what-next","title":"What next?","text":"<p>George Eliot Hospital and Roke have submitted a joint application for further funding through the AI Award to further test the tool and establish a rigorous evaluation ahead of any regulatory work that will be required to use the software in clinical workflow.</p> <p>The team are testing these techniques against the latest techniques demonstrated as part of the Medical Image Computing and Computer Assisted Intervention (MICCAI) conferences.</p>","tags":["SECONDARY CARE","COMPUTER VISION","MACHINE LEARNING","UNSTRUCTURED DATA","VISUAL DATA","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/ct-alignment/#who-was-involved","title":"Who was involved?","text":"<p>This project is a collaboration between NHSX, George Eliot Hospital, and Roke who were selected through the Home Office\u2019s Accelerated Capability Environment (ACE). The AI Lab Skunkworks exists within the NHS AI Lab to support the health and care community to rapidly progress ideas from the conceptual stage to a proof of concept.</p> <p>The NHS AI Lab is working with the Home Office programme: Accelerated Capability Environment (ACE) to develop some of its skunkworks projects, providing access to a large pool of talented and experienced suppliers who pitch their own vision for the project.</p> <p>Accelerated Capability Environment (ACE) is part of the Homeland Security Group within the Home Office. It provides access to more than 250 organisations from across industry, academia and the third sector who collaborate to bring the right blend of capabilities to a given challenge. Most of these are small and medium-sized enterprises (SMEs) offering cutting-edge specialist expertise.</p> <p>ACE is designed to bring innovation at pace, accelerating the process from defining a problem to developing a solution and delivering practical impact to just 10 to 12 weeks.</p> <p>Roke is a long-established science and engineering organisation, providing AI and machine learning expertise and lending technical capabilities to projects with NHS AI Lab Skunkworks.</p> Output Link Open Source Code &amp; Documentation Github Case Study Case Study Technical report PDF Video walkthrough YouTube","tags":["SECONDARY CARE","COMPUTER VISION","MACHINE LEARNING","UNSTRUCTURED DATA","VISUAL DATA","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/ct-alignment/#_1","title":"CT Alignment and Lesion Detection","text":"","tags":["SECONDARY CARE","COMPUTER VISION","MACHINE LEARNING","UNSTRUCTURED DATA","VISUAL DATA","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/cvd_pathways/","title":"CVD Pathways","text":"","tags":["PRIMARY CARE","SECONDARY CARE","DISEASES","POPULATION HEALTH","LINKAGE","PYTHON","RAP","STRUCTURED DATA","WIP"]},{"location":"our_work/cvd_pathways/#about-cvd-pathways","title":"About CVD Pathways","text":"<p> Figure 1: Overview of the CVD Pathways project aims, areas of interest and outputs.</p> <p>Cardiovascular Disease (CVD) is a major health concern and has been identified in the NHS Long Term Plan as the single biggest condition where lives can be saved by the NHS over the next 10 years. Understanding the complete patient journey, from initial risk factors to outcomes, is required for effective prevention and treatment of CVD.</p> <p>The CVD Pathways project addresses this by creating an Extract, Transform, Load (ETL) data pipeline, producing healthcare records for patients currently at-risk or having a high-risk of acquiring CVD. This pipeline links primary care data (from CVDPREVENT) with secondary care data for over 16 million patients.</p> <p>CVDPREVENT is a national primary care audit focusing on six high-risk conditions for stroke, heart attack, and dementia:</p> <ul> <li>Atrial fibrillation (AF)</li> <li>High blood pressure</li> <li>High cholesterol</li> <li>Diabetes</li> <li>Non-diabetic hyperglycaemia</li> <li>Chronic kidney disease</li> </ul> <p>This primary care data is then enhanced through linkage with key secondary care datasets, including:</p> <ul> <li>Mortality records</li> <li>Admitted patient care (hospitalisation episodes)</li> <li>Outpatient appointments</li> <li>A&amp;E attendances</li> <li>Prescriptions dispensed in primary care</li> <li>Segmentation data (Bridges to Health)</li> <li>Specialised cardiac audits (e.g., MINAP)</li> </ul> <p>The result is an enriched, longitudinal view of patient health, paving the way for analytics in areas such as risk stratification, intervention effectiveness, and understanding multimorbidity in relation to CVD. This also builds upon the previous CVD Prevent Tool project (cvd-prevent-tool), which feeds into the insights generated as part of CVDPREVENT and the NHS Benchmarking teams.</p> <p>Overall, this project aims to provide an enriched view of patient history, with the goal is to use these enriched assets to support further analytics, outlined in our \u201cWhat\u2019s next?\u201d steps below.</p>","tags":["PRIMARY CARE","SECONDARY CARE","DISEASES","POPULATION HEALTH","LINKAGE","PYTHON","RAP","STRUCTURED DATA","WIP"]},{"location":"our_work/cvd_pathways/#why-cvd-pathways","title":"Why CVD Pathways?","text":"<p>Historically, analysing the complete patient journey for CVD has been challenging due to disparate data sources, and the lack of a \"single source of truth\" detailing a patient's history.</p> <p>The CVD Pathways project was initiated to overcome this.</p> <ul> <li>The problem: Policymakers and clinicians needed a unified view of patient data to understand risk factors, track disease progression, evaluate interventions, and reduce the burden of CVD.</li> <li>Our solution: We developed a an ETL pipeline to create a linked asset, allowing for investigating and analysis to support clinical and policy decision makers.</li> </ul> <p>This provides a powerful platform to:</p> <ul> <li>Enhance Understanding: Gain deeper insights into how CVD develops, how patients interact with the health system, and the impact on patient outcomes.</li> <li>Inform Evidence-Based Decisions: Equip clinical and policy teams with data to better understand prevention strategies and care pathways.</li> <li>Improve Patient Outcomes: By identifying high-risk groups and effective interventions earlier, we can contribute to reducing CVD-related mortality and morbidity.</li> <li>Support National Health Priorities: Directly align with NHS and Government goals to tackle cardiovascular disease, a leading cause of death and disability.</li> </ul>","tags":["PRIMARY CARE","SECONDARY CARE","DISEASES","POPULATION HEALTH","LINKAGE","PYTHON","RAP","STRUCTURED DATA","WIP"]},{"location":"our_work/cvd_pathways/#what-we-did-etl-data-pipeline","title":"What we did - ETL Data Pipeline","text":"<p>Our core work involved designing and implementing an ETL (Extract, Transform, Load) data pipeline. This automated process takes raw data from diverse sources and turns it into a structured, analysis-ready format.</p>","tags":["PRIMARY CARE","SECONDARY CARE","DISEASES","POPULATION HEALTH","LINKAGE","PYTHON","RAP","STRUCTURED DATA","WIP"]},{"location":"our_work/cvd_pathways/#data","title":"Data","text":"<p>As described above, we integrated data from the national CVDPREVENT audit (primary care) with secondary care datasets including hospital admissions, A&amp;E events, outpatient appointments, mortality data, and prescriptions. This involved handling data for over 16 million individuals, linking together records spanning several years.</p>","tags":["PRIMARY CARE","SECONDARY CARE","DISEASES","POPULATION HEALTH","LINKAGE","PYTHON","RAP","STRUCTURED DATA","WIP"]},{"location":"our_work/cvd_pathways/#methods","title":"Methods","text":"<p>A summary of our pipeline data-flow is shown in the figure below.</p> <p> Figure 2: Simplified overview of the CVD Pathways ETL data pipeline.</p> <p>The process involved several key stages:</p> <ol> <li>Extraction: Accessing and extracting pseudonymised data from the various source systems (e.g., CVDPREVENT, Civil registrations of deaths, SUS hospitalisations).</li> <li>Transformation: Processing of the raw primary and secondary care assets into patient events:<ul> <li>Cleaning: Standardising formats, handling missing values, and ensuring data quality.</li> <li>Linking: Accurately linking records for the same patient across different datasets using pseudonymised identifiers. This creates the longitudinal patient record.</li> <li>Feature Engineering: Deriving meaningful variables from the raw data. For example, calculating the time between specific events, or summarising a patient's history of certain conditions.</li> <li>Structuring: Organising the transformed data into the analytical assets described below (Events Table and Patient Table).</li> </ul> </li> <li>Loading: Producing the final analytical assets for use in further analytics.</li> </ol> <p>We did this through the use of data engineering tools and techniques, primarily using Python/PySpark, Databricks and Azure. This was all done within NHS England's secure data environment, the Unified Data Access Layer (UDAL). The result of this is a scalable, reproducible pipeline. </p>","tags":["PRIMARY CARE","SECONDARY CARE","DISEASES","POPULATION HEALTH","LINKAGE","PYTHON","RAP","STRUCTURED DATA","WIP"]},{"location":"our_work/cvd_pathways/#key-exploitable-results-ker-benefits","title":"Key Exploitable Results (KER) &amp; Benefits","text":"<p>The primary output of this project is the creation of two powerful, linked analytical data assets. These assets are the foundation for a wide range of impactful analyses.</p> <p>Benefit 1: Longitudinal Patient Events and Records</p> <ul> <li>Output: The Events Table. This long-format table details healthcare interactions and events for each patient chronologically (e.g., diagnosis, hospitalisation, prescriptions, death).</li> <li>Impact: For the first time, we can readily analyse end-to-end patient pathways, identify points for intevention and prevention, and understand sequences of events leading to CVD outcomes.</li> </ul> <p>Benefit 2: Enriched Patient-Level Profiles and Summaries</p> <ul> <li>Output: The Patient Table. This wide-format table provides a single row per patient, summarising their demographic information, key characteristics, and aggregated history (e.g., total number of hospitalisations for stroke, presence of specific comorbidities).</li> <li>Impact: Enables risk stratification, identification of distinct patient cohorts, and analysis of factors associated with different CVD outcomes.</li> </ul>","tags":["PRIMARY CARE","SECONDARY CARE","DISEASES","POPULATION HEALTH","LINKAGE","PYTHON","RAP","STRUCTURED DATA","WIP"]},{"location":"our_work/cvd_pathways/#teams","title":"Teams","text":"<p>This project is a collaboration between NHS England Data Science and CVDR Programme teams, in addition to the Office for Health Improvement &amp; Disparities (OHID, part of Department of Health and Social Care).</p>","tags":["PRIMARY CARE","SECONDARY CARE","DISEASES","POPULATION HEALTH","LINKAGE","PYTHON","RAP","STRUCTURED DATA","WIP"]},{"location":"our_work/cvd_pathways/#whats-next","title":"What's Next?","text":"<p>With the ETL pipeline and core data assets now established, the CVD Pathways project is moving into a phase of analytics. Key areas of focus will include:</p> <ul> <li>Risk Stratification: Developing models to identify groups of patients at varying levels of risk for future cardiac events (e.g., heart attack, stroke).</li> <li>Time-to-First CVD Event Analysis: Understanding the typical timeframe from a patient's first appearance in a high-risk cohort to their first major CVD event.</li> <li>Intervention and Prevention Analysis: Comparing observed patient pathways against clinical guidelines to assess adherence to preventative measures and potential opportunities for improvement.</li> <li>CVD and Multimorbidity: Investigating how co-existing conditions influence the development and progression of CVD.</li> <li>Survivability Analysis: Analysing survival rates post-CVD event, stratified by risk factors and demographics.</li> </ul> <p>As an example, we are analysing patient pathways, specifically looking at patient's who have had a ST-Elevation Myocardial Infarction (STEMI) heart attack. This is a type of heart attack that that affects the lower chambers, and is more severe and dangerous compared to other types of heart attack.</p> <p> Figure 3: Overview of clinical pathway for a patient who has a STEMI heart attack.</p> <p>These analyses aim to generate actionable insights for clinicians, policymakers, and public health initiatives to ultimately improve patient's cardiovascular health outcomes. We're also working on publishing our codebase as part of our guidelines around reproducibility and transparency.</p>","tags":["PRIMARY CARE","SECONDARY CARE","DISEASES","POPULATION HEALTH","LINKAGE","PYTHON","RAP","STRUCTURED DATA","WIP"]},{"location":"our_work/cvd_pathways/#links","title":"Links","text":"Output Link Data Processing Pipeline (CVD Prevent Tool) GitHub","tags":["PRIMARY CARE","SECONDARY CARE","DISEASES","POPULATION HEALTH","LINKAGE","PYTHON","RAP","STRUCTURED DATA","WIP"]},{"location":"our_work/cvd_pathways/#_1","title":"CVD Pathways","text":"","tags":["PRIMARY CARE","SECONDARY CARE","DISEASES","POPULATION HEALTH","LINKAGE","PYTHON","RAP","STRUCTURED DATA","WIP"]},{"location":"our_work/data-lens/","title":"Data Lens","text":"<p>As the successful candidate from a Dragons\u2019 Den-style project pitch, Data Lens was first picked as a pilot project for the NHS AI (Artificial Intelligence) Lab Skunkworks team in September 2020.</p> <p>The pitch outlined a common data problem for analysts and researchers across the UK: large volumes of data held on numerous incompatible databases in different organisations. The team wanted to be able to quickly source relevant information with one search engine.</p>","tags":["WORKFORCE","NATURAL LANGUAGE PROCESSING","GENERATION","UNSTRUCTURED DATA","TEXT DATA","PYTHON","WEBDEV","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/data-lens/#results","title":"Results","text":"<p>A prototype website written in HTML/CSS/JavaScript (frontend), JavaScript (backend) and python (scrapers, search) implementing elasticsearch and natural language search across a number of NHS databases.</p>","tags":["WORKFORCE","NATURAL LANGUAGE PROCESSING","GENERATION","UNSTRUCTURED DATA","TEXT DATA","PYTHON","WEBDEV","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/data-lens/#case-study","title":"Case Study","text":"<p>This is a backup of the case study published here on the NHS England Transformation Directorate website.</p> <p>A pilot project for the NHS AI Lab Skunkworks team, Data Lens brings together information about multiple databases, providing a fast-access search in multiple languages</p>","tags":["WORKFORCE","NATURAL LANGUAGE PROCESSING","GENERATION","UNSTRUCTURED DATA","TEXT DATA","PYTHON","WEBDEV","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/data-lens/#overview","title":"Overview","text":"<p>As the successful candidate from a Skunkworks problem-sourcing event, Data Lens was first picked as a pilot project for the NHS AI (Artificial Intelligence) Lab Skunkworks team in September 2020.</p> <p>The pitch outlined a common data problem for analysts and researchers across the UK: large volumes of data held on numerous incompatible databases in different organisations. The team wanted to be able to quickly source relevant information with one search engine.</p>","tags":["WORKFORCE","NATURAL LANGUAGE PROCESSING","GENERATION","UNSTRUCTURED DATA","TEXT DATA","PYTHON","WEBDEV","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/data-lens/#how-data-lens-works","title":"How Data Lens works","text":"<p>Following a 12-week development phase, a first-stage prototype of the Data Lens has been completed. Using Natural Language Processing (NLP) and other AI technologies, the Data Lens project is creating a universal search engine for health and social care data catalogues and metadata.</p> <p>The Data Lens joins up data catalogues from NHS Digital, the Health Innovation Gateway, MDXCube, NHS Data Catalogue, PHE Fingertips and the Office for National Statistics.</p> <p>By providing user-friendly access to previously time-consuming separate data catalogues, Data Lens aims to:</p> <ul> <li>present information about data from across the sector with one search</li> <li>give preview information and direct users to an original location (avoiding the need for another database)</li> <li>provide multilingual support and a user focused approach</li> <li>reduce workload and improve the quality of information available</li> <li>build up a picture of what data is collected and how it flows through the health and social care system.</li> </ul> <p>The search tool not only increases data access and collaboration, it is learning to improve the results it provides by tracking what people search for, whether they click through and which dataset they use so that its results can be even more relevant over time.</p> <p>Using Natural Language Processing (NLP), the engine is able to suggest relevant results that go beyond the scope of the search terms it is given. With the use of browser translation, it also supports searches and results in all 71 languages supported by Amazon Web Services (AWS), increasing the usability and inclusivity of the product.</p> <p>The prototype is getting support through the NHS digital service development pipelines - part of the journey towards achieving a fully fledged AI product that is making a real difference to the delivery of health and social care.</p>","tags":["WORKFORCE","NATURAL LANGUAGE PROCESSING","GENERATION","UNSTRUCTURED DATA","TEXT DATA","PYTHON","WEBDEV","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/data-lens/#why-data-lens-is-needed","title":"Why Data Lens is needed","text":"<p>The health and social care sector has huge amounts of data, spread across many NHS organisations and even more databases. When searching for information it can often be difficult to find out whether it exists, and where it is. Searching and cross-referencing multiple data catalogues can also be extremely time-consuming.</p> <p>This project furthers the \u2018Joining-Up Care Agenda\u2019 by enabling cross-organisational views of data.</p> <p>Using artificial intelligence to power this search engine reduces the time required to make the most of existing data sets, and answers the call from the Secretary of State for Health and Social Care to turbo-charge data responsiveness and ease the burden of data collection across the health and care system.</p>","tags":["WORKFORCE","NATURAL LANGUAGE PROCESSING","GENERATION","UNSTRUCTURED DATA","TEXT DATA","PYTHON","WEBDEV","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/data-lens/#open-source-data-lens-code","title":"Open source Data Lens code","text":"<p>Code and documentation from the development of this project is available for developers and AI enthusiasts. By making code freely available it is hoped that new search engines can be developed and that more organisations will engage with using AI.</p> <p>With thanks to colleagues at NHSX Analytics Unit, Accelerated Capability Environment (ACE) and Naimuri - the ACE community member selected via competition - we undertook the following open approach over 12 weeks and 6 sprints:</p> <ol> <li>NHSX Analytics Unit colleagues identified a number of openly licenced data sets from the healthcare world - forming the core of the proof-of-concept.</li> <li>Partners at Naimuri built the platform on the community edition of ElasticSearch, a popular system for full-text search engines.</li> <li>The team employed various Natural Language Processing (NLP) techniques in order to go further than providing simple keyword searches.</li> <li>The AI was trained to understand better semantic similarities in searches, i.e. to suggest results for \u201csmoking\u201d alongside \u201ccancer\u201d using vector analysis and cosine similarity.</li> <li>User feedback was built into the AI training, so the more users who give \u201cthumbs up\u201d (or down) to suggested results, the better the results will become over time.</li> <li>Fuzzy matching was implemented to help with typos and misspellings.</li> <li>A recommendation engine was developed to suggest related but not searched for sets.</li> <li>Finally, around 9 published NHS acronyms and jargon busters were used to help unpick things like \u201cA&amp;E\u201d and \u201cIP\u201d.</li> </ol> <p>The team brought in metadata from NHS Digital, NHS England and Improvement, Public Health England, Office for National Statistics and Health Data Research UK in order to prove that Data lens could onboard from different organisations in different ways: APIs, scraping, even manual metadata files.</p> <p>Working with the AI Lab Skunkworks on this project was Agile in the truest sense of the word. We pitched an idea, had funding approved and were up and running in a very short amount of time. I sincerely hope it can be taken forward into production to help its users get value from the wealth of data and information that is produced by the Health and Social Care sector. \u2013 Paul Ross, Data Engineer, NHSX Analytics Unit.</p> <p>This project has shown the value of better data access using intelligent, domain specific search. The approach of creating a proof of concept and the freedom it's given us to apply advanced technology has really added value. \u2013 Kieran Moran, Naimuri.</p> <p>At ACE our overriding mission is to keep the public safe, so we welcomed the opportunity to work with NHSX, and help them tackle the challenges they and the wider healthcare sector face. \u2013 Simon Christoforato, CEO of ACE\u2019s Vivace supplier community.</p> Output Link Open Source Code &amp; Documentation Github Case Study Case Study","tags":["WORKFORCE","NATURAL LANGUAGE PROCESSING","GENERATION","UNSTRUCTURED DATA","TEXT DATA","PYTHON","WEBDEV","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/data-lens/#_1","title":"Data Lens","text":"","tags":["WORKFORCE","NATURAL LANGUAGE PROCESSING","GENERATION","UNSTRUCTURED DATA","TEXT DATA","PYTHON","WEBDEV","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/ds218_rap_community_of_practice/","title":"NHS Reproducible Analytical Pipelines (RAP) Champion Squad","text":"<p> <p> </p> <p>Figure 1: Multiple examples of how the team has marketed Reproducible Analytical Pipelines as a way of working, on LinkedIn, Teams, through drop-in sessions, seminars and conference talks. Click the image to visit our website.</p> <p>Reproducible Analytical Pipelines (RAP) is a way of working promoted across the Civil Service, which promises faster, more efficient, more robust and transparent analysis and data pipelines.</p> <p>Find out more about RAP on our guide See our helpful LinkedIn posts</p> <p>Our Squad of RAP Champions has supported the rollout of RAP across the Analytics area within NHS England. We've approached through three main pillars of work:</p> <ul> <li>Making RAP guidance specific to our analysts needs: We took existing RAP guidance (such as the Government RAP strategy) and interpreted it in the local context of NHS England, making guidance specific to our systems and the problems faced by our analysts.</li> <li>Helping colleagues start their RAP journey: We put together a program for how to learn RAP, transform pipelines (through the \"thin slice approach\") and become a RAP champion yourself.</li> <li>Building a RAP community: RAP is about sharing and re-use, so we've been actively engaging with the NHS R and Python communities, AnalystX , the Govt Data Science community, and consolidating this learning in our own NHS RAP Community of Practice, and Health RAP Playbook (made with colleagues from other organisations in the industry).</li> </ul> <p>We've also been active in promoting RAP up and down the business (as grassroots level, to managers and also to directors), and across the industry, making it something people want to be involved in, and ensuring they knew the value it offered.</p>","tags":["WORKFORCE","RAP","BEST PRACTICE","PYTHON","R","COMPLETE"]},{"location":"our_work/ds218_rap_community_of_practice/#results","title":"Results","text":"<p>In the past two years we've:</p> <ul> <li>Massively raised the profile of RAP in the business - with benefits to colleagues learning and skills</li> <li>Helped start up two other squads of RAP champions within NHS England, who are now themselves going on to train more colleagues in RAP</li> <li>Greatly increased the number of publications with published code (it was zero, and now is everything here: NHSDigital/data-analytics-services#rap-publication-repositories).</li> <li>The teams who have implemented RAP have found their code is easier to understand, reuse, and often a lot faster, with one pipeline going from taking two analysts 2 weeks, to running in just 40 minutes.</li> <li>We also got through to the finals of HSJ Digital Awards - Replicating Digital Best Practice Award.</li> </ul> <p>Our guidance is widely used by other organisations within the sector, and we've had great success rolling out RAP within NHS England.</p> Output Link NHS RAP Community of Practice Website Website NHS Digital Data Services - Analytics Service Repo Github Repo Health RAP playbook Website","tags":["WORKFORCE","RAP","BEST PRACTICE","PYTHON","R","COMPLETE"]},{"location":"our_work/ds218_rap_community_of_practice/#_1","title":"NHS Reproducible Analytical Pipelines (RAP) Champion Squad","text":"","tags":["WORKFORCE","RAP","BEST PRACTICE","PYTHON","R","COMPLETE"]},{"location":"our_work/ds251_RAG/","title":"Retrieval Augmented Generation","text":"<p>See our RAG Demos and discussions here</p>","tags":["NATURAL LANGUAGE PROCESSING","LLM","RESEARCH","UNSTRUCTURED DATA","TEXT DATA","OPEN DATA","PYTHON"]},{"location":"our_work/ds251_RAG/#headlines","title":"Headlines:","text":"<ul> <li>LLMs produce more relevant and accurate content when given key information in the context window. </li> <li>All RAG techniques seek to utilise this strength of LLMs by maximising the use of the context window. </li> <li>Modern LLM systems will involve many RAG techniques, often coming as standard in AI development architectures.</li> </ul>","tags":["NATURAL LANGUAGE PROCESSING","LLM","RESEARCH","UNSTRUCTURED DATA","TEXT DATA","OPEN DATA","PYTHON"]},{"location":"our_work/ds251_RAG/#_1","title":"Retrieval Augmented Generation","text":"","tags":["NATURAL LANGUAGE PROCESSING","LLM","RESEARCH","UNSTRUCTURED DATA","TEXT DATA","OPEN DATA","PYTHON"]},{"location":"our_work/ds255_privacyfp/","title":"Building a Tool to Assess the Privacy Risk of Text Data - Extended","text":"<p>Warning</p> <p>This codebase is a proof of concept and is under constant development so should only be used for demonstration purposes within a controlled environment.</p> <p></p> Figure 1: Diagram of the high-level overview of Privacy Fingerprint using open-source models.","tags":["WORKFORCE","TEXT DATA","LLM","PYTHON","ETHICS","NATURAL LANGUAGE PROCESSING","UNSTRUCTURED DATA","SYNTHETIC DATA","RESEARCH","PAUSED"]},{"location":"our_work/ds255_privacyfp/#aim","title":"Aim","text":"<p>To develop a modular tool to score the privacy risk of healthcare free-text data with open-source tools. </p>","tags":["WORKFORCE","TEXT DATA","LLM","PYTHON","ETHICS","NATURAL LANGUAGE PROCESSING","UNSTRUCTURED DATA","SYNTHETIC DATA","RESEARCH","PAUSED"]},{"location":"our_work/ds255_privacyfp/#background","title":"Background","text":"<p>This concept is built upon \"Building A Tool to Assess the Privacy Risk of Text Data\" article, where ChatGPT-3.5 was used to generate synthetic unstructured data and Amazon's AWS Comprehend Medical was used to extract entitities contributing to re-idetification risk. </p> <p>In this project we use open source Large Language Models (LLMs) for generating synthetic unstructured data locally and open source named-entity reconginition (NER) models for extracting our entities. The next steps will be to develop a range of experiments so we can then assess the risk associated with various scenarios.</p>","tags":["WORKFORCE","TEXT DATA","LLM","PYTHON","ETHICS","NATURAL LANGUAGE PROCESSING","UNSTRUCTURED DATA","SYNTHETIC DATA","RESEARCH","PAUSED"]},{"location":"our_work/ds255_privacyfp/#methodology","title":"Methodology","text":"<p>The current pipeline has been broken down into 6 components:</p> <ol> <li>Generating Synthetic Patient Data using Synthea: Synthea-international is an expansion of Synthea, an open-source synthetic patient generator that produces de-identified health records for synthetic patients.</li> <li>Generating Synthetic Patient Medical Notes: Utilizes LLama2 to generate synthetic medical notes.</li> <li>Re-extracting Entities from the Patient Medical Notes: Utilizes UniversalNER, an open-source generative large language model, specifically trained to extract a range of entities when prompted with a list of entities you want to extract.</li> <li>Normalising Entities Extracted for Scoring: This is a standardisation process to ensure that outputs coming out can be consistently assessed, i.e., dates are all formatted the same.</li> <li>Scores the Uniqueness of Standardised Entities Extracted: (Py)CorrectMatch is used to extract out information on the uniqueness of individual records, and the global uniquness across a whole dataset, by training a Gaussian Copula model on the data.</li> <li>Calculates SHAP Values from Tranformed Data generated from a Fitted Gaussian Copula Model: SHAP (SHapley Additive exPlanations) is used to interpret and explain the results produced from (Py)CorrectMatch, which evaluates the uniqueness of data rows across a whole dataframe.</li> </ol>","tags":["WORKFORCE","TEXT DATA","LLM","PYTHON","ETHICS","NATURAL LANGUAGE PROCESSING","UNSTRUCTURED DATA","SYNTHETIC DATA","RESEARCH","PAUSED"]},{"location":"our_work/ds255_privacyfp/#results","title":"Results","text":"<p>TBC</p> Output Link Open Source Code Github Open Documentation Github.io Case Study NA Technical report NA Algorithmic Impact Assessment NA","tags":["WORKFORCE","TEXT DATA","LLM","PYTHON","ETHICS","NATURAL LANGUAGE PROCESSING","UNSTRUCTURED DATA","SYNTHETIC DATA","RESEARCH","PAUSED"]},{"location":"our_work/ds278_Assurance_Research_Path/","title":"AI Assurance Research Path","text":"<p>The NHS AI Quality Community of Practice (AIQCoP) is a network of testing and assurance professionals dedicated to supporting the assurance of healthcare AI. While not an assurance function itself, the AIQCoP provides guidance, resources, and advice to assist colleagues and organisations in navigating the complexities of AI assurance. While the project sits within NHS England, the remit of the AIQCoP is the whole of the NHS. </p> <p>The AIQCoP conducts cutting-edge research to address real-world challenges in AI assurance. Current research areas include boundary analysis in image classifiers, focusing on understanding and validating how AI models make classification decisions; bias in AI training data, investigating scenario and demographic biases to mitigate risks in model performance; literacy bias in chatbots and AI search systems, aiming to ensure equitable access for users with varying literacy levels; and exploring AI-driven automation of risk logs, test scripts, and test data generation. This work combines traditional assurance techniques with novel approaches to enhance the reliability and fairness of AI systems.</p> <p>For years now the AIQCoP has relied on embedded Data Scientists from NHS England to help advise on the technical aspects of AI Assurance. </p> <p>We have had a paper published on our research into using mixup augmentations in assurance of AI.  Read our Mixup Assurance paper here on ArXiv</p> <p>The investigations that started with Mixup have inspired a new method of peering into the black box of neural network classifiers.  RISE is an experimental tool which can help assurers, data scientists and clinicians to evaluate Ai Classifiers, read more on the blog </p> <p> Will and Ben at the Health and Care Analytics Conference talking about AI Assurance</p> <p>Most information about the work in the Assurance Research Path appears on the AIQCoP sharepoint site, access may need to be requested to view.</p> <p>AI Quality Community of Practice</p> <p>Alternatively you can always email us at: nhsdigital.instant@nhs.net</p>","tags":["NATURAL LANGUAGE PROCESSING","LLM","PYTHON","BEST PRACTICE","MACHINE LEARNING","CLASSIFICATION","COMPUTER VISION","UNSTRUCTURED DATA","VISUAL DATA","WIP"]},{"location":"our_work/ds278_Assurance_Research_Path/#_1","title":"AI Assurance Research Path","text":"","tags":["NATURAL LANGUAGE PROCESSING","LLM","PYTHON","BEST PRACTICE","MACHINE LEARNING","CLASSIFICATION","COMPUTER VISION","UNSTRUCTURED DATA","VISUAL DATA","WIP"]},{"location":"our_work/ds304_synthetic_clinical_notes/","title":"Synthetic Clinical Notes","text":"","tags":["SYNTHETIC DATA","TEXT DATA","LLM","PYTHON","SECONDARY CARE","WIP","UNSTRUCTURED DATA"]},{"location":"our_work/ds304_synthetic_clinical_notes/#what-we-are-doing","title":"What we are doing","text":"<p>During a patient\u2019s stay in hospital, various notes are written about them by the clinicians involved in their care. At the end of their stay, a clinician must write a discharge summary using these clinical notes. The summary communicates to the post-hospital care team what has happened to the patient during their hospital stay and their ongoing care plan.  </p> <p>The manual process of writing this summary adds to clinicians\u2019 workloads, and it can vary in quality. There is currently some work going on to develop an AI augmented discharge summary tool (AADS), which will automate some of this process using Large Language Models [LLMs]. </p> <p>Data governance processes have delayed the use of real data for the development of the AADS tool. The Synthetic Clinical Notes project aims to create a range of synthetic patient admissions and their corresponding clinical notes to facilitate the testing and development of AADS until the relevant data governance agreements are in place. The synthetic notes will continue to be used alongside real data once this is available.</p> <p></p>","tags":["SYNTHETIC DATA","TEXT DATA","LLM","PYTHON","SECONDARY CARE","WIP","UNSTRUCTURED DATA"]},{"location":"our_work/ds304_synthetic_clinical_notes/#why-we-are-doing-it","title":"Why we are doing it","text":"<p>Although there are open data sources containing anonymised clinical notes, they often have restrictive data usage agreements which exclude purposes other than scientific research. </p> <p>Furthermore, synthetic data allows us to introduce the kind of noise that might trip up the discharge summariser in the real world, but in a controlled way. Some examples of this kind of noise include typos, medical abbreviations, different writing styles, and contradicting diagnoses. </p> <p>Once the AADS tool is live, we can also convert real failure states into shareable synthetic test cases, giving us synthetic versions of real clinical notes which are challenging for the tool to adequately summarise. These synthetic versions could be compiled into an evaluation suite which could be shared between trusts alongside the AADS tool itself. </p> <p>The synthetic clinical notes are not intended for comprehensive assurance, and will be used in addition to real data, not as a replacement for it. </p>","tags":["SYNTHETIC DATA","TEXT DATA","LLM","PYTHON","SECONDARY CARE","WIP","UNSTRUCTURED DATA"]},{"location":"our_work/ds304_synthetic_clinical_notes/#how-we-are-doing-it","title":"How we are doing it","text":"<p>We have created a reusable pipeline within the NHS Federated Data Platform [FDP]. Our pipeline is made up of several useful building blocks: </p> <ul> <li> <p>To generate variation in patient demographics we reused our previous work on Developing Artificial Primary Care Records. This gives each of our fake patients a realistic name, gender and date of birth. </p> </li> <li> <p>We use a list of common reasons for admission by age and gender to generate admission details for each synthetic patient. At this stage, we also generate some synthetic clinicians who will be involved in the patient\u2019s care. </p> </li> <li> <p>We insert the demographic and admission information into an LLM prompt and ask the LLM to generate a synthetic patient journey containing a series of events which are realistic for this type of admission. </p> </li> <li> <p>For each event in the synthetic journey, we make a new LLM call to generate the clinical note corresponding to the event. We based the structure of the clinical notes on templates that are used by real clinicians. Depending on which synthetic clinician is writing the note, the LLM is given different style instructions (e.g. conciseness, use of bullet points, informal style etc). </p> </li> <li> <p>To add further realistic noise to these notes, we insert typos and medical abbreviations. The rate of typos and medical abbreviations is dependent on which synthetic clinician is writing the note. </p> </li> <li> <p>We validate the synthetic notes using additional validation LLM prompts. This is to ensure that the notes are faithful to the information contained in the synthetic patient journey. </p> </li> <li> <p>We have also built an evaluation pipeline to facilitate iterative improvements to our clinical note generation pipeline. The evaluation makes use of several LLM-as-a-judge evaluators. </p> </li> </ul>","tags":["SYNTHETIC DATA","TEXT DATA","LLM","PYTHON","SECONDARY CARE","WIP","UNSTRUCTURED DATA"]},{"location":"our_work/ds304_synthetic_clinical_notes/#results","title":"Results","text":"<p>The diagram below shows an example synthetic patient called Karl Timothy Birch, admitted with chest pain and shortness of breath, and an example synthetic clinical note written about him by a synthetic clinician. If Karl were a real patient, this note would be fed into AADS along with others written about Karl during his hospital stay, and a clinician would use AADS to produce a semi-automated summary of the treatment Karl received. This summary would then be sent on to his GP and others involved in his onward care. We can use Karl\u2019s synthetic clinical notes, alongside those of our other synthetic patients, to test AADS and uncover opportunities for improvement prior to testing on real data.</p> <p></p>","tags":["SYNTHETIC DATA","TEXT DATA","LLM","PYTHON","SECONDARY CARE","WIP","UNSTRUCTURED DATA"]},{"location":"our_work/ds304_synthetic_clinical_notes/#_1","title":"Synthetic Clinical Notes","text":"","tags":["SYNTHETIC DATA","TEXT DATA","LLM","PYTHON","SECONDARY CARE","WIP","UNSTRUCTURED DATA"]},{"location":"our_work/epma_autocoding/","title":"ePMA Auto-Coding","text":"","tags":["PYTHON","SECONDARY CARE","PRESCRIBING","TEXT DATA","LINKAGE","WIP"]},{"location":"our_work/epma_autocoding/#the-problem","title":"The Problem","text":"<p>The ePMA (electronic prescribing and medicine administration) data collection is a person-level data asset containing information on what medication is prescribed and administered in secondary care settings. The ePMA data that hospitals submit contain a number of free text fields including: medication name, dosage, route and frequency. This creates some challenges when aiming to collect and eventually disseminate this data:</p> <ol> <li>Free text can potentially contain PID (patient identifiable data) which is not permitted</li> <li>Free text descriptions of medication make analysis hard because the same medication could be described in multiple ways </li> </ol>","tags":["PYTHON","SECONDARY CARE","PRESCRIBING","TEXT DATA","LINKAGE","WIP"]},{"location":"our_work/epma_autocoding/#our-solution","title":"Our Solution","text":"<p>The ePMA auto-coding pipeline was created which maps the free text descriptions of medications to valid SNOMED codes using dm+d (dictionary of medicines and devices). The pipeline has a series of deterministic steps as follows (high level view only):</p> <ol> <li>Filter submissions - records that have been mapped before or deemed to be unmappable are not run through the pipeline again</li> <li>Preprocessing - convert to lowercase, replace words and punctuation</li> <li>Read data - the input ePMA data with dm+d reference data from VTM (chemical), VMP (product), AMP (branded product)</li> <li>Exact matching - does the input exactly match any of the dm+d reference data?</li> <li>Entity matching - split the medication description into moiety, unit, strength and dose form and attempt to match to dm+d reference data</li> <li>Fuzzy matching* - compare the input against the dm+d reference data using fuzzy logic</li> </ol> <p>*Fuzzy logic is a technique using various distance metrics to find out how different two texts are from one another. In it's simplest form it can compare word to word by how many characters need removing, adding or changing (e.g. the fuzzy matching score between \"internationalization\" and \"internationalisation\" is 95.00 because one character needs changing. A score of 100 means the words are identical), this technique helps to pick up spelling mistakes and spelling differences. More complex versions of fuzzy matching allow for sentences to be compared, with options to ignore the order of words, duplicated words and partial matches within a sentence.</p> <p></p>","tags":["PYTHON","SECONDARY CARE","PRESCRIBING","TEXT DATA","LINKAGE","WIP"]},{"location":"our_work/epma_autocoding/#results","title":"Results","text":"<p>A review of the most common prescriptions and administrations found:</p> <ul> <li>93% correct</li> <li>4% wrong granularity - meaning extra information such as dose and form were provided but autocoding only mapped to the parent level</li> <li>3% incorrect</li> </ul> <p>Granularity:</p> Level Proportion Virtual Therapeutic  Moiety (VTM) 49.1% Virtual Medicinal Product (VMP) 42.6% Actual Medicinal Product (AMP) 8.3% <p>General:</p> <ul> <li>Richer data more useful for analysis and other downstream applications</li> <li>Pipeline capable of processing many thousands of records at a time</li> <li>Rules are able to be modified when new supplier systems of ePMA data are submitting</li> </ul> <p>Incorrect matches:</p> <p>Sometimes the autocoding will make a small number of incorrect matches and work is ongoing to improve methodology and minimise these. Where known incorrect matches are discovered, these are excluded from future mappings and will be published as known issues for analysts to be aware of.</p>","tags":["PYTHON","SECONDARY CARE","PRESCRIBING","TEXT DATA","LINKAGE","WIP"]},{"location":"our_work/epma_autocoding/#outputs","title":"Outputs","text":"Output Link Published repo Github Repo","tags":["PYTHON","SECONDARY CARE","PRESCRIBING","TEXT DATA","LINKAGE","WIP"]},{"location":"our_work/epma_autocoding/#_1","title":"ePMA Auto-Coding","text":"","tags":["PYTHON","SECONDARY CARE","PRESCRIBING","TEXT DATA","LINKAGE","WIP"]},{"location":"our_work/hsma-wlmds/","title":"Waiting List Minimum Dataset (WLMDS) Proofs of Concept","text":"","tags":["SECONDARY CARE","FORECASTING","MACHINE LEARNING","MODELLING","RESEARCH","STRUCTURED DATA","PSEUDONYMISED","PYTHON","R","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/hsma-wlmds/#background","title":"Background","text":"<p>As part of the Health Modelling Associates Programme (HSMA), you spend 9 months working on a project. A project that came out of the 2024 HSMA cohort was utilising the Waiting List Minimum Dataset (WLMDS), creating proof of concepts that evaluated the use of various data science techniques on this data. In particular, focusing on the differences of experience on the waiting list between people who are on one waiting list, versus people who may be on two or more waiting lists simultaneously (what we defined as \"multiple pathways\").</p> <p></p>","tags":["SECONDARY CARE","FORECASTING","MACHINE LEARNING","MODELLING","RESEARCH","STRUCTURED DATA","PSEUDONYMISED","PYTHON","R","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/hsma-wlmds/#codebase","title":"Codebase","text":"<p>A codebase to process the data into a usable format for modelling is a significant output of this project. It can be found on github (currently in a private repository but aiming to go public when a review is completed). This repository sets up an easy process for testing a range of data science techniques, and wrangling the dataset into a manageable asset for analytics. Below you can find an outline of the module for processing and analysis of WLMDS data</p> <p></p>","tags":["SECONDARY CARE","FORECASTING","MACHINE LEARNING","MODELLING","RESEARCH","STRUCTURED DATA","PSEUDONYMISED","PYTHON","R","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/hsma-wlmds/#modelling-techniques","title":"Modelling Techniques","text":"<p>The first data science technique that a proof of concept was created for is clustering. Several clustering techniques were tested to try and identify whether there were clear groups of patients that were more likely to have single or multiple pathways, and if there were groupings, particularly for multiple pathways, identify whether some way of colocating services would be appropriate.</p>","tags":["SECONDARY CARE","FORECASTING","MACHINE LEARNING","MODELLING","RESEARCH","STRUCTURED DATA","PSEUDONYMISED","PYTHON","R","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/hsma-wlmds/#forecasting","title":"Forecasting","text":"<p>We also tested the implementation of forecasting models, predicting single vs multiple pathways demand over a 12-week forecast using Prophet. This was a very basic proof of concept, but the architecture can be easily expanded if necessary.</p>","tags":["SECONDARY CARE","FORECASTING","MACHINE LEARNING","MODELLING","RESEARCH","STRUCTURED DATA","PSEUDONYMISED","PYTHON","R","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/hsma-wlmds/#binary-classifiers","title":"Binary Classifiers","text":"<p>In this proof of concept, the aim was to identify whether we can predict classes of patients that are more likely to end up on multiple pathways. We tested a Gradient Boosting Classifier, a Logistic Regression model, and an XGBoost Model. This was challenging due to a lack of a wide range of features we could use, and a massively imbalanced dataset (as many more patients are on only one pathway at a time as opposed to two or more). We ended up with a model with almost 60% accuracy, however with more time it has a great potential for improvement, and the pipeline is well set up to allow this.</p> Output Link repo (currently private) Github Repo HSMA website Website Blog post Blog","tags":["SECONDARY CARE","FORECASTING","MACHINE LEARNING","MODELLING","RESEARCH","STRUCTURED DATA","PSEUDONYMISED","PYTHON","R","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/hsma-wlmds/#_1","title":"Waiting List Minimum Dataset (WLMDS) Proofs of Concept","text":"","tags":["SECONDARY CARE","FORECASTING","MACHINE LEARNING","MODELLING","RESEARCH","STRUCTURED DATA","PSEUDONYMISED","PYTHON","R","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/long-stay-baseline/","title":"Long Stayer Risk Stratification Baseline Models","text":"<p>Long Stayer risk stratification baseline models was selected as a project to run in tandem with the Long Stayer Risk Stratification project, and started in March 2022.</p> <p>Baseline models provide a mechanism to generate baseline metrics to assess the performance of more complex models, and establish the effectiveness of simple approaches.</p> <p>Intended Audience</p> <p>This report has been written for analysts and data scientists at NHS Trusts/ALBs</p> <p>A series of Jupyter Notebooks used to generate this report are available on Github.</p>","tags":["SECONDARY CARE","MACHINE LEARNING","CLASSIFICATION","MODELLING","STRUCTURED DATA","PYTHON","SQL","COMPLETE"]},{"location":"our_work/long-stay-baseline/#1-background","title":"1. Background","text":"<p>Hospital long stayers, those with a length of stay (LoS) of 21 days or longer, have significantly worse medical and social outcomes than other patients. Long-stayers are often medically optimised (fit for discharge) many days before their actual discharge. Moreover, there are a complex mixture of medical, cultural and socioeconomic factors which contribute to the causes of unnecessary long stays.</p> <p>This project aims to complement previous work by generating simple baseline regression and classification models that could be replicated at other hospital trusts, and is divided into two phases:</p> <ol> <li>Series of Jupyter Notebooks containing baseline model code</li> <li>Reproducible Analytical Pipeline including data pipelines</li> </ol> <p>Currently, this project has completed Phase 1.</p>","tags":["SECONDARY CARE","MACHINE LEARNING","CLASSIFICATION","MODELLING","STRUCTURED DATA","PYTHON","SQL","COMPLETE"]},{"location":"our_work/long-stay-baseline/#2-approach","title":"2. Approach","text":"<p>The aim of this project is to perform the simplest possible feature engineering and modelling to arrive at a reasonable baseline model, for more advanced feature engineering and modelling to be compared against.</p> <p>The approach involved:</p> <ol> <li>Defining the population and data cleaning</li> <li>Feature engineering, focussing on basic numerical and categorical features</li> <li>Simple baseline models implemented using commonly available packages including scikit-learn 1.1.1, CatBoost 1.0.6 and XGBoost 1.3.3</li> <li>Analysis of model performance by demographic</li> <li>A comparison of regression-based and classification-based risk stratification models</li> <li>A set of extensions for future work</li> </ol>","tags":["SECONDARY CARE","MACHINE LEARNING","CLASSIFICATION","MODELLING","STRUCTURED DATA","PYTHON","SQL","COMPLETE"]},{"location":"our_work/long-stay-baseline/#3-data-ingest-and-processing","title":"3. Data ingest and processing","text":"<p>GHNHSFT performed a SQL export from their EPR system containing ~770,000 records across 99 columns, with significant sparsity across several columns and a section of rows, as visualised by light coloured blocks (null values) in the below image:</p> <p></p> <p>Figure 1. Plot of data sparsity of raw data. Null values are light coloured blocks. (Note that not all columns are labelled)</p> <p>The population for this study was defined as non-elective, major cases as recorded in the <code>IS_MAJOR</code> and <code>elective_or_non_elective</code> fields.</p> <p>Filtering the dataset to this definition resulted in a reduction of ~770,000 rows to ~170,000 rows (78% reduction):</p> <p></p> <p>Figure 2. Plot of data sparsity of \"major\" cases. Null values are light coloured blocks. (Note that not all columns are labelled)</p> <p>Data was processed by:</p> <ol> <li>Converting datetime columns into the correct data type</li> <li>Ordering records by <code>START_DATE_TIME_HOSPITAL_PROVIDER_SPELL</code></li> <li>Removing fields not available at admission</li> <li>Removing empty and redundant (e.g. <code>LENGTH_OF_STAY_IN_MINUTES</code> duplicates <code>LENGTH_OF_STAY</code>) columns</li> <li>Removing duplicate rows</li> <li>Removing local identifiers</li> <li>Imputing <code>stroke_ward_stay</code> as <code>N</code> if not specified</li> <li>Binary encoding <code>stroke_ward_stay</code>, <code>IS_care_home_on_admission</code>, <code>IS_care_home_on_discharge</code> and <code>IS_illness_not_injury</code></li> </ol> <p>This resulted in ~170,000 rows across ~50 columns as visualised in the below image:</p> <p></p> <p>Figure 3. Plot of data sparsity of clean data. Null values are light coloured blocks. (Note that not all columns are labelled)</p> <p>The resulting data dictionary is available here.</p> <p>Additionally, Length of Stay was capped to 30 days, due to a long tail of long stayers over ~15 days and the definition of long stayer being over 21 days. The effect of capping can be visualised by comparing box plots of the distribution of length of stay on the raw data (left image, y scale up to 300 days) and the capped data (right image, y scale up to 30 days):</p> <p></p> <p>Figure 4. Plot of the distribution of long stayers in the raw (left) data and capped (right) data. Note different y scales.</p> <p>The resulting distribution of length of stays shows a ~bimodal distribution caused by the capping - the majority of stays are short (&lt;5 days), which a long tail and population of long stayers:</p> <p></p> <p>Figure 5. Plot of density of length of stay for capped data.</p>","tags":["SECONDARY CARE","MACHINE LEARNING","CLASSIFICATION","MODELLING","STRUCTURED DATA","PYTHON","SQL","COMPLETE"]},{"location":"our_work/long-stay-baseline/#4-feature-engineering","title":"4. Feature engineering","text":"<p>After discussion with GHNHSFT, the following decisions were made in feature selection:</p> <ol> <li> <p>Select the following features for inclusion in the model, which are available on admission:</p> <pre><code>\"ae_arrival_mode\",\n\"AGE_ON_ADMISSION\",\n\"EL CountLast12m\",\n\"EMCountLast12m\",\n\"IS_illness_not_injury\",\n\"IS_cancer\",\n\"IS_care_home_on_admission\",\n\"IS_chronic_kidney_disease\",\n\"IS_COPD\",\n\"IS_coronary_heart_disease\",\n\"IS_dementia\",\n\"IS_diabetes\",\n\"IS_frailty_proxy\",\n\"IS_hypertension\",\n\"IS_mental_health\",\n\"MAIN_SPECIALTY_CODE_AT_ADMISSION_DESCRIPTION\",\n\"OP First CountLast12m\",\n\"OP FU CountLast12m\",\n\"SOURCE_OF_ADMISSION_HOSPITAL_PROVIDER_SPELL_DESCRIPTION\",\n\"stroke_ward_stay\",\n\"LENGTH_OF_STAY\",\n\"arrival_day_of_week\",\n\"arrival_month_name\"\n</code></pre> </li> <li> <p>Exclude the following features, but retain for later analysis of model fairness:</p> <pre><code>\"ETHNIC_CATEGORY_CODE_DESCRIPTION\",\n\"IMD county decile\",\n\"OAC Group Name\",\n\"OAC Subgroup Name\",\n\"OAC Supergroup Name\",\n\"PATIENT_GENDER_CURRENT_DESCRIPTION\",\n\"POST_CODE_AT_ADMISSION_DATE_DISTRICT\",\n\"Rural urban classification\"\n</code></pre> </li> <li> <p>Generate <code>arrival_day_of_week</code> and <code>arrival_month_name</code> recalculated from <code>START_DATE_TIME_HOSPITAL_PROVIDER_SPELL</code></p> </li> </ol> <p>This resulted in a dataset of ~170,000 rows across 30 columns.</p> <p>One-hot encoding was performed for categorical variables, but non-one-hot encoded features were also kept for models like CatBoost which manage categorical variables themselves.</p>","tags":["SECONDARY CARE","MACHINE LEARNING","CLASSIFICATION","MODELLING","STRUCTURED DATA","PYTHON","SQL","COMPLETE"]},{"location":"our_work/long-stay-baseline/#5-statistical-analysis","title":"5. Statistical analysis","text":"<p>In order to select appropriate modelling approaches, some basic statistical analysis was conducted to understand normality and inter-correlation of the selected features.</p>","tags":["SECONDARY CARE","MACHINE LEARNING","CLASSIFICATION","MODELLING","STRUCTURED DATA","PYTHON","SQL","COMPLETE"]},{"location":"our_work/long-stay-baseline/#correlation-analysis","title":"Correlation analysis","text":"<p>Correlation analysis confirmed presence of significant collinearity between different features.</p> <p>Top 20 one-hot encoded features correlated with <code>LENGTH_OF_STAY</code>, ranked by absolute correlation, were:</p> <p></p> <p>Figure 6. Plot of top 20 correlated features with <code>LENGTH_OF_STAY</code>. Blue columns are positively correlated (ie. increase length of stay) and red columns are negatively correlated (ie. reduce length of stay).</p> <p>These indicate that age and age-related illness, as well as arrival mode are strong factors in determining length of stay.</p>","tags":["SECONDARY CARE","MACHINE LEARNING","CLASSIFICATION","MODELLING","STRUCTURED DATA","PYTHON","SQL","COMPLETE"]},{"location":"our_work/long-stay-baseline/#variation-inflation-factors","title":"Variation inflation factors","text":"<p>Variation inflation factors (VIF) confirmed the presence of multi-colinearity between a number of features (VIF &gt; 10).</p>","tags":["SECONDARY CARE","MACHINE LEARNING","CLASSIFICATION","MODELLING","STRUCTURED DATA","PYTHON","SQL","COMPLETE"]},{"location":"our_work/long-stay-baseline/#homoescadisticity","title":"Homoescadisticity","text":"<p>A basic ordinary least squares (OLS) regression model was fitted to the full feature set, then residuals calculated.</p> <p>Residuals failed Shapiro-Wilk, Kolmogorov-Smirnov and Anderson-Darling tests for normality, as well as visual inspection:</p> <p></p> <p>Figure 7. Plot of residuals (errors) in an OLS model of length of stay for all data.</p> <p>OLS methods were therefore excluded from modelling.</p>","tags":["SECONDARY CARE","MACHINE LEARNING","CLASSIFICATION","MODELLING","STRUCTURED DATA","PYTHON","SQL","COMPLETE"]},{"location":"our_work/long-stay-baseline/#6-modelling","title":"6. Modelling","text":"<p>The machine learning modelling approach was as follows:</p> <ol> <li>Split the data into a training (70%), validation (15%) and test (15%) data set</li> <li>Check the data splits do not introduce selection bias for length of stay, age, sex, or ethnicity</li> <li>Train baseline models with default parameters on the training set</li> <li>Evaluate baseline models on the validation test</li> <li>Select the best performing model</li> <li>Tune the best performing model using cross-validation on the training and validation set</li> <li>Report the final performance of the model using the test set</li> </ol> <p></p> <p>Figure 8. Summary of machine learning approach used in this project.</p> <p>Training, validation and test splits were representative of the population and did not introduce selection bias:</p> <p>Length of stay</p> <p></p> <p>Age</p> <p></p> <p>Sex</p> <p>Proportion of <code>male</code>, <code>female</code> patients in each split:</p> <pre><code>train: [0.53, 0.47]\nvalidate: [0.51, 0.49]\ntest: [0.53, 0.47]\n</code></pre> <p>Ethnicity</p> <p>Proportions of each ethnicity for each split:</p> <pre><code>train: [0.87, 0.05, 0.02, 0.02, 0.01, 0.01, 0.01, 0.0, ...]\nvalidate: [0.88, 0.05, 0.03, 0.02, 0.01, 0.01, 0.01, 0.0, ...]\ntest: [0.87, 0.05, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, ...]\n</code></pre>","tags":["SECONDARY CARE","MACHINE LEARNING","CLASSIFICATION","MODELLING","STRUCTURED DATA","PYTHON","SQL","COMPLETE"]},{"location":"our_work/long-stay-baseline/#61-regression-models","title":"6.1 Regression models","text":"<p>A range of baseline regression models were selected:</p> Model Rationale Mean The simplest baseline, uses the mean length of stay as the prediction in all cases ElasticNet A regularised implementation of linear regression that can be used for multi-colinear datasets such as in this dataset DecisionTreeRegressor A simple, single tree regressor that is highly explainable RandomForestRegressor An ensemble of decision trees with potentially better performance than a single tree XGBRegressor A boosted tree technique that can improve on ensemble techniques such as RandomForest CatBoostRegressor A boosted tree technique designed specifically for datasets with high levels of categorical features as in this dataset <p>Each model was trained with default parameters, and evaluated using root mean squared error (RMSE) on both the training set and then on the (unseen) validation set:</p> Model Training RMSE (days) Validation RMSE (days) Mean 6.89 6.94 ElasticNet 6.55 6.60 DecisionTree 0.55 9.11 RandomForest 2.46 6.52 XGBoost 5.97 6.32 CatBoost 6.13 6.26 <p>The best performing baseline model was Catboost with an RMSE of 6.26 days. Both DecisionTree and RandomForest models overfit the training data, as seen with low training RMSE resulting in much higher validation RMSE.</p> <p>A single metric (e.g. RMSE) does not capture the behaviour of each model, so we visualise both the Predicted vs Actual plots as well as the corresponding relative error for both the training set and the validation set:</p> <p>Training performance:</p> <p></p> <p>Figure 11. Plots of predicted vs actual (left, red dashed line shows ideal model) and corresponding relative errors (right, red solid line shows mean error with 95% limits of agreement in green dashed lines) on the training dataset.</p> <p>The RandomForest model appears to fit the training data well, but when compared with the performance on the validation set below, we can see this is due to overfitting on the training data set:</p> <p>Validation performance:</p> <p></p> <p>Figure 12. Plots of predicted vs actual (left, red dashed line shows ideal model) and corresponding relative errors (right, red solid line shows mean error with 95% limits of agreement in green dashed lines) on the validation dataset.</p> <p>In all cases, the poor predictive power at higher length of stays is evident - there appears to be a linear increase in error caused by the models' inability to predict higher length of stays.</p> <p>This is likely due to the bimodal nature of the underlying length of stay values - most stayers are short, while there is a significant portion of long stayers.</p> <p>Further tuning of the CatBoost model using GridSearch and cross-validation led to the following results:</p> Parameter Optimal value <code>depth</code> 6 <code>l2_leaf_reg</code> 9 <code>learning_rate</code> 0.1 <p>with</p> Model Training RMSE (days) Validation RMSE (days) Test RMSE (days) Test MAE (days) CatBoost (tuned) 6.24 6.18 6.06 4.12 <p>The test MAE of 4.12 days compares reasonably well to the previous work using a convolutional neural network which achieved a MAE of 3.8 days.</p> <p>However, a plot of predicted vs actual using the test dataset shows again the model's inability to capture long stayers:</p> <p></p> <p>Figure 13. Plots of predicted vs actual (left, red dashed line shows ideal model) and corresponding relative errors (right, red solid line shows mean error with 95% limits of agreement in green dashed lines) on the test dataset for the final model.</p> <p>We can still explore the most important features that make up the prediction by plotting feature importances of the final model:</p> <p></p> <p>Figure 14. Feature importances for the final regression model.</p> <p>These broadly align with the correlated features explored earlier on - namely, age, arrival mode, serious illness but also include the number of previous visits, which can be considered a proxy for serious illness itself.</p> <p>Because the final model, using CatBoost, does not include one-hot encoding of the categorical data as CatBoost deals with this internally, we don't have further granularity on admission mode and arrival mode to compare.</p>","tags":["SECONDARY CARE","MACHINE LEARNING","CLASSIFICATION","MODELLING","STRUCTURED DATA","PYTHON","SQL","COMPLETE"]},{"location":"our_work/long-stay-baseline/#62-demographic-analysis","title":"6.2 Demographic analysis","text":"<p>While the model is not performant enough to deploy into production, it is still important to understand whether or not the model incorporates bias into its predictions.</p> <p>There are many kinds of bias in machine learning projects, and here we are looking at representation bias:</p> <p>Does the model perform better or worse for specific categories of people across sex, ethnicity and other demographics?</p> <p>The specific categories are:</p> <pre><code>\"ETHNIC_CATEGORY_CODE_DESCRIPTION\", \"IMD county decile\", \"OAC Group Name\", \"OAC Subgroup Name\", \"OAC Supergroup Name\", \"PATIENT_GENDER_CURRENT_DESCRIPTION\", \"POST_CODE_AT_ADMISSION_DATE_DISTRICT\", \"Rural urban classification\"\n</code></pre> <p>Before looking at model performance, we need to understand how represented each category is, before drawing conclusions on categories with small sample size (note that for brevity we will only share results from <code>\"ETHNIC_CATEGORY_CODE_DESCRIPTION\", \"IMD county decile\",\"PATIENT_GENDER_CURRENT_DESCRIPTION\"</code>):</p> <p></p> <p>Figure 15. Underlying counts for ethnicity - all data.</p> <p>We can see that for <code>ETHNIC_CATEGORY_CODE_DESCRIPTION</code>, the overwhelming majority of patients report <code>British</code>. We should be careful what conclusions we draw in further analysis about smaller categories, as the sample size will be very small and likely not statistically representative.</p> <p></p> <p>Figure 16. Underlying counts for sex - all data.</p> <p>Sex is broadly equal, with slightly more female than male patients in this dataset.</p> <p></p> <p>Figure 17. Underlying counts for index of multiple deprivation - all data</p> <p>Index of Multiple Deprivation (IMD) deciles are skewed to the lower end, ie. there are more deprived patients present in this dataset than not.</p> <p>Now we can look at the distribution of length of stay for the above categories:</p> <p></p> <p>Figure 18. Underlying length of stay by ethnicity - all data.</p> <p>There is significant variation of length of stay for different ethnic groups, for example with White and black Caribbean patients having a length of stay of 2.6 days on average, versus 6.0 days for Irish patients. However, as discussed previously, the count of these groups is 560 and 892 individuals respectively so further statistical hypothesis tests need to be conducted to understand if the distributions are truly different (e.g. a two-sided Kolmogorov-Smirnov test).</p> <p> Figure 19. Underlying length of stay by sex - all data.</p> <p>Mean length of stay is almost identical across patient sex.</p> <p></p> <p>Figure 20. Underlying length of stay by index of multiple deprivation - all data.</p> <p>There are small variations in length of stay across IMD deciles, although more tests need to be conducted to understand if these differences are statistically significant.</p> <p>Because we are interested in if the model performs differently by category, we will plot the error of the predictions of the test dataset relative to the overall (mean) error for all categories. This will help identify potential discrimination in model performance.</p> <p></p> <p>Figure 21. Relative error in length of stay predictions for different ethnic groups - test data.</p> <p>The model appears to perform significantly worse for Caribbean (overestimating length of stay by 2.7 days compared to the mean error) and Any other mixed background (underestimating length of stay by 1.8 days compared to the mean error). Sample sizes are 719 and 536 patients respectively. As discussed the small sample sizes need further investigation and/or additional data collection to establish the statistical significance of this performance difference.</p> <p></p> <p>Figure 22. Relative error in length of stay predictions for different sex - test data.</p> <p>Sex has almost no (0.002 days) error from the average.</p> <p></p> <p>Figure 23. Relative error in length of stay predictions for different index of multiple deprivations deciles - test data.</p> <p>The lowest IMD county decile (1) has an error of 0.5 days underestimating from the mean error, which at under a day may not lead to any difference in treatment if this prediction is used in clinical practice (ie. a length of stay of 1.5 days is the same as a length of stay of 2.0 days - both would count as 2 whole days).</p> <p>We also know that length of stay varies by group, so further plots of the ratio of MAE to length of stay are generated in the notebooks, but not included here for brevity.</p> <p>The final model generated did not adequately capture length of stay across the population. Some sample sizes of demographic groups were too small to draw conclusions, but the process of exploring the underlying distribution of the target feature (length of stay), count (n) and model performance were important and should remain part of future work.</p>","tags":["SECONDARY CARE","MACHINE LEARNING","CLASSIFICATION","MODELLING","STRUCTURED DATA","PYTHON","SQL","COMPLETE"]},{"location":"our_work/long-stay-baseline/#63-classification-models","title":"6.3 Classification models","text":"<p>In addition to predicting the length of stay in days, we are also interested in stratifying the risk of a patient becoming a long stayer. This can be inferred from their predicted length of stay (see model comparison), but we can also train a classification model to do this directly.</p> <p>The agreed stratification of risk of long stay is defined as:</p> Risk Category Day Range for Risk Category 1 - Very low risk 0-6 2 - Low risk 7-10 3 - Normal risk 11-13 4 - Elevated risk 14-15 5 - High risk &gt;15 <p>We keep the training features the same as in the regression models, and encode risk from the actual length of stay as the target feature.</p> <p>Postscript: classification models based on increasing risk (1-5) are ordinal in nature, and an appropriate model should be used where different classes are not treated as independent as per the examples in this implementation.</p> <p>The classification equivalents of the regression models were selected:</p> Model Regression version Classification version Dummy Mean Prior ElasticNet ElasticNet LogisticRegression Decision Tree DecisionTreeRegressor DecisionTreeClassifier Random Forest RandomForestRegressor RandomForestClassifier XGBoost XGBRegressor XGBClassifier CatBoost CatBoostRegressor CatBoostClassifier <p>The training, validation and test regime was the same as for the regression models.</p> <p>Class imbalance</p> <p>In the training set, we observe the following class imbalance:</p> Risk score Number of patients % of total patients 1 89711 74.0 2 12634 10.4 3 5226 9.1 4 2613 4.3 5 10990 2.2 <p>ie. the majority of patients are low risk, and the highest risk group is only 2.2% of the population.</p> <p>Class and/or sample weights were calculated using the above training imbalances and passed into all models.</p> <p>Models were trained using default parameters, and evaluated using the weighted F1 score which represents the balance between precision and recall, and accounts for class imbalance. F1 scores range from 0 to 1 (where 1 is \"ideal\" or maximum).</p> Model Training weighted F1 score Validation weighted F1 score Prior 0.63 0.62 ElasticNet 0.54 0.53 DecisionTree 1.00 0.59 RandomForest 1.00 0.64 XGBoost 0.57 0.54 CatBoost 0.57 0.54 <p>While the RandomForest model obtained the highest validation weighted F1 score (0.64), it also overfit the training data (weighted F1 score of 1.00).</p> <p>A visual inspection of model performance, plotting both total counts of risk categories in actual vs predicted cases, as well as the proportion of actual risk in each predicted category, are shown below for both the training and validation data sets:</p> <p>Training performance:</p> <p></p> <p>Figure 24. Plots of predicted vs actual risks on the training dataset. Left image shows count of actual and predicted risks for each category. Right image shows proportion of actual risk that makes up each predicted risk category.</p> <p>We can see that both the DecisionTree and RandomForest models severely overfit the training data.</p> <p>We also see that none of the models are able to capture the nature of the highest risk categories, with every risk category containing a large (&gt;50%) proportion of the lowest risk level (level 1). This is despite weighting the models to account for class imbalance.</p> <p>Validation performance:</p> <p></p> <p>Figure 25. Plots of predicted vs actual risks on the validation dataset. Left image shows count of actual and predicted risks for each category. Right image shows proportion of actual risk that makes up each predicted risk category.</p> <p>The RandomForest model has an anomaly in its predictions for risk category 4 where it is missing any of the highest risk category 5 compared to other predictions. This is likely due to the overfitting observed in the previous plot.</p> <p>Both CatBoost and XGBoost have similar levels of predictive power, defined by the lower proportion of very low risk in the predictions for high risk, although at ~50% these are still too high.</p> <p>Both CatBoost and XGBoost over-predict higher risk categories, while under-predicting the lowest risk category. This will lead both to false positives where very low risk cases are shown as high risk, and false negatives where high risk cases are shown as lower risk.</p> <p>CatBoost was selected as the final model due to the lack of significant difference in performance with XGBoost, and for consistency with the final regression model. Further tuning of the CatBoost model using GridSearch (with a smaller parameter space than with regression due to compute time) and cross-validation led to the following results:</p> Parameter Optimal value <code>depth</code> 10 <code>l2_leaf_reg</code> 1 <code>learning_rate</code> 0.1 <p>with</p> Model Training weighted F1 score Validation weighted F1 score Test weighted F1 score Test balanced accuracy Test AUC (OVR, weighted) CatBoost (tuned) 0.61 0.75 0.60 0.27 0.70 <p>Balanced accuracy was determined as 0.27, a poor result for accurately predicting the correct class. The overall Area Under the receiving operator characteristic Curve (AUC), which was calculated as a weighted one-versus-rest metric, was 0.70.</p> <p></p> <p>Figure 26. Plots of predicted vs actual risks on the test dataset for the final model. Left image shows count of actual and predicted risks for each category. Right image shows proportion of actual risk that makes up each predicted risk category.</p> <p>The final model still assigns over 50% of the lower risk class (the most populated class) to every predicted class, which would lead to a high number of false positives. It also fails to capture the highest risk class adequately, leading to a high number of false negatives.</p> <p>Despite the poor performance, we can still explore the most important features that make up the prediction by plotting feature importances of the final model:</p> <p></p> <p>Figure 27. Feature importances for the final regression model.</p> <p>In this case, <code>arrival_month_name</code> and <code>arrival_day_of_week</code> are the two most important features, which differs from the regression model and correlation analysis. This may be why the false positive and false negative rates for the model are so high, and needs further exploration.</p> <p>Demographic analysis of the risk stratification model was not conducted as the model performance did not justify exploring whether there was representation bias at this stage.</p>","tags":["SECONDARY CARE","MACHINE LEARNING","CLASSIFICATION","MODELLING","STRUCTURED DATA","PYTHON","SQL","COMPLETE"]},{"location":"our_work/long-stay-baseline/#64-model-comparison","title":"6.4 Model comparison","text":"<p>As a final modelling step, we can compare both the regression models and classification models, by encoding the predicted length of stay from the regression model as a corresponding risk.</p> <p>This comparison may help us understand whether a classification or regression approach is more suitable for this type of data.</p> <p></p> <p>Figure 28. Comparison of both models. Left image shows proportion of actual risk for each predicted risk category for the classification model. Right image shows proportion of actual risk for each equivalent predicted risk category derived from the regression model.</p> <p>Here we can see that the regression model, encoded as a risk stratification model, performs much better than the classification approach:</p> <ul> <li>The number of very low risk patients is much lower for higher risk patients, under 20% in the case of high risk. This means lower false positives.</li> <li>The proportion of high risk patients is higher in the predicted higher risk categories. This means lower false negatives.</li> </ul> <p>If risk stratification is the key desired output, then further refining the regression model may be the better approach to improving the overall performance of the system.</p>","tags":["SECONDARY CARE","MACHINE LEARNING","CLASSIFICATION","MODELLING","STRUCTURED DATA","PYTHON","SQL","COMPLETE"]},{"location":"our_work/long-stay-baseline/#7-conclusions","title":"7. Conclusions","text":"<p>A number of baseline machine learning models were trained on EPR data from GHNHSFT.</p> <p>The best performing regression model achieved a Mean Absolute Error of 4.1 days, compared to 3.8 days for previous work using a convolutional neural network.</p> <p>Simpler baseline models benefit from enhanced explainability and less compute resources for training. In this case the most important features were related to age and serious illness.</p> <p>The overall performance of the best regression model was still poor - despite an MAE of 4.1 days, the model failed to capture long stayers and requires further work before use.</p> <p>The best performing classification model achieved a weighted F1 score of 0.6.</p> <p>The overall performance of the best classification model was poor - the model failed to capture high risk and assigned a high proportion (&gt;50%) of very low risk patients to higher risk groups.</p> <p>Using the regression model to calculate equivalent risk scores led to a better risk stratification model, where only ~20% of very low risk patients were assigned to the high risk group.</p> <p>Demographic analysis showed that the model performed differently for different ethnicities and indices of multiple deprivation, but both model performance needs to be improved and sample sizes need to be increased in order to draw any meaning from these initial findings.</p> <p>There is opportunity for much future work, which should be balanced with the utility of these predictions in the clinical context.</p>","tags":["SECONDARY CARE","MACHINE LEARNING","CLASSIFICATION","MODELLING","STRUCTURED DATA","PYTHON","SQL","COMPLETE"]},{"location":"our_work/long-stay-baseline/#8-future-work","title":"8. Future work","text":"","tags":["SECONDARY CARE","MACHINE LEARNING","CLASSIFICATION","MODELLING","STRUCTURED DATA","PYTHON","SQL","COMPLETE"]},{"location":"our_work/long-stay-baseline/#modelling-improvements","title":"Modelling improvements","text":"<ol> <li>Feature engineering of free text fields. Early on we decided to focus on simple numerical and categorical features for this project. A huge amount of rich data is present in fields such as <code>presenting_complaint</code> and <code>reason_for_admission</code>.</li> <li>Including features available after admission. Fields such as <code>all_diagnoses</code> and <code>all_treatments</code> will provide clinically important information, and may improve the performance of the predictions.</li> <li>Focussing on a smaller number of features. Once the most important features are identified, a model using the top e.g. 10 features could be trained and tested.</li> <li>Building two models - one for short stay and one for long stay. This may help capture the bimodal nature of the underlying dataset.</li> <li>Including <code>MINOR</code> cases. This project focussed on <code>MAJOR</code>, <code>non-elective</code> cases. 70%+ of the original data belonged to minor cases, and in combination with the above, including this data could lead to an improvement in model performance.</li> <li>Treating Length of Stay as a discrete variable and applying poisson distribution appropriate approaches to modelling.</li> <li>Exploring Generalised Linear Models using e.g. pyGAM.</li> <li>Exploring Bayesian approaches.</li> <li>Exploring the addition of latent variable(s).</li> </ol>","tags":["SECONDARY CARE","MACHINE LEARNING","CLASSIFICATION","MODELLING","STRUCTURED DATA","PYTHON","SQL","COMPLETE"]},{"location":"our_work/long-stay-baseline/#demographic-analysis-improvements","title":"Demographic analysis improvements","text":"<ol> <li>Statistical testing of fairness. Once model performance reaches a sufficient level, further statistical tests of model performance across demographics should be conducted using e.g. a two-sided Kolmogorov-Smirnov test.</li> <li>Combine smaller groups. For example, grouping <code>British</code> and <code>Non-British</code> ethnicities would allow statistical comparisons to be made between the majority group and other groups.</li> </ol>","tags":["SECONDARY CARE","MACHINE LEARNING","CLASSIFICATION","MODELLING","STRUCTURED DATA","PYTHON","SQL","COMPLETE"]},{"location":"our_work/long-stay-baseline/#technical-improvements","title":"Technical improvements","text":"<ol> <li>Move from Notebooks to python scripts. Jupyter Notebooks are an excellent exploratory tool, but do not work well with version control or automated testing.</li> <li>Implement a Reproducible Analytical Pipeline. This will allow reuse of the approaches here and improve overall code quality.</li> <li>Abstract visualisation code into functions. This will improve readability of the code.</li> </ol>","tags":["SECONDARY CARE","MACHINE LEARNING","CLASSIFICATION","MODELLING","STRUCTURED DATA","PYTHON","SQL","COMPLETE"]},{"location":"our_work/long-stay-baseline/#acknowledgments","title":"Acknowledgments","text":"<ol> <li>Joe Green, GHNHSFT for presenting the challenge to Skunkworks and supporting problem definition/data selection</li> <li>Tom Lane, GHNHSFT for support in final stages</li> <li>Brad Pearce and Peter Coetzee, Polygeist, for the original CNN-based model</li> <li>Jennifer Hall, Matthew Cooper and Sanson Poon, NHS AI Lab Skunkworks for guidance, code and report review</li> <li>Chris Mainey, NHSE, for suggestions of additional modelling improvements</li> </ol> Output Link Open Source Code &amp; Documentation Github","tags":["SECONDARY CARE","MACHINE LEARNING","CLASSIFICATION","MODELLING","STRUCTURED DATA","PYTHON","SQL","COMPLETE"]},{"location":"our_work/long-stay-baseline/#_1","title":"Long Stayer Risk Stratification Baseline Models","text":"","tags":["SECONDARY CARE","MACHINE LEARNING","CLASSIFICATION","MODELLING","STRUCTURED DATA","PYTHON","SQL","COMPLETE"]},{"location":"our_work/long-stay/","title":"Long Stayer Risk Stratification","text":"<p>As the successful candidate from the AI Skunkworks problem-sourcing programme, Long Stayer Risk Stratification was first picked as a pilot project for the AI Skunkworks team in April 2021.</p>","tags":["SECONDARY CARE","MACHINE LEARNING","NEURAL NETWORKS","MODELLING","STRUCTURED DATA","PYTHON","SQL","COMPLETE"]},{"location":"our_work/long-stay/#results","title":"Results","text":"<p>A proof-of-concept demonstrator written in Python (machine learning model and backend), and HTML/CSS/JavaScript (frontend).</p>","tags":["SECONDARY CARE","MACHINE LEARNING","NEURAL NETWORKS","MODELLING","STRUCTURED DATA","PYTHON","SQL","COMPLETE"]},{"location":"our_work/long-stay/#case-study","title":"Case Study","text":"","tags":["SECONDARY CARE","MACHINE LEARNING","NEURAL NETWORKS","MODELLING","STRUCTURED DATA","PYTHON","SQL","COMPLETE"]},{"location":"our_work/long-stay/#info","title":"Info","text":"<p>This is a backup of the case study published here on the NHS England Transformation Directorate website.</p>","tags":["SECONDARY CARE","MACHINE LEARNING","NEURAL NETWORKS","MODELLING","STRUCTURED DATA","PYTHON","SQL","COMPLETE"]},{"location":"our_work/long-stay/#case-study_1","title":"Case Study","text":"<p>Studies show that many patients who stay in hospital for extended periods experience negative outcomes: According to the Journal of Gerontology, \u201cTen days of bed rest in hospital leads to the equivalent of 10 years ageing in the muscles of people over 80\u201d (Kortbein et al 2004). Long stays also create problems for busy hospitals because beds stay occupied for longer and require extra resources to manage. In this context, a long stay is regarded as any stay longer than 21 days.</p> <p>These longer length of stays are often avoidable, as one study showed 60% of immobile older patients had no medical reason that required bed rest (Graf 2006, American Journal of Nursing).</p> <p>The Business Intelligence team at Gloucestershire Hospital (GHFT), supported by GHFT's CIO and senior clinical leaders, developed an idea to use AI to address the issue of \u201clong stayers\u201d and was awarded a rapid feasibility project as part of the Skunkworks\u2019 third round of competition at the NHS AI Lab.</p> <p>Long stayers at the Gloucestershire Hospitals Trust occupy an average of 278 beds per day, which is around 4% of all admissions but accounts for 34% of bed use. The ability to identify and intervene early could make a real difference to these patients.</p>","tags":["SECONDARY CARE","MACHINE LEARNING","NEURAL NETWORKS","MODELLING","STRUCTURED DATA","PYTHON","SQL","COMPLETE"]},{"location":"our_work/long-stay/#overview","title":"Overview","text":"<p>Sarah Hammond, Gloucestershire Hospitals\u2019 Associate CIO, and Joe Green, Deputy Head of Business Intelligence, report that more than 30% of bed days in all of the Gloucestershire Trust\u2019s acute hospitals are used by long stayers. It has been observed that long stayers have an 11% mortality rate during their hospital stay (compared to 5% of all admissions). Of these long stayers, 23% became unwell after being deemed medically fit for discharge (compared to 1% overall).</p> <p>Long stayers at the Gloucestershire Hospitals Trust occupy an average of 278 beds per day, which is around 4% of all admissions but accounts for 34% of bed use. The ability to identify and intervene early could make a real difference to these patients.</p> <p>After the completion of a rigorous Data Protection Impact Assessment (DPIA) and resulting Data Protection Agreement (DPA), the \u201clongstayers\u201d project had access to a good volume of data from over 7 years\u2019 worth of admissions information. By analysing this data using machine learning methods (computer algorithms that learn from data), the team hoped to understand whether it is possible to identify the patients most at risk of becoming \u201clongstayers\u201d.</p> <p>The challenge was to discover whether AI can provide a useful prediction solution. The follow up is whether that solution could be effectively and safely put into production and the results shared for wider use.</p> <p>The project sought to:</p> <ul> <li>predict which patients are most likely to become \"long stayers\" (stay in hospital for more than 21 days)</li> <li>provide hospital staff with a prototype tool that provides a visible long stay risk score on every electronic patient record as soon as a new patient arrives at the hospital</li> <li>maximise the learning opportunities from the project by working in the open and making the resulting source code available for continued - experimentation by other researchers and developers</li> <li>contribute to potential better outcomes for people staying in hospitals around the country.</li> </ul>","tags":["SECONDARY CARE","MACHINE LEARNING","NEURAL NETWORKS","MODELLING","STRUCTURED DATA","PYTHON","SQL","COMPLETE"]},{"location":"our_work/long-stay/#what-we-did","title":"What we did","text":"<p>Over a 12-week period, the project investigated an experimental approach to using AI to better understand the admissions data at Gloucestershire Hospitals. This rapid innovation is intended to explore a proof of concept that could help predict which patient will be a long stayer.</p> <p>Data from more than 1 million admissions, stored in a secure SQL warehouse, was joined to an electronic patient record system to link key patient demographic indicators to each admission. The data available included age, sex, deprivation level, geography, and admission history like a patient\u2019s presenting complaints, their socio-economic status, emergency department investigations and care home admissions.</p> <p>This provided a rich and ready dataset for the project using data that is owned and securely held by the trust. This therefore allowed the data to be used for research purposes when anonymised appropriately.</p> <p>The team:</p> <ul> <li>worked with clinicians to understand the problem and possible uses for the tool</li> <li>developed an historical dataset for the project using patient admissions information</li> <li>opted to progress a machine learning Generative Adversarial Networks (GANs) solution using an innovative AI algorithm</li> <li>explained which factors contributed to the likelihood of a patient becoming a long-stayer (by building a parametric risk model)</li> <li>tested how well the model worked against the expectations from the initial data analysis.</li> </ul>","tags":["SECONDARY CARE","MACHINE LEARNING","NEURAL NETWORKS","MODELLING","STRUCTURED DATA","PYTHON","SQL","COMPLETE"]},{"location":"our_work/long-stay/#why-a-gans-solution-was-chosen","title":"Why a GANs solution was chosen","text":"<p>GANs, first designed by Ian Godfellow, involves simultaneously training two models. This includes a generative model that captures the data distribution, and a discriminative model that estimates the probability that a sample came from the training data.</p> <p>The Generative Adversarial Networks (GANs) was chosen because the use of the GANs to predict length of stay from a structured dataset offered a novel and experimental approach to a problem that may have been more commonly solved with models such as gradient boosted trees.</p> <p>There are many types of GANs, for this project a deep-convolutional generative adversarial network (DC-GAN) was selected to predict the length of stay. This model architecture (which broadly followed the approach outlined here) allowed for flexibility with applications over a wide range of data processing problems and the ability to apply a convolutional methodology to mixed data.</p> <p>Through training it was clear that the discriminator portion of DC-GAN was most effective and that, combined with heuristics that were learned during an initial data-analysis phase, a new hybrid model could be used. Once the DC-GAN model had produced the length of stay estimate, this was fed into a risk model that used a cumulative density function model to produce a risk score for each patient\u2019s likelihood of becoming a long-term hospital stayer. The final output was a risk score for that patient, from 1 to 5, with 5 being the highest risk.</p>","tags":["SECONDARY CARE","MACHINE LEARNING","NEURAL NETWORKS","MODELLING","STRUCTURED DATA","PYTHON","SQL","COMPLETE"]},{"location":"our_work/long-stay/#the-outcomes-and-lessons-learned","title":"The outcomes and lessons learned","text":"<p>The project succeeded in achieving:</p> <ul> <li>a \u201clong stay risk\u201d model that successfully detects two-thirds of long stayers and stratifies the risk in a useful way</li> <li>identification of the factors that were predictive of long-stayers (specific to Gloucestershire)</li> <li>a model performance accuracy within 1% at all stages when the model was tested on unseen data before, during and after the 2020 COVID waves.</li> </ul> <p>This proof-of-concept risk identifier will enable hospital staff to look closely at whether the patients identified as having a high probability of a long stay could benefit from earlier interventions and changes to their care pathway.</p> <p>The team at Gloucestershire Hospitals is keen to heed the lessons learned by working together, and taking the next steps to understand what technical, compliance, and logistical requirements are necessary to adopt it.</p> <p>Our aim was to develop a proof of concept for a \u201clong stay risk\u201d score algorithm. Would it be possible to predict a patient\u2019s length of stay the minute they arrive at the front door? The initial \u201clong stay risk\u201d model successfully detects two-thirds of long stayers at time of arrival, or very soon after. \u2013 Joe Green, Deputy Head of Business Intelligence, Gloucestershire Hospitals Trust</p> <p>The results have been very positive. It is hoped the information will allow trust staff to carefully tailor a patient\u2019s care pathway accordingly. Based on a well-established evidence base showing the negative impacts of unnecessary long stays, the AI tool has the potential to lead to decrease in the length of hospital stays overall, with corresponding reductions in patient deterioration and mortality during admission, and also lead to reduced readmission rates.</p>","tags":["SECONDARY CARE","MACHINE LEARNING","NEURAL NETWORKS","MODELLING","STRUCTURED DATA","PYTHON","SQL","COMPLETE"]},{"location":"our_work/long-stay/#what-next","title":"What next?","text":"<p>Gloucestershire Hospitals Business Intelligence team reports: \u201cWe will soon begin taking the model output tables, which run every 15 minutes, into our electronic patient record system to test and evaluate with clinicians.\u201c</p> <p>The Skunkworks team will continue to support Gloucestershire Hospitals with how to further test and evaluate the model, and adopt it safely into wider hospital use. The team is also preparing to support other organisations considering a similar approach.</p>","tags":["SECONDARY CARE","MACHINE LEARNING","NEURAL NETWORKS","MODELLING","STRUCTURED DATA","PYTHON","SQL","COMPLETE"]},{"location":"our_work/long-stay/#who-was-involved","title":"Who was involved?","text":"<p>This project is a collaboration between NHSX, Gloucestershire Hospitals NHS Foundation Trust, Polygeist and the Home Office\u2019s Accelerated Capability Environment (ACE). The AI Lab Skunkworks exists within the NHS AI Lab to support the health and care community to rapidly progress ideas from the conceptual stage to a proof of concept.</p> <p>The NHS AI Lab is working with the Home Office programme: Accelerated Capability Environment (ACE) to develop some of its skunkworks projects, providing access to a large pool of talented and experienced suppliers who pitch their own vision for the project.</p> <p>Accelerated Capability Environment (ACE) is part of the Homeland Security Group within the Home Office. It provides access to more than 250 organisations from across industry, academia and the third sector who collaborate to bring the right blend of capabilities to a given challenge. Most of these are small and medium-sized enterprises (SMEs) offering cutting-edge specialist expertise.</p> <p>ACE is designed to bring innovation at pace, accelerating the process from defining a problem to developing a solution and delivering practical impact to just 10 to 12 weeks.</p> <p>Polygeist, a software company specialising in state-scale analytics, builds world-leading AI technology for defence, national security, law enforcement, and healthcare customers. The team for this project was able to produce a live system, producing insights, from a standing start, in 12 weeks.</p> Output Link Open Source Code &amp; Documentation Github Case Study Case Study Technical report On request: ai-skunkworks@nhsx.nhs.uk","tags":["SECONDARY CARE","MACHINE LEARNING","NEURAL NETWORKS","MODELLING","STRUCTURED DATA","PYTHON","SQL","COMPLETE"]},{"location":"our_work/long-stay/#_1","title":"Long Stayer Risk Stratification","text":"","tags":["SECONDARY CARE","MACHINE LEARNING","NEURAL NETWORKS","MODELLING","STRUCTURED DATA","PYTHON","SQL","COMPLETE"]},{"location":"our_work/ndrs_embedded_ds/","title":"Embedded Data Scientists in the National Disease Registration Service (NDRS)","text":"","tags":["GENOMICS DATA","POPULATION HEALTH","SQL","R","TEXT DATA","LINKAGE"]},{"location":"our_work/ndrs_embedded_ds/#aim","title":"Aim","text":"<p>The aim of this collaboration is to support NDRS with implementing data science based projects and support the upskilling of staff. </p>","tags":["GENOMICS DATA","POPULATION HEALTH","SQL","R","TEXT DATA","LINKAGE"]},{"location":"our_work/ndrs_embedded_ds/#projects","title":"Projects","text":"<p>We have supported NDRS with a range of projects.</p>","tags":["GENOMICS DATA","POPULATION HEALTH","SQL","R","TEXT DATA","LINKAGE"]},{"location":"our_work/ndrs_embedded_ds/#clinical-measurement-extractor","title":"Clinical Measurement Extractor","text":"<p>This is an LLM pipeline to extract out clinical measurments from free-text data. This is using AWS services such as BedRock and SageMaker. For more information, please follow the link to this project page.</p> Output Link Private repo Github Repo","tags":["GENOMICS DATA","POPULATION HEALTH","SQL","R","TEXT DATA","LINKAGE"]},{"location":"our_work/ndrs_embedded_ds/#ons-population-rap-pipeline","title":"ONS Population RAP Pipeline","text":"<p>This is a gold-level RAP, R-based codebase that scrapes population data from the ONS website to produce three tables reporting population counts broken down by year, LSOA, gender, age, and age bands. This data is used by NDRS analysts to support the calculation of population-based metrics.</p> Output Link Private repo Github Repo","tags":["GENOMICS DATA","POPULATION HEALTH","SQL","R","TEXT DATA","LINKAGE"]},{"location":"our_work/ndrs_embedded_ds/#germline-count-rap-pipeline","title":"Germline Count RAP Pipeline","text":"<p>An R-based codebase was developed to create visualisations showing the monthly and yearly counts of specific germline tests, as well as the percentage change in counts over time. The goal is to help evaluate the data quality of germline test counts and assess appropriate timelines for data sharing.</p> Output Link Private repo Github Repo","tags":["GENOMICS DATA","POPULATION HEALTH","SQL","R","TEXT DATA","LINKAGE"]},{"location":"our_work/ndrs_embedded_ds/#supporting-the-cancer-genomics-improvement-programme","title":"Supporting the Cancer Genomics Improvement Programme","text":"<p>The aim of this project is to link multiple datasets to capture the cancer diagnosis-to-treatment pathway, specifically for cases where genetic testing is required. Support for this project involved writing a hybrid of SQL and R scripts to define the cohort of interest and to extract relevant dates and events from pathology testing.</p> Output Link Private repo Github Repo","tags":["GENOMICS DATA","POPULATION HEALTH","SQL","R","TEXT DATA","LINKAGE"]},{"location":"our_work/ndrs_embedded_ds/#_1","title":"Embedded Data Scientists in the National Disease Registration Service (NDRS)","text":"","tags":["GENOMICS DATA","POPULATION HEALTH","SQL","R","TEXT DATA","LINKAGE"]},{"location":"our_work/nhs-resolution/","title":"Predicting negligence claims with NHS Resolution","text":"<p>NHS Resolution provides expertise to the NHS on resolving concerns and disputes. The organisation holds a wealth of historic data around claims, giving insight and valuable data around the causes and impacts of harm.</p> <p>The NHS Resolution team wanted to understand whether AI methods could be applied to their data to better understand and identify risk, preventing harm and saving valuable resources.</p> <p>We aimed to prove the value of machine learning in determining insights from the available data. Automated machine learning was used to run repeated processes on the available data in order to select the AI models that uncovered the most relevant information.</p>","tags":["WORKFORCE","FINANCIAL","MACHINE LEARNING","CLASSIFICATION","MODELLING","STRUCTURED DATA","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/nhs-resolution/#case-study","title":"Case Study","text":"<p>This is a backup of the case study published here on the NHS England Transformation Directorate website.</p> <p>NHS trusts undertake an enormous number of activities each year (around 240 million in the year 2018 to 2019 according to the Kings Fund). The vast majority of people receive safe care, however NHS Resolution received over 15,000 claims for compensation last year on behalf of the NHS in England, which includes hospital trusts and GPs. Although many (almost half the claims in 2018 to 2019) get settled without damages, NHS Resolution figures show that claims can cost the health and care system up to \u00a32.6 billion a year.</p> <p>In early 2021, the NHS AI Lab Skunkworks team started a rapid feasibility study with NHS Resolution to investigate whether it is possible to use machine learning AI to predict the number of claims a trust is likely to receive and learn what drives them in order to improve safety for patients.</p>","tags":["WORKFORCE","FINANCIAL","MACHINE LEARNING","CLASSIFICATION","MODELLING","STRUCTURED DATA","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/nhs-resolution/#overview","title":"Overview","text":"<p>NHS Resolution provides expertise to the NHS on resolving concerns and disputes fairly, shares learning to enable organisation-wide improvement and to preserve valuable resources for patient care.</p> <p>The organisation holds a wealth of historic data around claims, giving insight and valuable data around the causes and impacts of harm.</p> <p>Over a 10-week period, the project sought to:</p> <ul> <li>improve patient safety by reducing the time lag between incidents and the detection of claims, giving the opportunity for prevention</li> <li>create a risk profile for individual trusts by analysing a variety of NHS data</li> <li>better understand future costs to the NHS by predicting the volume of incidents that lead to a claim</li> <li>help reduce new claims arising by understanding the factors that drive the likelihood of a claim</li> </ul> <p>The NHS AI Lab Skunkworks team worked with NHS Resolution and Accelerated Capability Environment (ACE) suppliers to provide a rapid delivery plan to:</p> <ul> <li>develop a machine learning model that could predict claims</li> <li>produce a code pipeline that could prepare input data, then train and run the chosen model.</li> </ul>","tags":["WORKFORCE","FINANCIAL","MACHINE LEARNING","CLASSIFICATION","MODELLING","STRUCTURED DATA","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/nhs-resolution/#how-the-ai-machine-learning-was-developed","title":"How the AI machine learning was developed","text":"<p>The project aimed to prove the value of machine learning in determining insights from the available data. Automated machine learning was used to run repeated processes on the available data in order to select the AI models that uncovered the most relevant information.</p> <p>NHS Resolution provided multiple sources of data from the healthcare system spanning two years. This included claims, incidents, workforce, maternity, staff survey and publicly available population data sources.</p> <p>The team applied automated machine learning methods to rapidly test multiple approaches. This testing:</p> <ul> <li>looked for correlations, or similarities, between the datasets and investigated effects associated with time lag</li> <li>used different modelling approaches including an XGBoost model and a Bayesian Hierarchical Model (BHM) to reach an end result that could both provide accurate predictions and actionable explanations</li> <li>identified the \u201cdecision tree\u201d model as the best performing and most cost-effective to train because of its ability to cope with varied data quality (data variables and missing values)</li> <li>included a focus on \u201cmodel explainability\u201d - the need to understand how the model is making predictions so that we can explain what drives negligence claims</li> </ul>","tags":["WORKFORCE","FINANCIAL","MACHINE LEARNING","CLASSIFICATION","MODELLING","STRUCTURED DATA","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/nhs-resolution/#constraints","title":"Constraints","text":"<p>There were a number of constraints that impacted the project including a lack of completeness and consistency across datasets. The data understandably showed a strong association between the number of claims and the size of the trust population, and it was necessary to eliminate the \u201csize effect\u201d so as not to hide other effects present in the data.</p> <p>It was also not possible to try to predict the rate of claims per specialty (for instance the rate of claims specific to maternity), only by trust, because the data was not complete or granular enough to do so.</p>","tags":["WORKFORCE","FINANCIAL","MACHINE LEARNING","CLASSIFICATION","MODELLING","STRUCTURED DATA","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/nhs-resolution/#data-security","title":"Data security","text":"<p>The data used for this testing process was pseudonymised by replacing any personal identifiable information with artificial data (pseudonyms). It was responsibly managed in accordance with General Data Protection Regulations (GDPR).</p> <p>The project made use of ACE\u2019s PodDev environment for the data. Using this bespoke environment meant that sensitive clinical data could be included securely and destroyed at the end of the project in a way that met the requirements of the NHS Resolution data controller.</p>","tags":["WORKFORCE","FINANCIAL","MACHINE LEARNING","CLASSIFICATION","MODELLING","STRUCTURED DATA","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/nhs-resolution/#impacts-and-outcomes","title":"Impacts and outcomes","text":"<p>The completion of the \u201cproof of concept\u201d feasibility study led to some significant successes. Although claims prediction was not perfected, the results significantly outperformed baseline models when predicting rates of claims, by trust and by month.</p> <p>Whilst useful for hospital trusts, predicting the rate of claims is not yet impactful in improving patient safety without further development of this project.</p> <p>The project has provided an informative view of the data landscape across the wider health service, and its relevance to understanding medical negligence claims.</p> <p>As a result of the work, it is possible to gain the following insights into what drives rates of claims, from wider health service data:</p> <ul> <li>The datasets for incidents, specialty, referral-to-treatment and hospital activities have the largest potential impact on the predicted rate of claims.</li> <li>The presence of specialities in a trust tends to have a correlation with the predicted rate of claims.</li> <li>The number of incidents, and specifically the percentage of incidents resulting in severe harm or death of the patient, also has a correlation with the predicted rate of claims.</li> <li>Longer waiting times also appear to correlate with the predicted rate of claims, although this varies with specialty.</li> </ul> <p>The results are an indication that prediction of claims is possible but that large volumes of quality data are required to take the challenge further.</p> <p>NHSX Skunkworks was a perfect way for us to kickstart our AI journey. We were able to work with experts who guided us through the process and help us every step of the way. We\u2019ve learned so much and are keen to take this forward for further investigation. \u2013 Niamh McKenna, Chief Information Officer, NHS Resolution This project highlighted not just that the data could broadly be used for better-than-baseline forecasts, but it also suggested a number of actionable ideas on how to improve data quality, something we really care about because without good data there cannot be good AI. \u2013 Giuseppe Sollazzo, Head of Skunkworks, NHS AI Lab, NHSX</p>","tags":["WORKFORCE","FINANCIAL","MACHINE LEARNING","CLASSIFICATION","MODELLING","STRUCTURED DATA","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/nhs-resolution/#what-next","title":"What next?","text":"<p>The resulting code base and methodology is a starting point for further machine learning exercises. The proof of concept has demonstrated how machine learning could increase NHS Resolution\u2019s understanding of medical negligence claims.</p> <p>In order to demonstrate a reduction in patient harm, more work will need to be done to select and engineer new features.</p> <p>Ideally, improving the consistency of the claim reporting methodology across trusts would significantly improve the predictive power of the negligence claims data.</p> <p>Future improvements to both the data and the model would provide a more accurate forecasting model and more insightful explanations.</p> Output Link Case Study NHSE Transformation Site","tags":["WORKFORCE","FINANCIAL","MACHINE LEARNING","CLASSIFICATION","MODELLING","STRUCTURED DATA","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/nhs-resolution/#_1","title":"Predicting negligence claims with NHS Resolution","text":"","tags":["WORKFORCE","FINANCIAL","MACHINE LEARNING","CLASSIFICATION","MODELLING","STRUCTURED DATA","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/nursing-placement-optimisation/","title":"Nursing Placement Schedule Optimisation Tool","text":"<p>This project is an example of the AI Skunkworks team offering capability resources to produce proof-of-concepts which could be applicable to the NHS at large. The project ran from January 2022 to May 2022.</p> <p>User Interface that is seen upon launching the tool: </p> <p>User Interface while the tool is running: </p> <p>User Interface after the tool has run: </p>","tags":["SECONDARY CARE","WORKFORCE","MACHINE LEARNING","SIMULATION","MODELLING","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/nursing-placement-optimisation/#results","title":"Results","text":"<p>A proof-of-concept demonstrator written in Python (machine learning model, user interface and backend)</p>","tags":["SECONDARY CARE","WORKFORCE","MACHINE LEARNING","SIMULATION","MODELLING","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/nursing-placement-optimisation/#case-study","title":"Case Study","text":"<p>This is a backup of the case study published here on the NHS England Transformation Directorate website.</p> <p>Students training to become nurses undertake placements at teaching hospitals as part of their studies. These placements endeavour to help students experience and learn practical skills they can apply in their roles once they are qualified. In general, these placement schedules are produced by hand and what may seem to be a relatively simple task, quickly becomes complex once you consider the different constraints involved.</p> <p>The challenge Can a tool or approach be developed which automatically generates student nurse placement schedules that adhere to the requirements and constraints of the different stakeholders, while additionally providing a more diverse range of placements for the students?</p>","tags":["SECONDARY CARE","WORKFORCE","MACHINE LEARNING","SIMULATION","MODELLING","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/nursing-placement-optimisation/#overview","title":"Overview","text":"<p>Any placement allocation process involves three stakeholders - the Trust who will be coordinating and hosting the students on placement, the university where the students undertake classroom study and where the final degree will be provided, and the students themselves.</p> <p>There is a range of contexts which require consideration. For example, the universities will have set dates when placements should take place, and these are usually particular for each university. Some universities will require students to visit specific types of wards over the course of their placements, while others may be more flexible.</p> <p>Similarly, the Trust has requirements that must be met. Each ward at a hospital has a maximum student hosting capacity, and this can vary depending on the student\u2019s current year of study. Wards must also keep to internal Education Audit Standards to ensure they can provide necessary support to students, and this must be checked regularly.</p> <p>When these requirements are combined with the fact that Trusts can have relationships with multiple universities, placing students at tens of wards at a time, it is clear such complex schedules could be time consuming for a placement coordinator to produce. It was calculated the generation of placements for a single year group of students from one university can take each Trust anywhere between five and ten hours. Given multiple universities and multiple intakes the time spent scheduling placements can easily stretch into hundreds of hours for each Trust, each year.</p> <p>Presently time limitations and the lack of quantification ability presented by the manual process mean it is not feasible for placements to cover a range of disciplines and specialities, yet this is something universities would like to support to enhance skills development of nurse placements.</p>","tags":["SECONDARY CARE","WORKFORCE","MACHINE LEARNING","SIMULATION","MODELLING","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/nursing-placement-optimisation/#what-we-did","title":"What we did","text":"<p>We worked with Imperial College Healthcare NHS Trust, in conjunction with North West London CCGs to undertake this project to develop an AI-driven solution to the problem of placing students. Imperial hosts students from seven universities, placing them across three hospitals, totalling more than 80 wards and placement settings. Imperial College Healthcare NHS Trust is one the largest acute trusts in the UK (according to the Kings Fund), so would provide substantial evidence as to whether this solution was viable. The data used was anonymised as this task could be undertaken without needing to provide any details on a personal level. Instead, information about the hospitals were taken, and randomly generated student profiles were created containing examples of the information the Trust would have about each student.</p> <p>The task posed here is one of optimisation, of which there are many different approaches. The method selected was a Genetic Algorithm, where the best version of something is found through a process which is like the evolutionary process seen in nature. The algorithm runs as follows:</p> <ul> <li> <p>Create a population of objects (in this case, it was a population of potential schedules for all students)</p> </li> <li> <p>Apply \u2018mutations\u2019 and produce \u2018offspring\u2019 from this population of objects</p> </li> <li>Mutations were produced by randomly changing the allocated ward for a random placement</li> <li> <p>Offspring were produced by combining schedules e.g. taking the front half of one schedule, and the back half of another schedule and sticking them together to produce a hybrid</p> </li> <li> <p>Put the \u2018mutated\u2019 and \u2018offspring\u2019 objects back into the population, and score the population</p> </li> <li> <p>The scoring part is key, as this is what dictates what a \u2018good\u2019 schedule looks like. This is where you define what absolutely cannot be in a schedule, and what you\u2019d like a good schedule to have</p> </li> <li> <p>Repeat the process hundreds of times until you have found a schedule which meets all your needs.</p> </li> </ul> <p></p> <p>A diagram showing how the genetic algorithm works.</p> <p>Genetic Algorithms will not produce a perfect, optimal solution as the way the problem space is explored is random. This means, while all your requirements will be met, the most varied selection of placements might not be found because it would take too long for the Genetic Algorithm to find that solution. However, a more varied selection of placements will be allocated than at the beginning, or if the process was carried out with the current, manual process.</p> <p>The chosen approach was to use such a Genetic Algorithm, run multiple times to produce multiple different schedule options, which are then presented back to the placement coordinator for them to cast their expert eye over and choose the best option. Alongside the schedules themselves, the scores which the Genetic Algorithm uses can be presented so that different schedules can be compared to each other. Further, this score can be broken down into components, so the exact strengths and weaknesses of each schedule can be seen, allowing the placement coordinator to evaluate which suits their Trust and its students.</p>","tags":["SECONDARY CARE","WORKFORCE","MACHINE LEARNING","SIMULATION","MODELLING","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/nursing-placement-optimisation/#deciding-how-to-score-schedules","title":"Deciding how to score schedules","text":"<p>The scoring of schedules is arguably the most important part of a Genetic Algorithm because it defines what a \u2018good\u2019 schedule and \u2018bad\u2019 schedule might look like. The scoring components can be divided into two categories: \u2018must-haves\u2019 and \u2018nice-to-haves\u2019.</p> <p>Must-have scoring components are things a schedule must adhere to or otherwise it could not be used. In the current process, the placement coordinator would ensure any schedule produced meets these criteria:</p> <ul> <li> <p>Each ward and placement setting does not have more students allocated than their capacity allows. This includes both overall and for each year group. Year groups have different capacities because each year requires a different level of support and management.</p> </li> <li> <p>Any ward a student is placed on is aligned to the student\u2019s COVID risk level. Some wards will have a higher risk of COVID exposure than others, and some students will be at a higher risk of COVID than others.</p> </li> <li> <p>Students must only be on one placement at a time. While this is an obvious statement to a human, this must be included in the scoring of the schedules, so the algorithm understands this is not acceptable.</p> </li> <li> <p>Students must have a specific ward or placement setting allocated for each placement. Similar to the previous point, this feels like an obvious criteria, but must be enforced to make sure the final schedule produced is useful.</p> </li> </ul> <p>Meanwhile, nice-to-have scoring components focus more on trying to ensure diversity of placements. These are the things the Genetic Algorithm tries to maximise (or increase as much as possible):</p> <ul> <li> <p>Number of unique wards. This helps to signal to the algorithm that, as much as possible, students should be placed at a ward where they have not been before.</p> </li> <li> <p>Number of unique specialities. To build upon unique wards, this aims to send students to gain experience in a speciality which they have not previously been placed in.</p> </li> <li> <p>Placement Capacity Utilisation percentage. This helps to address cases where wards which can host lots of students are chosen more commonly, because it is known they have lots of capacity. By including this component, the algorithm tries to ensure wards and placement settings are as full as possible across the board, spreading the placement load across the Trust.</p> </li> <li> <p>Focus on specific specialisms. The tool has four scoring components which promote inclusion of specialities - Medical, Surgical, Critical Care and Community wards. This helps to ensure placements include specific topics or types of wards. As this is not always necessary, these can be turned on and off, and are turned off by default in the open-source code.</p> </li> </ul>","tags":["SECONDARY CARE","WORKFORCE","MACHINE LEARNING","SIMULATION","MODELLING","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/nursing-placement-optimisation/#the-user-interface","title":"The user interface","text":"<p>A simple user interface was produced using Streamlit, an interactive interface for web browser-based applications, allowing the user to adjust elements of the placement optimisation tool, and to view progress as the tool produces schedules.</p> <p>Once all schedules are produced, a comparison table is both displayed and saved down, summarising the various scoring components and helping the user begin to understand which schedule of those produced might be the best.</p>","tags":["SECONDARY CARE","WORKFORCE","MACHINE LEARNING","SIMULATION","MODELLING","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/nursing-placement-optimisation/#automating-the-report","title":"Automating the report","text":"<p>The final benefit of the tool is that insight into the schedules can be easily obtained as the schedules are generated electronically, meaning they can be easily reformatted to provide a different view of the information. As an example, it is straightforward to change a schedule from a format where the placements on each ward are shown, to a format where the placements for each student are shown.</p> <p>This simplicity extends to being able to calculate hours on placement each week, which is currently a mandatory reporting ask for Trusts. We hear that a substantial amount of time is spent by placement coordinators manually analysing placement schedules to produce reporting around the hours of placement time the Trust has supported each week, from each university.</p> <p>As part of the tool, the final schedule produced is reported in several different ways:</p> <ul> <li>From the student\u2019s perspective, showing what placements locations they have been allocated for each week of the studies</li> <li>From the ward\u2019s perspective, showing what students they have on placement with them and when</li> <li>From a ward capacity utilisation perspective, showing the placement coordinator where there is spare capacity if a placement needs to be manually reallocated</li> <li>From a placement hours perspective, providing various summaries of hours across wards, university cohorts and both weekly and quarterly summaries for the mandatory reporting required of the Trust.</li> </ul> <p></p> <p>An example schedule produced using fake data.</p>","tags":["SECONDARY CARE","WORKFORCE","MACHINE LEARNING","SIMULATION","MODELLING","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/nursing-placement-optimisation/#outcomes-and-lessons-learned","title":"Outcomes and lessons learned","text":"<p>The resulting code, released as open source on our GitHub (available to anyone to re-use), enables users to:</p> <ul> <li> <p>Load information about wards, placement, and students into the tool</p> </li> <li> <p>Run the genetic algorithm to produce up to 10 schedules</p> </li> <li> <p>See score comparisons across produced schedules, allowing quantitative comparison</p> </li> <li> <p>Automate mandatory reporting</p> </li> <li> <p>Where relevant technical expertise is available, the scoring metrics could be extended and updated to reflect exactly what the user would be looking for.</p> </li> </ul> <p>The tool is estimated to save hundreds of hours constructing and analysing schedules for nursing students. This time can be spent much more effectively elsewhere, thereby freeing up placement coordinators to utilise their expertise across the Trust. Additionally, the tool produces improved schedules by consistently taking into account the wards and specialities that a student has already undertaken a placement within.</p> <p>It should be noted this tool aims to support placement coordinators, as the process of producing placement schedules isn\u2019t just fitting together all the pieces. Bespoke requests will come in from students, requiring tweaking of placement schedules to accommodate a wide range of circumstances. This tool aims to provide a high-quality baseline from which placement coordinators can construct schedules which meet every requirement, from all over the Trust.</p> <p>We can't fix the nursing shortage without training more nurses and for that, we need to have (the right) clinical placements available. This tool will not only help us to allocate placements more efficiently and effectively, but it will also free up valuable time for the practice learning facilitators to focus on teaching and professional development for students. Ultimately more students will be able to get the placements and training they need. \u2013 Hai Lin Leung, Programme Manager, North West London CCG</p> <p>Finally, a successful element of this project has been the knowledge sharing and mutual upskilling between the two parties. AI Lab Skunkworks helped Imperial College Healthcare Trust identify and implement a novel AI-led approach which provided a solution they may otherwise not have been able to explore. Meanwhile, Imperial College Healthcare Trust shared their expertise in how the placement allocation process works, particularly what does and does not work well. This was useful underlying information the AI Lab Skunkworks team hopes to share more broadly across the range of projects we work on.</p>","tags":["SECONDARY CARE","WORKFORCE","MACHINE LEARNING","SIMULATION","MODELLING","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/nursing-placement-optimisation/#whats-next","title":"What\u2019s next?","text":"<p>The NHS AI Lab Skunkworks team has released the code from the project on GitHub to demonstrate how the tool might work, using generated fake data.</p> <p>A pilot is planned to evaluate how well the tool works in practice, working with Imperial College Healthcare Trust during a future student intake. Following this pilot, and an evaluation exercise, improvements will be identified and implemented in a subsequent phase of work.</p>","tags":["SECONDARY CARE","WORKFORCE","MACHINE LEARNING","SIMULATION","MODELLING","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/nursing-placement-optimisation/#who-was-involved","title":"Who was involved?","text":"<p>This project was a collaboration between the NHS AI Lab Skunkworks, within the Transformation Directorate at NHS England, and Imperial College Healthcare NHS Trust.</p> <p>NHS AI Lab Skunkworks is a team of data scientists, engineers and project leaders who support the health and social care community to rapidly progress ideas from the conceptual stage to a proof of concept.</p> Output Link Open Source Code &amp; Documentation Github Case Study Case Study","tags":["SECONDARY CARE","WORKFORCE","MACHINE LEARNING","SIMULATION","MODELLING","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/nursing-placement-optimisation/#_1","title":"Nursing Placement Schedule Optimisation Tool","text":"","tags":["SECONDARY CARE","WORKFORCE","MACHINE LEARNING","SIMULATION","MODELLING","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/open-safely/","title":"Working with a Trusted Research Environment - the NHS @Home programme","text":"<p>OpenSAFELY gives trusted researchers restricted levels of access to the server to run analysis on real data and obtain aggregate results, without having sight of the patient level data. Aggregate results are checked to ensure there are no disclosure risks before being released from the server. This highly secure way of working enables researchers to have access to large and sensitive datasets in a safe manner.</p> <p>This project had two main aims:</p> <ul> <li> <p>To understand the reach and coverage of the NHS @Home programme during the pandemic. Specifically looking at: blood pressure monitoring, pulse oximetry and proactive care interventions.</p> </li> <li> <p>To understand how to approach an analysis project using OpenSAFELY, including the amount of time and resource required, and whether this platform would be useful for future analyses.</p> </li> </ul>","tags":["PRIMARY CARE","DATA VALIDATION","STRUCTURED DATA","SQL","COMPLETE"]},{"location":"our_work/open-safely/#case-study","title":"Case Study","text":"<p>Trusted research environments (TREs) take the form of a secure data environment that allows analysts and researchers to undertake in-depth analysis on rich, joined-up datasets without them seeing any identifiable information. Data is held within a secure server and does not leave that server. Following a recommendation from the Goldacre review TREs form a key part of the Data saves lives: reshaping health and social care with data policy paper.</p> <p>The challenge Using TREs for analytical projects is still a new concept to many analysts. The NHS AI Lab Skunkworks team and NHS England\u2019s Health Inequalities and Evaluation Analytics Team embarked on a partnership project to gain experience of using a TRE to answer key and important questions for a defined project. They used a TRE called OpenSAFELY, an open-source software platform for analysis of electronic health care records data for COVID-19 related research.</p> <p>OpenSAFELY gives trusted researchers restricted levels of access to the server to run analysis on real data and obtain aggregate results, without having sight of the patient level data. Aggregate results are checked to ensure there are no disclosure risks before being released from the server. This highly secure way of working enables researchers to have access to large and sensitive datasets in a safe manner.</p> <p>This case study outlines the team\u2019s experiences of working through the OpenSAFELY platform to understand the impact of the NHS @Home programme.</p>","tags":["PRIMARY CARE","DATA VALIDATION","STRUCTURED DATA","SQL","COMPLETE"]},{"location":"our_work/open-safely/#the-nhs-home-programme","title":"The NHS @Home programme","text":"<p>During the COVID-19 pandemic there was a necessity to safely deliver high quality care into patients\u2019 homes where appropriate. As part of its response to the pandemic, NHS England brought forward initiatives to help patients better self-manage their health and care at home under the NHS @Home programme. NHS @Home is a nationally-led programme of work providing better connected, more personalised care in people\u2019s homes, including care homes.</p> <p>Now, two years on from the start of the pandemic, the NHS @Home team wanted to understand the reach and coverage of their initiatives to inform future planning. In particular, the team wanted to understand the reach of the following initiatives:</p> <ul> <li>Blood pressure monitoring</li> <li>Covid Oximetry</li> <li>Proactive care</li> </ul> <p>Obtaining this information can be difficult, because access to patient level data within primary care electronic health records is controlled by strict information governance rules due to the importance of patient privacy. This limits the scope of analysis which can be carried out on primary care data. The two main software systems used in primary care in England are operated by the companies TPP and EMIS.</p> <p>Therefore, to answer the study questions, the NHS AI Lab Skunkworks Team and NHS England\u2019s Health Inequalities and Evaluation Analytics Team embarked on a partnership project exploring the Uptake of NHS @Home interventions during the COVID-19 pandemic, using the OpenSAFELY platform. OpenSAFELY is a TRE which provides access to patient records in the TPP system, and for certain studies also provides access to patient records in the EMIS system.</p>","tags":["PRIMARY CARE","DATA VALIDATION","STRUCTURED DATA","SQL","COMPLETE"]},{"location":"our_work/open-safely/#project-aims","title":"Project aims","text":"<p>This project had two main aims:</p> <ul> <li> <p>To understand the reach and coverage of the NHS @Home programme during the pandemic. Specifically looking at: blood pressure monitoring, pulse oximetry and proactive care interventions.</p> </li> <li> <p>To understand how to approach an analysis project using OpenSAFELY, including the amount of time and resource required, and whether this platform would be useful for future analyses.</p> </li> </ul> <p>Part of the initial project aims was to look at whether coding of uptake of the interventions of interest varied across TPP and EMIS. Unfortunately, OpenSAFELY were unable to provide access to the EMIS system at the time, so the scope was narrowed to TPP only, which still represents a significant dataset of approximately 24 million patient records.</p> <p>This case study will focus on the second bullet point, the experience of using OpenSAFELY, as the code for the analysis will be published separately and there is ongoing review and further development of the analysis.</p>","tags":["PRIMARY CARE","DATA VALIDATION","STRUCTURED DATA","SQL","COMPLETE"]},{"location":"our_work/open-safely/#what-we-did","title":"What we did","text":"<p>There are several stages to completing an OpenSAFELY project. These include:</p> <ul> <li>Setting up the project</li> <li>The co-piloting period and project development</li> <li>Project Wrap up</li> </ul>","tags":["PRIMARY CARE","DATA VALIDATION","STRUCTURED DATA","SQL","COMPLETE"]},{"location":"our_work/open-safely/#setting-up-the-project","title":"Setting up the project","text":"<p>The initial step was to complete the study access request and receive approval for the project through OpenSAFELY. This included:</p> <ul> <li>Naming the study sponsor, study lead and team who would need to access the server and work on the analysis</li> <li>Stating the purpose and scope of the study and the desired outputs</li> <li>Stating which datasets need to be accessed\u00a0\u00a0</li> <li>Providing additional details, including the relevant experience of the researchers and whether the study is research, a service evaluation, or an audit</li> </ul> <p>Once this was granted, the next step for everyone working on the analysis was to successfully apply to become an accredited researcher, recognised by the Office of National Statistics, and to start setting up the technical requirements for OpenSAFELY in parallel.</p> <p>The application process to be an accredited researcher will be detailed by OpenSAFELY at the start of the project, however an outline of the process can be found here. Having filled in the application form, it is necessary to attend a virtual training session, which walks through best practices for data publication and data privacy. These sessions must be booked in advance and, due to availability, the whole accreditation process could take a few weeks to complete. Hence, it is worth filling out the application form and booking the training as soon as possible before the start of the co-pilot period.</p> <p>For the technical setup, there are two main options for setting up the OpenSAFELY environment:\u00a0\u00a0</p> <ul> <li>The first option is to use an online development environment. The online environment recommended by OpenSAFELY is Gitpod, which provides 50 free hours of monthly usage and negates the need for any software other than that required for accessing the TPP server. The Gitpod environment is easy to set up and integrates well with GitHub.</li> <li>If higher levels of monthly usage are envisaged, which may particularly be the case during the more time intensive co-piloting period, it may be preferable to have the required software downloaded locally. The required software is:</li> <li>Git</li> <li>Docker</li> <li>Coding language and IDE (Integrated development environment, the software which provides the environment for programming - for this project python and Visual Studio code were used)</li> </ul> <p>For this project the decision was taken to download the required software locally. This software is not part of the standard set up for NHS England laptops, and higher levels of permissions (e.g. a developer account) are required to install it. Obtaining the necessary permissions takes time, so to allow swifter progress the project was instead undertaken on cloud-based virtual machines with a data science set up provided by the NHS Transformation Directorate Analytics Unit. This provided a fast, effective and secure method for working on the code locally.</p> <p>Code for all OpenSAFELY projects (regardless of the setup) is stored on GitHub, a website designed for storing and collaborating on code. Instructions for setting up a GitHub account and preparing the necessary computing environment for completion of the project can be found in the OpenSAFELY Getting Started Guide.</p> <p>Another key component to the technical set up (regardless of whether Gitpod or local set up is used) is installing the software required to access the TPP server. This is needed to view the results when the analysis code has been run on real data. This is done by connecting to the TPP VPN. Unfortunately, due to the specific settings for the VPN, it was not possible to connect to the VPN via a cloud-based virtual machine. Connecting to the VPN on NHS England laptops was also not possible at the time. To negate this problem, the OpenSAFELY co-pilots accessed the results and completed all the necessary checks prior to releasing them from the TPP server.</p>","tags":["PRIMARY CARE","DATA VALIDATION","STRUCTURED DATA","SQL","COMPLETE"]},{"location":"our_work/open-safely/#the-co-piloting-period-and-project-development-period","title":"The co-piloting period and project development period","text":"<p>For the first four weeks of each project OpenSAFELY operate a \u2018co-piloting\u2019 period, where an OpenSAFELY \u2018co-pilot\u2019 provides an enhanced level of support, with regular meetings, to help researchers to understand and implement OpenSAFELY ways of working. The same co-pilot also continues to provide support beyond the co-piloting period. This was found to be a hugely beneficial system, however ongoing technical difficulties slowed progress during the co-piloting period, as obtaining the correct set-up for completing the project, and installing the necessary software, proved challenging.</p> <p>The project team also met for regular review meetings with a small group of collaborators and stakeholders in NHS England in an agile way of working. This group included members with relevant expertise, including analysts, subject experts and data scientists, who inputted knowledge and experience as well as providing steers for each stage of the analysis and giving feedback on work already undertaken.</p> <p>In addition to gaining an understanding of the NHS @home interventions and working with OpenSAFELY, the collaborative nature of the project also provided benefits for team members involved, in learning from each other\u2019s skills and areas of expertise. Senior team members assisted the development of junior team members\u2019 skills, in particular the development of:</p> <ul> <li> <p>Best practice for coding such as the benefits of dynamic coding, use of functions to prevent duplication of code and reduce code maintenance burden and commenting the code.</p> </li> <li> <p>GitHub workflows including the benefits of raising small pull requests in making code reviews simpler and importance of maintaining good repo structure.</p> </li> </ul> <p>A key part of the process is writing the code for extracting the relevant study cohort and variables for analysis. This code is known as the study definition. In addition to allowing time for resolving technology and set up issues, it is also important to be aware that creating the code, particularly for the study definition, is an iterative process. It is beneficial to clearly define the study population and the variables of interest at the start of the project with the co-pilot, as it may take several iterations to correctly code the method for extracting the study cohort. Care should also be taken to ensure that key demographics align with NHS standards and that definitions within the study definition are applied correctly.\u00a0\u00a0</p> <p>Examples of issues which require careful consideration and clarification include: the correct manner for extracting demographic variables, the appropriate inclusion criteria (i.e. what data needed to be present for a patient to be included in the study), issues relating to redaction of small numbers and rounding principles. Redaction and rounding should be applied within the code. Co-pilots may be able to assist with functions which can apply redacting and rounding automatically, but careful consideration must be given to exactly what redaction and rounding is required in each scenario.\u00a0\u00a0</p> <p>Many variables are extracted based on a codelist (a collection of clinical codes that classifies patients as having certain conditions or demographic properties \u2013 code systems include SNOMED and CTV3). OpenSAFELY provide a web-based tool, OpenCodelists, for creating and managing codelists. The website can be searched to see if an appropriate codelist already exists or used to create a new one. The project team are responsible for use of codelists and it is important to ensure that codelists used are appropriate for the purpose. Clinical or other expert input may be required to ensure this.\u00a0\u00a0</p> <p>Currently, for studies looking at multiple time periods, separate cohorts are extracted for each of the relevant time periods (e.g. one week or one month). For this project separate cohorts were extracted for each week over the time period from April 2019 to June 2022. This can result in the project taking significant time to run in the server. One way to reduce this workload is to put any variables which do not change significantly over time into a \u2018static\u2019 study definition and run this cohort extraction only once. OpenSAFELY provide a function to then join the weekly or monthly and static datasets.</p> <p>Whilst working with data at arm\u2019s length has significant benefits for security and information governance, it does bring additional challenges. OpenSAFELY provide the means to create fake data, which can be used to test the code prior to running on real data, however the return expectations for the fake data (what it is expected to look like) must be defined by the researcher and this can lead to a mismatch in the format of the fake data compared to the real data. Consequently, even if the project runs locally, upon running in the server there may be problems. Co-pilots and OpenSAFELY tech support are very willing to help, and can be contacted quickly via OpenSAFELY\u2019s dedicated Slack channel\u00a0\u00a0</p> <p>(Slack is a messaging app for business, which can also be accessed via a web browser), however trouble-shooting such issues can still be difficult.</p> <p>Depending on the size of the dataset being analysed, there is also a risk of exceeding the memory limit within the server. To mitigate this, OpenSAFELY provide suggestions of ways to improve performance and minimise memory usage, such as considering the datatype for each column of a dataframe.</p> <p>It is beneficial for the co-pilot to check the study definition prior to running on the server. Time should also be allowed for running on the server (due to potential for technical difficulties) and obtaining release of results (which requires a full review by OpenSAFELY to ensure there are no disclosure issues).</p>","tags":["PRIMARY CARE","DATA VALIDATION","STRUCTURED DATA","SQL","COMPLETE"]},{"location":"our_work/open-safely/#project-wrap-up","title":"Project wrap-up","text":"<p>Once all the required results have been obtained, the next steps are to analyse the results and share them internally for further validation and analysis. In line with OpenSAFELY\u2019s transparency principles, the GitHub repository containing the project code will be made public.</p>","tags":["PRIMARY CARE","DATA VALIDATION","STRUCTURED DATA","SQL","COMPLETE"]},{"location":"our_work/open-safely/#lessons-learned","title":"Lessons learned","text":"<ul> <li> <p>Be prepared: it\u2019s helpful to have the software in place and to map out study aims and requirements prior to beginning the co-piloting period. Safe researcher accreditation should be obtained as early as possible as access to the server is dependent on completion of this step.</p> </li> <li> <p>Use of co-piloting period: OpenSAFELY recommend 50-60% of the researchers\u2019 working time be spent on the project during the co-piloting period. This ensures maximum benefit is obtained from the enhanced level of co-pilot support. Co-pilots can help identify appropriate codelists, ensure correct understanding of OpenSAFELY functions and assist with issues such as redacting and rounding scripts.</p> </li> <li> <p>Technical skills: use of the OpenSAFELY module requires a fair amount of technical skill, including writing code, use of software and GitHub and understanding of Git ways of working. It is beneficial to have at least one member of the team who already has some level of technical skill in these areas, and if not then some basic training before the start of the project may be required.</p> </li> <li> <p>Time investment: undertaking projects with OpenSAFELY is time intensive due to the time needed to learn about OpenSAFELY ways of working, this should be factored into the planning for a project.</p> </li> <li> <p>Limitations of the dataset: clinical coding of healthcare interventions is not always consistent, which may limit the usefulness of the data obtained.</p> </li> <li> <p>Importance of the study definition: correct extraction of cohort and variables within the study definition requires careful planning. Thorough checking of codelists and study definition code, along with good communication and liaison with co-pilot, can help with this.</p> </li> <li> <p>Benefits of collaboration: collaborating across teams is beneficial for exchanging knowledge and expertise and ensuring the project team has a good skills mix.\u00a0\u00a0</p> </li> <li> <p>Slack channel: Making good use of the Slack channel can help to progress the project as OpenSAFELY support, including tech support, are very willing to help and generally respond quickly.</p> </li> <li> <p>Server time: consider ways to improve performance and reduce memory usage. Allow time for server downtime and keep in mind dates and times for planned maintenance (speak to co-pilot or check on Slack).</p> </li> <li> <p>Checking codelists and definitions: Dedicate time to ensure codelists meet the requirements of the project and definitions of variables within the study definition are correct. Ensure key demographics align with NHS standards.</p> </li> <li> <p>Good coding practice: good coding practice, such as using functions to prevent duplication of code, is important to reduce the likelihood of errors and reduce code maintenance burden, make code reviews easier and improve readability of the code.</p> </li> </ul>","tags":["PRIMARY CARE","DATA VALIDATION","STRUCTURED DATA","SQL","COMPLETE"]},{"location":"our_work/open-safely/#summary","title":"Summary","text":"<p>The level of access to a vast amount of primary care data makes this platform worth using for future studies. However, the significant time investment required and the need for a certain level of technical skill within the research team should be factored into the decision to undertake a project through OpenSAFELY. Good planning and preparation, such as having the correct software in place prior to starting the study, are essential to ensure smooth running of the project. The co-piloting scheme is extremely helpful, and it is advisable to make best use of the initial co-piloting period to become familiar with OpenSAFELY ways of working.</p> Output Link Case Study TBD","tags":["PRIMARY CARE","DATA VALIDATION","STRUCTURED DATA","SQL","COMPLETE"]},{"location":"our_work/open-safely/#_1","title":"Working with a Trusted Research Environment - the NHS @Home programme","text":"","tags":["PRIMARY CARE","DATA VALIDATION","STRUCTURED DATA","SQL","COMPLETE"]},{"location":"our_work/p11_synpathdiabetes/","title":"Applying our SynPath Simulator to a Diabetes Pathway","text":"Figure 1: Table of learning algorithms considered for the simulation intelligence layer  <p>Using the SynPath framework we created a diabetes simulation for 800 patients.  These patients could interact within a fictional local area with hospitals providing outpatient and inpatient services, GP practices and community healthcare services.</p>","tags":["DISEASES","SIMULATION","GENERATION","TIME SERIES","SYNTHETIC DATA"]},{"location":"our_work/p11_synpathdiabetes/#results","title":"Results","text":"<p>The project showed how to develop a set of environments, interactions and patients from academic literature, policy, and clinical resources. The model currently runs a simulation that prints outputs of patient records into the console.</p> <p>Future collaboration around validation and how to apply learning algorithms are being pursued.</p> Output Link Open Source Code &amp; Documentation Github Case Study Awaiting Sign-Off Technical report Here","tags":["DISEASES","SIMULATION","GENERATION","TIME SERIES","SYNTHETIC DATA"]},{"location":"our_work/p11_synpathdiabetes/#_1","title":"Applying our SynPath Simulator to a Diabetes Pathway","text":"","tags":["DISEASES","SIMULATION","GENERATION","TIME SERIES","SYNTHETIC DATA"]},{"location":"our_work/p12_synthvae/","title":"Investigating Differential Privacy in a Variational AutoEncoder for Synthetic Data Generation","text":"<p> Figure 1: Schematic Representation of a Variational Autoencoder</p> <p>This project investigates the potential suitability of Variational Autoencoders (VAEs) as a synthetic data generation tool in the context of the NHS. To effectively address this direction, this work focussed on four key aspects: quality, privacy, ease of use, and interpretability.</p> <p>We evaluate the performance of the VAE approach alongside five alternative methods available in July/August 2021, namely Gaussian Copula, CTGAN, CopulaGAN, SDV\u2019s TVAE and Independent (a model which assumes independence across variables).  Evaluating this set of models provides context to the performance of the VAE with respect to both basic (e.g. Independent) and complex (e.g. CTGAN) approaches.</p> <p>We then tested how the metrics and visualisations changed when differential privacy was incorporated into the variational autoencoder as a function of differential levels of privacy (increased privacy budget).</p>","tags":["SIMULATION","GENERATION","TIME SERIES","SYNTHETIC DATA","PYTHON"]},{"location":"our_work/p12_synthvae/#results","title":"Results","text":"<p>We found that a variational autoencoder could indeed generate medium to high fidelity synthetic data for a single tabular table with numerical and categorical gaussian variables.</p> <p>As the privacy budget increases, we see the quality decrease as expected.  However, the level of privacy increase associated with increasing the privacy budget appears not to have a direct correlation.  This warrants further work as this might be down to implementation, the metrics being used for evaluation or may point to a feature of the VAE not incorporating the differential privacy correctly.</p> Output Link Open Source Code &amp; Documentation Github Case Study Awaiting Sign-Off Technical report Here","tags":["SIMULATION","GENERATION","TIME SERIES","SYNTHETIC DATA","PYTHON"]},{"location":"our_work/p12_synthvae/#_1","title":"Investigating Differential Privacy in a Variational AutoEncoder for Synthetic Data Generation","text":"","tags":["SIMULATION","GENERATION","TIME SERIES","SYNTHETIC DATA","PYTHON"]},{"location":"our_work/p14_mcr/","title":"Using Model Class Reliance to Understand the Impact of Commerical Data on Predictions","text":"<p>The primary aim of the project was to apply the novel variable importance technique, model class reliance, to machine learning models which could predict registered respiratory deaths in the UK. The objective was to assess the value of commercial health data in healthcare predictions compared to other available datasets.</p>","tags":["PRESCRIBING","DISEASES","MACHINE LEARNING","CLASSIFICATION","MODELLING","STRUCTURED DATA","COMPLETE","EXPERIMENTAL","PYTHON"]},{"location":"our_work/p14_mcr/#results","title":"Results","text":"<p>In order to apply MCR, a set of optimal models had to be created which can successfully make the required predictions. The project managed to achieve this outcome with the machine learning model PADRUS (Predicting the amount of deaths by respiratory disease using sales). PADRUS is a random forest regressor which makes accurate weekly predictions of respiratory deaths in 314 local authorities across England 17 days in advance. The models\u2019 features are created from the following dataset types:</p> <ul> <li>week number,</li> <li>commercial sales,</li> <li>weather,</li> <li>indices of multiple deprivation,</li> <li>age and population,</li> <li>demographics,</li> <li>housing, and</li> <li>land use.</li> </ul> <p>MCR was applied to PADRUS showing the highest and lowest impact variables had on predictions across all instances of the model. Grouped MCR was also employed in order for variables to be evaluated in concert as a collection of features created from a dataset type.</p> <p>The MCR results implied model instances of PADRUS were using variables in different ways to achieve the same predictive results, and suggested where variables could be interchangeable or critical to predictions.</p> <p>The addition of commercial data show a significant increase in predictive power.  Further results are closed whilst a publication is being reviewed.</p> Output Link Open Source Code &amp; Documentation Github Case Study Awaiting Sign-Off Technical report Here","tags":["PRESCRIBING","DISEASES","MACHINE LEARNING","CLASSIFICATION","MODELLING","STRUCTURED DATA","COMPLETE","EXPERIMENTAL","PYTHON"]},{"location":"our_work/p14_mcr/#_1","title":"Using Model Class Reliance to Understand the Impact of Commerical Data on Predictions","text":"","tags":["PRESCRIBING","DISEASES","MACHINE LEARNING","CLASSIFICATION","MODELLING","STRUCTURED DATA","COMPLETE","EXPERIMENTAL","PYTHON"]},{"location":"our_work/p21_synthvae/","title":"Developing our SynthVAE code","text":"<p> Figure 1: Figure showing the DAG representing the causal relationships between a selection of variables and job opportunity. Not made to truly reflect the causal relationships, purely for example purposes. Created using dagitty.</p> <p>Continuation of the previous development of our variational autoencoder (VAE) to correct for an error discovered since the last project finished.  This error appears when trying to generate data for continuous variables which follow non-Gaussian distributions.  Previously, standard scaling had been used to normalise these variables which was causing the non-gaussian variables to be synthesised poorly.  This was replaced with a Gaussian mixture model from the RDT python library to scale and transform these variables into ones with a Gaussian distribution.</p> <p>The second phase of this worked focussed on understanding the different ways of measuring and implementing fairness within the synthetic data.</p>","tags":["SIMULATION","MODELLING","TIME SERIES","SYNTHETIC DATA","PYTHON","COMPLETE"]},{"location":"our_work/p21_synthvae/#results","title":"Results","text":"<p>The gaussian mixture model was able to cope with non-gaussian variables thus extending the range of datasets which we can generate from greatly.  Additional hyper-parameter tuning and general coding improvements have increased the reusability and performance of the code.</p> <p>Regarding fairness, there are many metrics to choose from and to make the situation more complex, not all metrics are compatible with one another, i.e. you might be able to satisfy an equal odds metric for a group but not an equal prediction for the same group. This means that the level of fairness required is project specific and has to be re-evaluated depending on the research needs.</p> <p>Further work will explore the adaption of directed acyclic graphs to control for fairness and the impact this has on quality and privacy.</p> Output Link Open Source Code &amp; Documentation Github Case Study Awaiting Sign-Off Technical report Here","tags":["SIMULATION","MODELLING","TIME SERIES","SYNTHETIC DATA","PYTHON","COMPLETE"]},{"location":"our_work/p21_synthvae/#_1","title":"Developing our SynthVAE code","text":"","tags":["SIMULATION","MODELLING","TIME SERIES","SYNTHETIC DATA","PYTHON","COMPLETE"]},{"location":"our_work/p22_txtrayalign/","title":"Descriptive text from X-Ray Images (TxtRayAlign)","text":"<p> Figure 1: A contrastive retrieval mechanism.   A query image is encoded and compared with the embeddings of a corpus of reference reports.  The report with the greatest cosine similarity in the shared embedding space is returned as the output. </p> <p>TxtRayAlign exploits contrastive training to learn similarities between text and images, allowing a retrieval-based mechanism to find reports that are \u201csimilar\u201d to an image.</p>","tags":["SECONDARY CARE","NATURAL LANGUAGE PROCESSING","UNSTRUCTURED DATA","MULTI MODAL","VISUAL DATA","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/p22_txtrayalign/#results","title":"Results","text":"<p>We observe that even the best performing model (ResNet50-DeCLUTR) only retrieves anything of relevance for 62% of queries. The retrieved sentences tend to contain findings that are not relevant for the query, as indicated by the relatively poor precision. Further, the query image contains findings that are only poorly covered by the retrieved sentences, as indicated by the low recall.</p> <p>The results of our investigation indicate that this approach can help generate reasonably grammatical and clinically meaningful sentences, yet falls short in achieving this with sufficient accuracy. While improvements to the model could be made, our findings are corroborated by others in literature. Besides improving performance, future work could develop other applications of TxtRayAlign for other downstream tasks, such as image-to-image or text-to-image retrieval.</p> Output Link Open Source Code &amp; Documentation Github Case Study Awaiting Sign-Off Technical report Here","tags":["SECONDARY CARE","NATURAL LANGUAGE PROCESSING","UNSTRUCTURED DATA","MULTI MODAL","VISUAL DATA","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/p22_txtrayalign/#_1","title":"Descriptive text from X-Ray Images (TxtRayAlign)","text":"","tags":["SECONDARY CARE","NATURAL LANGUAGE PROCESSING","UNSTRUCTURED DATA","MULTI MODAL","VISUAL DATA","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/p23_stm/","title":"Text Analysis using Structural Topic Modelling","text":"<p> Figure 1: Example Screenshot from STM insights</p> <p>The development of an R code for investigating the topics found in free text survey data using a technique that monitors both the content of the responses but also the metadata (e.g. when the response was made, which organisation the response relates to) in order to support the construction of these topics.</p>","tags":["WORKFORCE","NATURAL LANGUAGE PROCESSING","MODELLING","UNSTRUCTURED DATA","TEXT DATA","COMPLETE","DEPLOYED"]},{"location":"our_work/p23_stm/#results","title":"Results","text":"<p>The code base has been developed as an open reusable code and being used internally for topic modelling of survey responses.</p> Output Link Open Source Code &amp; Documentation Github Case Study Awaiting Sign-Off Technical report Here","tags":["WORKFORCE","NATURAL LANGUAGE PROCESSING","MODELLING","UNSTRUCTURED DATA","TEXT DATA","COMPLETE","DEPLOYED"]},{"location":"our_work/p23_stm/#_1","title":"Text Analysis using Structural Topic Modelling","text":"","tags":["WORKFORCE","NATURAL LANGUAGE PROCESSING","MODELLING","UNSTRUCTURED DATA","TEXT DATA","COMPLETE","DEPLOYED"]},{"location":"our_work/p24_lime/","title":"Investigating Superpixels in LIME for Explaining Predictions of Facial Images","text":"<p> Figure 1: An illustration of LIME workflow</p> <p>A work experience project investigating the application of a Local Interpretable Model-agnostic Explanations (LIME) technique to an image classification task around identifying Rosacea.  A binary classification model was trained on the normal and Rosacea faces to generate the LIME explanation for Rosacea faces. Secondly, the fine-tuned model was integrated into the LIME pipeline to generate explanations based on the crucial features on which predictions were made in the classification model. Hence the experimentations helped in understanding the features the classification model took.</p>","tags":["DISEASES","CLASSIFICATION","EXPLAINABILITY","UNSTRUCTURED DATA","VISUAL DATA","COMPLETE","EXPERIMENTAL","PYTHON"]},{"location":"our_work/p24_lime/#results","title":"Results","text":"<p>Image pre-processing (through contrast enhancement) improved the LIME by increasing the number of superpixels for the given input image so that some broad features such as discolouration and rashes were identified.   However, the coarseness of the superpixels generated through the pipeline was not sufficient to pick up the  features which would discern between different types of Rosacea.  It may require domain/ imaging modality-specific pre-processing tasks to enhance the quality of explanation by improving the distinctiveness of the features that may help pick the right number of superpixels.</p> Output Link Open Source Code &amp; Documentation Github Case Study n/a Technical report Here","tags":["DISEASES","CLASSIFICATION","EXPLAINABILITY","UNSTRUCTURED DATA","VISUAL DATA","COMPLETE","EXPERIMENTAL","PYTHON"]},{"location":"our_work/p24_lime/#_1","title":"Investigating Superpixels in LIME for Explaining Predictions of Facial Images","text":"","tags":["DISEASES","CLASSIFICATION","EXPLAINABILITY","UNSTRUCTURED DATA","VISUAL DATA","COMPLETE","EXPERIMENTAL","PYTHON"]},{"location":"our_work/p31_txtrayalign2/","title":"Adding a Clinical Focus to Evaluating Multi-Modal Data Representations","text":"<p> Figure 1: Proposed clinical workflow applications of ML to radiology - using the CXR workflow as an example.  [CDSS = clinical decision support system, CXR = Chest x-ray, EHR = Electronic health record, PACS = Picture archiving and communication system] </p> <p>The use of Natural Language Generation (NLG) for the auto generation of radiology reports has the potential to provide multiple radiology workflow applications. Free text reports pose a challenging task from which to compare NLG outputs due to the \"ambiguity, syntax, synonymy, medical abbreviations\", use of negation, reference to \"out of reach\" information, linking of associated findings, and overall individual variation in reporting style seen between different radiologists</p> <p>An series of evaluation techniques were tested combining components from machine translation metrics (Bleu, Rouge, Meteor), Clinical metrics (CheXpert, Mirqi), and a suggested Clinical Scoring System.</p>","tags":["SECONDARY CARE","NATURAL LANGUAGE PROCESSING","UNSTRUCTURED DATA","MULTI MODAL","VISUAL DATA","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/p31_txtrayalign2/#results","title":"Results","text":"<p>Combinations of evaluation metrics were tested against three experiments which were trained on single sentence descriptions, multiple sentence descriptions and including the \"impression and findings\" section of the reports alongside the multiple sentence descriptions.</p> <p>The metrics used to evaluate the performance of models for clinical tasks require further refinement to ensure clinical accuracy is captured. The effect on model performance from adapting model training as well as performance on external dataset was also conducted.</p> <p>Three potential uses for NLG models in the clinical radiological workflow highlighted in this work include;</p> <ul> <li>use as a safety-net for radiologists to auto-fill positive findings if not included in the report by the radiologist,</li> <li>provide preliminary reports for acute CXRs to support junior doctors interpreting scans on the wards in the first instance whilst awaiting the radiologists report communicating critical findings,</li> <li>automate follow up oncology scans, e.g. CT, reporting to provide a faster indication if a malignancy has progressed / quantifying response to therapy.</li> </ul> Output Link Open Source Code &amp; Documentation Github Case Study Awaiting Sign-Off Technical report Here","tags":["SECONDARY CARE","NATURAL LANGUAGE PROCESSING","UNSTRUCTURED DATA","MULTI MODAL","VISUAL DATA","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/p31_txtrayalign2/#_1","title":"Adding a Clinical Focus to Evaluating Multi-Modal Data Representations","text":"","tags":["SECONDARY CARE","NATURAL LANGUAGE PROCESSING","UNSTRUCTURED DATA","MULTI MODAL","VISUAL DATA","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/p32_phmdiabetes/","title":"Inequalities in Diabetes from PHM Data","text":"<p> Figure 1: Workflow in ESNEFT tools to process the data into Lower Super Output Area granularity for mapping and analysis </p> <p>A collaborations with East Suffolk and North Essex foundation trust (ESNEFT) to apply a suite of data science techniques to a large population health data including both primary and secondary care data.  The aim of the project was to identify inequalities in diabetes care whilst making reusable code bases which can now be applied for different conditions and in different organisations.</p> <p>A small collection of code bases have been created to support the analysis of inequalities in diabetes services for a single locality based on linked population health data.</p>","tags":["POPULATION HEALTH","DISEASES","MACHINE LEARNING","ETHICS","STRUCTURED DATA","PYTHON","COMPLETE"]},{"location":"our_work/p32_phmdiabetes/#results","title":"Results","text":"<p>The project was able to both deliver new insights around drivers for inequalities as well as reproducible analytical pipelines in the code-bases.  This means the code and approach can be reused for other disease or could be adapted for different localities.</p> Output Link Open Source Code &amp; Documentation DNA Risk Predict &amp; Diabetes Inequalities &amp; Morbidity Network Case Study Awaiting Sign-Off Technical report Here Project Slides Here","tags":["POPULATION HEALTH","DISEASES","MACHINE LEARNING","ETHICS","STRUCTURED DATA","PYTHON","COMPLETE"]},{"location":"our_work/p32_phmdiabetes/#_1","title":"Inequalities in Diabetes from PHM Data","text":"","tags":["POPULATION HEALTH","DISEASES","MACHINE LEARNING","ETHICS","STRUCTURED DATA","PYTHON","COMPLETE"]},{"location":"our_work/p33_patientsafetylms/","title":"Investigating Applying and Evaluating a Language Model to Patient Safety Data","text":"<p>In collaboration with the NHS England patient safety data team, we present an exploration of a selection of different language model pretraining and finetuning objectives with patient safety incident reports as the domain of interest, followed by a discussion of a number of methods for probing and evaluating these new models, and their respective embedding spaces.</p>","tags":["NATURAL LANGUAGE PROCESSING","LLM","RESEARCH","UNSTRUCTURED DATA","TEXT DATA","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/p33_patientsafetylms/#results","title":"Results","text":"<p>Results showed that the models trained on the patient safety incident reports using either the Masked Language Model (MLM) objective, or the MLM plus contrastive loss objective, appeared to have a superior performance on the presented pseudo-tasks when compared to their general domain equivalent. Whilst the performance in the frozen setting did not match that of the full fine-tuned setting, we have not performed a thorough investigation, for instance we could look to utilising larger base models. Further there are other examples of promising approaches which can better utilise frozen language models at scale, such as prompt learning and parameter efficient fine-tuning.</p> Output Link Open Source Code &amp; Documentation Github Case Study Awaiting Sign-Off Technical report Here","tags":["NATURAL LANGUAGE PROCESSING","LLM","RESEARCH","UNSTRUCTURED DATA","TEXT DATA","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/p33_patientsafetylms/#_1","title":"Investigating Applying and Evaluating a Language Model to Patient Safety Data","text":"","tags":["NATURAL LANGUAGE PROCESSING","LLM","RESEARCH","UNSTRUCTURED DATA","TEXT DATA","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/p34_hypergraphs/","title":"Transforming Healthcare Data With Graph-Based Techniques","text":"Figure 1 form https://www.medrxiv.org/content/10.1101/2023.08.31.23294903v1.full.pdf: Four different types of unweighted, fully connected graph models with 3 nodes. (A) undirected graph, (B) undirected hypergraph, (C) directed graph, (D) directed hypergraph of B-hyperarcs, the only type of hyperarc considered in this work. Dotted lines here represent nodes as part of the tail set in each hyperarc. Edges have been colour coded to help identify children from their parents \u2014 looking top-down, we can observe how the directed edges (children) are generated from their corresponding undirected edges (parents). Note also the existence of self-edges in the directed representations. <p>In this project we explored (directed) hypergraphs as a novel tool for assessing the temporal relationships between coincident diseases, addressing the need for a more accurate representation of multimorbidity. Directed hypergraphs offer a high-order analytical framework that goes beyond the limitations of directed graphs in representing complex relationships. After exploring novel weighting schemes which can capture different aspects of the underlying data, we then turn our attention at the power of these higher-order models through the use of PageRank centrality to detect and classify the temporal nature of conditions. </p>","tags":["RESEARCH","TIME SERIES","MODELLING","PRIMARY CARE"]},{"location":"our_work/p34_hypergraphs/#results","title":"Results","text":"<p>See the associated publication and report for detailed learning around applying these techniques to Charlson indexed data to explore disease progression.  This work then seeded two further PhD Internships exploring the addition of temporal information and alternative graph representations. </p> Output Link Open Source Code &amp; Documentation github Case Study Awaiting Sign-Off Technical report Report Publication Representing Multimordbid Disease Progressions Using Directed Hypergraphs","tags":["RESEARCH","TIME SERIES","MODELLING","PRIMARY CARE"]},{"location":"our_work/p34_hypergraphs/#_1","title":"Transforming Healthcare Data With Graph-Based Techniques","text":"","tags":["RESEARCH","TIME SERIES","MODELLING","PRIMARY CARE"]},{"location":"our_work/p41_nhssynth/","title":"NHSSynth","text":"Structure of the workflow incorporating user configuration, data preprocessing, model selection, evaluation, and visualisation <p>This project seeked to take the learning from our previous work on variational autoencoders see SynthVAE with differential privacy for single table tabular data generation, and turn the code into a pipeline where experiments could be rigorously undertaken including comparison with other architectures (e.g. GANs), application to other datasets with comparable metrics, and experiments around constraining the direct acylic graph to deal with biases in the data.  </p>","tags":["SYNTHETIC DATA","STRUCTURED DATA","MACHINE LEARNING","GENERATION","RESEARCH"]},{"location":"our_work/p41_nhssynth/#results","title":"Results","text":"<p>The pipeline is contained within the open code and allows for both config files to be run or a simpler command line interface.   Models can be switched in and out with a moderate amount of effort allowing for consistent comparisons and taking our synthetic generation work from a single investigation of a model to exploring how the latest models compare to our current workflows. </p> <p>Further work is needed to fix a bug when applying constraints and to enforce the mixed Guassian model to include higher modes. </p> Output Link Open Source Code &amp; Documentation Github Case Study Awaiting Sign-Off Technical report io page documentation","tags":["SYNTHETIC DATA","STRUCTURED DATA","MACHINE LEARNING","GENERATION","RESEARCH"]},{"location":"our_work/p41_nhssynth/#_1","title":"NHSSynth","text":"","tags":["SYNTHETIC DATA","STRUCTURED DATA","MACHINE LEARNING","GENERATION","RESEARCH"]},{"location":"our_work/p42_mortalityhypergraphs/","title":"Including Mortality in Hypergraphs for Multi-morbidity","text":"View of different charleston conditions highlight if there are more commonly predesecors or successors.  When moralitiy is included this skews the results unless correction is implemented. <p>This project investigated an approach to analysing disease set patterns using hypergraphs and multimorbidity data. Hypergraphs provide a powerful framework for modelling complex relationships among diseases, and their integration with multimorbidity data offers a comprehensive understanding of the co-occurrence of multiple diseases within patient populations. Additionally, this work extends on the previous work by incorporating mortality information into the hypergraphs and exploring the concept of temporality as hyperarc weights. The inclusion of mortality data enhances the analysis by considering the impact of diseases on patient outcomes, whilst temporality enables the inclusion of irregular time intervals which captures the dynamic nature of multimorbidity patterns over time.</p>","tags":["RESEARCH","TIME SERIES","MODELLING","PRIMARY CARE"]},{"location":"our_work/p42_mortalityhypergraphs/#results","title":"Results","text":"<p>To facilitate the understanding of hypergraphs and their applications in the multimorbidity domain, an interactive applet has been developed. This serves as an educational tool and visualisation device, teaching users about undirected and directed hypergraphs and demonstrating their usefulness in analysing complex disease relationships. We hope that our applet and the code bases we have created will promote the dissemination of knowledge about hypergraphs and their applications, empowering individuals to explore and comprehend complex healthcare data in the multimorbidity domain.</p> <p>See the full report for detailed results around the addition of mortality into this work.</p> Output Link Open Source Code &amp; Documentation GitHub Case Study Awaiting Sign-Off Technical report Report Demonstration Streamlit","tags":["RESEARCH","TIME SERIES","MODELLING","PRIMARY CARE"]},{"location":"our_work/p42_mortalityhypergraphs/#_1","title":"Including Mortality in Hypergraphs for Multi-morbidity","text":"","tags":["RESEARCH","TIME SERIES","MODELLING","PRIMARY CARE"]},{"location":"our_work/p43_medcat/","title":"Enriching Clinical Coding for Neurology Pathways using MedCAT","text":"<p>Neurology and other clinical specialities are awash with clinical data. However, these are generally not structured and lack the characteristics to allow straightforward automatic extraction of clinically relevant concepts. Software tools do exist that can recognise clinical terms in unstructured clinical data (e.g. clinic letters) and link them to other concepts. These are called \u2018named entity recognition and linking\u2019 (NER+L) tools. But many such tools require prior \u2018labelling\u2019 by a domain expert (i.e. person with specialty knowledge) of the relevant clinical concepts. MedCAT is a NER+L tool that can work without this prior labelling as it contains an algorithm that is aligned with a customisable knowledge database (ontology). This works in two stages: 1) linking unambiguous portions of texts (entities) to unique terms in the ontology then 2) linking ambiguous entities to terms in the ontology with the most similar contexts. </p> <p>However, evaluation of the MedCAT models which inform the NER+L process has only been performed on labelled data, and the learned numerical representations of concepts (embeddings) has not been assessed before.  The contributions of this project were: </p> <ol> <li>evaluation of three separate MedCAT models, </li> <li>comparison of three different clustering techniques as evaluation methods in the absence of labelled data, </li> <li>evaluation of MedCAT\u2019s learned concept embeddings, </li> <li>comparison of intrinsic and extrinsic evaluation metrics and </li> <li>comparison of qualitative and quantitative evaluation approaches. </li> </ol> <p></p> Schematic representation of the MedCAT workflow","tags":["NATURAL LANGUAGE PROCESSING","UNSTRUCTURED DATA","RESEARCH"]},{"location":"our_work/p43_medcat/#results","title":"Results","text":"<p>We found that all three models produced NER+L results which are not consistent with clinical understanding. Clustering can enable deeper examination of learned embeddings, but further work needs to be done on finding the best input data and clustering approach. Intrinsic evaluation metrics are only meaningful in the presence of extrinsic measures and further research needs to be done to identify the most informative set of metrics. Quantitative assessment must be supplemented by qualitative inspection. The work performed here forms the first phase in evaluation of MedCAT models\u2019 performance. Once optimal evaluation strategies have been identified, the next phase can be focused on improving MedCAT models. This will ultimately enable extraction of clinical terms that can be used for multiple downstream tasks such as automated clinical coding, research, monitoring of interventions, audits as well as service improvements.</p> Output Link Open Source Code &amp; Documentation Github Case Study Awaiting Sign-Off Technical report Report","tags":["NATURAL LANGUAGE PROCESSING","UNSTRUCTURED DATA","RESEARCH"]},{"location":"our_work/p43_medcat/#_1","title":"Enriching Clinical Coding for Neurology Pathways using MedCAT","text":"","tags":["NATURAL LANGUAGE PROCESSING","UNSTRUCTURED DATA","RESEARCH"]},{"location":"our_work/p51_privconcerns/","title":"Investigating Privacy Risks and Mitigations in Healthcare Language Models","text":"<p>See our blog on this work here</p> Output Link Open Source Code &amp; Documentation Code Case Study Awaiting Sign-Off Technical report Report]","tags":["LLM","PATIENT IDENTIFIABLE DATA","RESEARCH"]},{"location":"our_work/p51_privconcerns/#_1","title":"Investigating Privacy Risks and Mitigations in Healthcare Language Models","text":"","tags":["LLM","PATIENT IDENTIFIABLE DATA","RESEARCH"]},{"location":"our_work/p52_processmining/","title":"Process Mining with East Midlands Ambulance Service","text":"<p>Process Mining is the term given to a family of techniques that have been developed to analyse process flows through the recording of an event log. Process Mining has three main application areas: discovery, conformance checking, and enhancement. In process discovery, the output is a fact-based process model based on the recorded events in the database. Discovery is the most common process mining investigation, where the discovered process model is often not expected by the supervisor of the process. In conformance checking we measure how well the recorded processes fits a pre-defined process model. Conformance-checking techniques can therefore be used to check whether certain rules and policies are being abided by within an organisation. Finally, enhancement is focused on improving the existing process models by analysing how additional attributes affect the throughput times and frequencies of activities in the process.</p> <p>For this work we converted ambulance data into an event log and applied process mining techniques using the PM4Py library.  We used the PM2 methodologies to conduct the process mining project.</p> <p></p> PM2 Framework","tags":["EMERGENCY CARE","SIMULATION","MODELLING"]},{"location":"our_work/p52_processmining/#results","title":"Results","text":"<p>Most of the project time was spent designing appropriate business rules to be applied to the data in order to create the event log.  Directly Follows Graphs (DFGs) were then created to represent the highest occurring processes. Conformance techniques were used to measure how well the mined process models fitted the event log data. However, the variation in the treatment activities also caused the mined models to allow events including the call for an ambulance to happen in a sequence counterintuitive to real ambulance job cycles.</p> <p></p> A simple spaghetti diagram showing the process flow through activities captured in the event log <p>A succession of further process mining techniques were then applied to the data. A majority of these involved enriching the data and the iterative process around adding data to the event log to find out more about it. This is especially important with event logs where two processes account for such a high percentage of cases, (99% in our case).  Enrichment here meant attaching IMD information to postcodes or areas in the data as well as retrieving additional data about each patient, for example, age, sex, ethnicity, and adding that back into the event log.  Enrichment also meant feature engineering - transforming variables into either binary, or time of day into categorical, e.g. to morning, afternoon and night.</p> <p>Lastly, machine learning was applied to see if outcomes could be predicted. Please see the report or case study for full details.</p> Output Link Open Source Code &amp; Documentation github Case Study Awaiting Sign-Off Technical report Report","tags":["EMERGENCY CARE","SIMULATION","MODELLING"]},{"location":"our_work/p52_processmining/#_1","title":"Process Mining with East Midlands Ambulance Service","text":"","tags":["EMERGENCY CARE","SIMULATION","MODELLING"]},{"location":"our_work/p61_mmfair/","title":"Understanding Fariness and Explainability in Multimodal Approaches within Healthcare","text":"<p>Explainability, fairness, and bias identification and mitigation are all essential components for the integration of artificial intelligence (AI) solutions in high-stake decision making processes such as in healthcare. Whilst there have been developments in strategies to generate explanations and various fairness criteria across models, there is a need to better understand how multimodal methods impact these behaviours. Multimodal AI (MMAI) provides opportunities to improve performance and gain insights by modelling correlations and representations of data of different types. </p> <p>These approaches are incredibly powerful for the analysis of healthcare data, where the integration of data sources is key for gaining a holistic view of individual patients (personalised medicine) or evaluating models across different patient profiles to ensure safe and ethical use (population health). However, MMAI presents an unique challenges when deciding how best to incorporate and fuse information, maintaining an understanding of how data is processed (explainability), and ensuring bias is not amplified as a result.</p> <p>Here, we explore a case study, Multimodal Fusion of Electronic Health Data for Length-of-Stay Prediction, with a focus on treating time-series and static electronic health data as distinct modalities. We evaluate and compare different methods for fusing data in terms of predictive performance and various fairness metrics. Additionally, we apply SHAP to highlight the influence of specific features and explore how such explanations can be used to reveal or confirm bias in the underlying data. Our results showcase the importance of modelling time-series data, and an overall robustness to bias compared to unimodal approaches across various fairness metrics. We also describe exploratory analysis which can be conducted and developed further to mitigate bias post-hoc, or gain further insights into the relative importance of specific modalities from multimodal models.</p> <p></p> Overview of the models: unimodal, multimodal with two fusion methods: concatenation and a multiadaptation gate (MAG)","tags":["MULTI MODAL","RESEARCH","OPEN DATA"]},{"location":"our_work/p61_mmfair/#results","title":"Results","text":"<p>We found a reassuring consistency in feature importance across different fusion methods.  Both fusion methods produced fairer models with respect to insurance type (the most unfair variable in the unimodal case) with concatenation providing the lowest equalised odds.</p> <p>Further work is planned to expand this pipeline to other modalities and datasets to solidify our understanding of the interplay of model choices on fiarness.</p> Output Link Open Source Code &amp; Documentation Github Case Study Awaiting Sign-Off Technical report Report","tags":["MULTI MODAL","RESEARCH","OPEN DATA"]},{"location":"our_work/p61_mmfair/#_1","title":"Understanding Fariness and Explainability in Multimodal Approaches within Healthcare","text":"","tags":["MULTI MODAL","RESEARCH","OPEN DATA"]},{"location":"our_work/parkinsons-detection/","title":"Parkinson's Disease Pathology Prediction","text":"<p>\"Parkinson's Disease Pathology Prediction\" was selected as a project in 2022 following a successful pitch to the AI Skunkworks problem-sourcing programme.</p>","tags":["DISEASES","NEURAL NETWORKS","DEEP LEARNING","MODELLING","UNSTRUCTURED DATA","VISUAL DATA","CLASSIFICATION","PYTHON","COMPLETE"]},{"location":"our_work/parkinsons-detection/#results","title":"Results","text":"<p>A proof-of-concept demonstrator written in Python (machine learning models, command line interface (CLI), Jupyter Notebooks).</p>","tags":["DISEASES","NEURAL NETWORKS","DEEP LEARNING","MODELLING","UNSTRUCTURED DATA","VISUAL DATA","CLASSIFICATION","PYTHON","COMPLETE"]},{"location":"our_work/parkinsons-detection/#case-study","title":"Case Study","text":"<p>This is a backup of the case study published here on the NHS England Transformation Directorate website.</p> <p>Identification of Parkinson\u2019s Disease is carried out by neuropathologists who analyse post-mortem brain slices. This process is highly time intensive, and the neuropathologists are highly trained in their field. Being able to look at introducing automation to this process has potential to increase the speed at which Parkinson\u2019s Disease can be diagnosed in a brain, as well as freeing up neuropathologists who are otherwise required to spend hours looking at the brain slices themselves.</p> <p>The challenge Develop an approach to enhance the identification of biomarkers which are indicative of Parkinson\u2019s Disease, and explore whether automated identification of Parkinson\u2019s Disease in these slices is possible.</p>","tags":["DISEASES","NEURAL NETWORKS","DEEP LEARNING","MODELLING","UNSTRUCTURED DATA","VISUAL DATA","CLASSIFICATION","PYTHON","COMPLETE"]},{"location":"our_work/parkinsons-detection/#overview","title":"Overview","text":"<p>Parkinson\u2019s UK Brain Bank, at Imperial College London, receives brains which are donated by people who have taken the decision to donate their brain prior to their passing. The Parkinson\u2019s UK Brain Bank is the world's only brain bank solely dedicated to Parkinson's research1. Accurate identification and Parkinson\u2019s Disease in post-mortem brain tissue is critical to ensure that the brain is as useful as possible in research studies. These research studies aim to help us to understand what causes Parkinson\u2019s Disease and how drugs can be developed for Parkinson\u2019s Disease.</p> <p>Parkinson\u2019s Disease can be diagnosed in a slice of the brain by looking for the presence of a type of protein, which is \u2018stained\u2019 (their colour is changed to a more contrasting one) using a chemical. This protein is important, as Lewy Bodies are up of these proteins, and the presence of Lewy Bodies can be indicative of Parkinson\u2019s Disease.</p> <p>Once the staining is undertaken, a digital image of the brain slice can be taken. A number of these images are produced for different parts of the brain, and it is from these images that the neuropathologist produces their diagnosis.</p> <p>A significant benefit of working with the Parkinson\u2019s UK Brain Bank was having access to images of a large number of brains, which have been consistently processed and recorded as images. This gave the AI tooling a strong starting point to be developed from.</p> <p>In this project, a proof-of-concept solution was developed which had two key steps. First, brain slices were synthetically stained (i.e. staining was performed on the digital images using algorithms) to highlight the proteins of interest in bright, contrasting colours. This aimed to provide a view for neuropathologists that was less intensive to analyse.</p> <p>Following this step, an automated classifier was developed which used the synthetically stained brain slices to produce a judgement as to whether or not the image contained evidence of Parkinson\u2019s Disease. This classifier was able to do this with performance exceeding that of experts manually reviewing the images.</p>","tags":["DISEASES","NEURAL NETWORKS","DEEP LEARNING","MODELLING","UNSTRUCTURED DATA","VISUAL DATA","CLASSIFICATION","PYTHON","COMPLETE"]},{"location":"our_work/parkinsons-detection/#what-we-did","title":"What we did","text":"<p>Over a 12-week period, the project used the Parkinson\u2019s UK Brain Bank images to explore the viability of using AI to identify Parkinson\u2019s Disease.This rapid innovation is intended to explore a proof of concept that could help predict where Parkinson\u2019s Disease was present in brain slice images.</p> <p>Overall, about 400 brains were used for this project, split into those both with and without Parkinson\u2019s Disease present. The images from these brains were very detailed as they were taken using microscopes capable of magnifying at 200x what an eye can normally see. This also meant that the images were very large files, so had to be processed using powerful computer hardware. This ensured that analysis could happen within a reasonable timeframe.</p> <p>The project produced a solution which can be split into two key steps. The first of those was synthetic staining, where the existing images had an algorithm applied to them in order to highlight alpha-synuclein (also known as \u03b1-syn or \u03b1S) proteins more clearly. These are the proteins which are indicative of Parkinson\u2019s Disease being present. The second step was to automatically classify images of brain slices into whether they contain evidence of Parkinson\u2019s Disease or not.</p>","tags":["DISEASES","NEURAL NETWORKS","DEEP LEARNING","MODELLING","UNSTRUCTURED DATA","VISUAL DATA","CLASSIFICATION","PYTHON","COMPLETE"]},{"location":"our_work/parkinsons-detection/#synthetically-staining-the-brain-slices","title":"Synthetically staining the brain slices","text":"<p>Synthetic staining of the brain slice image refers to taking the original images of slices of brains, and applying an algorithm to them to highlight regions of interest. The benefit of this is to aid neuropathologists in being able to spot relevant material more quickly. This could speed up the overall process of diagnosing Parkinson\u2019s Disease in an image of a brain slice.</p> <p>The chosen algorithm was a pre-trained type of neural network which had been specifically designed to understand colour, texture and spatial elements of an image. The neural network is provided with hints as to the colours of certain elements in an image. Using this information, and the algorithm\u2019s own understanding of colour, texture and space within images, the algorithm attempts to colour the entire image. The results produced were successful in highlighting \u03b1-syn proteins very clearly. Additionally, it was seen that in brain slices without any \u03b1-syn proteins, the synthetic staining tended to not incorrectly stain these images. More details on the approach used can be found in the technical report (here).</p> <p></p> <p>Figure 1: An example of the synthetic staining process. a) the original slide, containing the \u03b1-syn proteins stained in a brownish colour b) a processed version of the original slide, filtered for the brownish colour c) the synthetically stained image after the algorithm has been applied to it. The \u03b1-syn proteins are now highlighted in a greenish colour.</p>","tags":["DISEASES","NEURAL NETWORKS","DEEP LEARNING","MODELLING","UNSTRUCTURED DATA","VISUAL DATA","CLASSIFICATION","PYTHON","COMPLETE"]},{"location":"our_work/parkinsons-detection/#identifying-presence-of-parkinsons-disease","title":"Identifying presence of Parkinson\u2019s Disease","text":"<p>With the ability to synthetically stain images, the next stage of the project could be attempted. This step aimed to classify images of brain slices into whether or not evidence of Parkinson\u2019s Disease was present in the image. For this, a particular type of Neural Network which had previously been demonstrated to work quickly on large datasets. This was important, as while powerful computers were used, the selection of an inappropriate algorithm might have meant that no useful results could be obtained within the 12 week time frame.</p> <p>The neural network model takes in the synthetically stained images and gives a classification for whether Parkinson\u2019s Disease is present in the image. The results seen using this method were excellent and could match (and exceed) the performance of experts in some aspects.</p>","tags":["DISEASES","NEURAL NETWORKS","DEEP LEARNING","MODELLING","UNSTRUCTURED DATA","VISUAL DATA","CLASSIFICATION","PYTHON","COMPLETE"]},{"location":"our_work/parkinsons-detection/#outcomes-and-lessons-learned","title":"Outcomes and lessons learned","text":"<p>In summary, the code from this project, released as open source on our Github (available to anyone to re-use, link here), demonstrates the viability of automation of identification of Parkinson\u2019s Disease in post-mortem brain slices. This is achieved by synthetically staining the images, before applying a neural network to predict whether or not the disease is present.</p> <p>The tool achieves cutting edge performance, and demonstrates ability which exceeds that of expert raters in some aspects. Despite this only being a proof of concept, results were promising, which is exciting for future further development of this work.</p> <p>A key lesson learned was the importance of good data. This is particularly true in projects involving images where quality and way the image was captured can vary significantly. A key driver of success in this project was having lots of consistent and high quality images to work with.</p>","tags":["DISEASES","NEURAL NETWORKS","DEEP LEARNING","MODELLING","UNSTRUCTURED DATA","VISUAL DATA","CLASSIFICATION","PYTHON","COMPLETE"]},{"location":"our_work/parkinsons-detection/#whats-next","title":"What\u2019s next?","text":"<p>The NHS AI Lab Skunkworks team has released the code from the project on Github to allow anyone to try the methodology out using fake data which has a similar appearance to the images of brain slices.</p> <p>A second phase is currently being planned to develop this work further, continuing to work in conjunction with Polygeist, Parkinson\u2019s UK and the Parkinson\u2019s UK Brain Bank at Imperial College London.</p>","tags":["DISEASES","NEURAL NETWORKS","DEEP LEARNING","MODELLING","UNSTRUCTURED DATA","VISUAL DATA","CLASSIFICATION","PYTHON","COMPLETE"]},{"location":"our_work/parkinsons-detection/#who-was-involved","title":"Who was involved?","text":"<p>This project was a collaboration between the NHS AI Lab Skunkworks, within the Transformation Directorate at NHS England and NHS Improvement, Parkinson\u2019s UK, Parkinson\u2019s UK Brain Bank at Imperial College London, Polygeist and the Home Office\u2019s Accelerated Capability Environment (ACE).</p> <p>NHS AI Lab Skunkworks is a team of data scientists, engineers and project leaders who support the health and social care community to rapidly progress ideas from the conceptual stage to a proof of concept.</p> <p>Accelerated Capability Environment (ACE) is part of the Homeland Security Group within the Home Office. It provides access to more than 250 organisations from across industry, academia and the third sector who collaborate to bring the right blend of capabilities to a given challenge. Most of these are small and medium-sized enterprises (SMEs) offering cutting-edge specialist expertise.</p> <p>ACE is designed to bring innovation at pace, accelerating the process from defining a problem to developing a solution and delivering practical impact to just 10 to 12 weeks.</p> <p>Polygeist, a software company specialising in state-scale analytics, builds world-leading AI technology for defence, national security, law enforcement, and healthcare customers. The team for this project was able to produce a live system, producing insights, from a standing start, in 12 weeks.</p> Output Link Open Source Code &amp; Documentation Github Technical report biorxiv.org Case Study Case Study","tags":["DISEASES","NEURAL NETWORKS","DEEP LEARNING","MODELLING","UNSTRUCTURED DATA","VISUAL DATA","CLASSIFICATION","PYTHON","COMPLETE"]},{"location":"our_work/parkinsons-detection/#_1","title":"Parkinson's Disease Pathology Prediction","text":"","tags":["DISEASES","NEURAL NETWORKS","DEEP LEARNING","MODELLING","UNSTRUCTURED DATA","VISUAL DATA","CLASSIFICATION","PYTHON","COMPLETE"]},{"location":"our_work/pets/","title":"Emerging Privacy Enhancing Technologies","text":"<p>Privacy Enhancing Technologies (PETs) are an emerging set of technologies and approaches that enable the derivation of useful results from data without providing full access to the data.   Whilst traditional de-identification of data reduces the utility as the privacy increases, PETs aim to keep the privacy very high whilst gaining a high enough level of utility that complex functions can be applied to the data.  </p> <p>To achieve a high confidence in the privacy of data we can either turn to a hardware approach or alternatively explore different statistical algorithms.   The hardware approach includes the use of trusted execution environments or enclaves which have absolute control on all the inputs and outputs into a space and thus can be used for secure controlled computation.   The algorithmic approach aims to allow certain operations on the accessible data which can be proven to have absolute or very high privacy.   These include Homomorphic Encryption and Secure Multi-Party Computation.   In addition output privacy can also be increase through the use of differential privacy to add a statistically significant level of noise making raw data inference impossible.  See the CDEIUK PETs adoption guide for more details.</p> <p></p> Not this sort of PET <p>The Royal society report \u201cFrom privacy to partnership The role of privacy enhancing technologies in data governance and\u00a0collaborative analysis\u201d 1 states that</p> <p>\u201cRecent advances in medical imaging, audio and AI have led to unprecedented possibilities in healthcare and research. This is especially true of the UK, where the public health system is replete with population-scale electronic patient records. These conditions, coupled with strong academic and research programmes, mean that the UK is well positioned to deliver timely and impactful health research and its translation to offer more effective treatments, track and prevent public health risks, utilising health data to improve and save lives 2.</p> <p>Anonymous data is not covered by current data protection law in the UK and EU. However, it is difficult to be certain that health data is anonymous, particularly in biometric and other non-textual data. Health data is subject to specific legal requirements in the UK, as well as the common law duty of confidentiality.\u201d**</p> <p>There is therefore a large need to investigate different ways of understanding and increasing privacy of data in healthcare in order to be confident of patient privacy and compliant. </p> <p>Three high level example use-cases of using PETs are:</p> <ul> <li>One-off Access for Research &amp; Analysis (such as federated learning across several trust secure data stores)</li> <li>Continual Access to User (such as synthetic data in secure data environment or regular API requests to data with a privacy accountant)</li> <li>Auto workflow (built with biometric data flows) (such as homomorphic encryption of biometric data to a central processing enclave)</li> </ul>","tags":["RESEARCH","OPEN DATA","WIP"]},{"location":"our_work/pets/#results","title":"Results","text":"<p>Our work supported the UK-US PETs Prize Challenge which brought together various teams competing to combine different PETs to allow AI models to learn to make better predictions without exposing any sensitive data with a prize pool of \u00a31.3m.</p> <p>Please see the challenge site for the winners.   </p> <p>A variety of solutions were put forwards including:</p> <ul> <li>Bloom filters with hashing encryption to allow privacy preserving feature mining (Scarlet-PETs GitHub)</li> <li>Synthetic Data using a variational autoencoder with differential privacy.   Adversarial attacks are then simulated at gateways to ensure privacy.  The synthetic data is then used with secure aggregation to develop a global model</li> <li>Differential Privacy with FedAvg kenziyuliu GitHub</li> <li>Differential Privacy with Homomorphic Encryption to pass the data to a secure aggregator Muscat Github</li> </ul> <p>Other open source examples can be see here)</p> Output Link Open Source Code &amp; Documentation Challenge Website Case Study Awaiting Sign-Off Technical report Coming Soon","tags":["RESEARCH","OPEN DATA","WIP"]},{"location":"our_work/pets/#_1","title":"Emerging Privacy Enhancing Technologies","text":"","tags":["RESEARCH","OPEN DATA","WIP"]},{"location":"our_work/ratings-and-reviews/","title":"NHS.UK: Automatic Moderation of Ratings & Reviews","text":"<p>The NHS.UK website receives around a hundred thousand reviews every year. These reviews need moderating -- there's a set of NHS policies which need to be applied to these before they can be published.</p> <p>This project automates much of that work. We use machine learning models - some built by us, some open-source - to make decisions about the different rules which need to be enforced. Users now receive instant feedback on when a review violates a rule, allowing them to edit and re-submit their review without delay. This reduces the average moderation time from days to seconds, makes for more reliable and consistent moderation, and creates a much more scalable service.</p> <p>Taking a ground truth from expert moderators, our models perform comparably to the people who used to do the moderation<sup>1</sup> manually. If a user disagrees with a decision our AI makes, there's still a human-in-the-loop who can make a final moderation decision.</p> <p></p> <p>The flow follows four stages:</p> <ol> <li>The review text is submitted to the website.</li> <li> <p>The Flask app contains the logic of the rules which are to be applied. Some of these rules are applied directly, on the Flask app compute.</p> </li> <li> <p>More complicated rules (such as detecting safeguarding concerns) are applied by models which are deployed independently of the Flask app. Here the Flask app sends queries to each of these, they evaluate the text, and send a response.</p> </li> <li> <p>The Flask app now has an answer for each of the rules which are to be applied, and can send a final response to the NHS.UK website.</p> </li> </ol> <p>If there's anything amiss, the website will produce a response informing the user (the person leaving the review) which rule has been broken. This ranges from asking the reviewer to remove something (such as a name) from a review, to directing them to resources specific to the content of their review. For most cases, the user has the opportunity to edit and re-submit their review. The reviews policy is here.</p>","tags":["WORKFORCE","NATURAL LANGUAGE PROCESSING","DATA VALIDATION","UNSTRUCTURED DATA","TEXT DATA","PYTHON","COMPLETE","DEPLOYED"]},{"location":"our_work/ratings-and-reviews/#results","title":"Results","text":"<ul> <li>Reduced moderation time from days to seconds.</li> <li>Gives users opportunity to edit reviews when rules are broken, which increases the proportion of reviews which will get published.</li> <li>Service is much more scaleable now - no longer constrained by moderation capacity.</li> <li>All models match or out-perform the way the work was done before.</li> <li>First AI product to gain clinical approval within NHS.UK.</li> </ul>","tags":["WORKFORCE","NATURAL LANGUAGE PROCESSING","DATA VALIDATION","UNSTRUCTURED DATA","TEXT DATA","PYTHON","COMPLETE","DEPLOYED"]},{"location":"our_work/ratings-and-reviews/#outputs","title":"Outputs","text":"Output Link Published repo for the Automoderation API Github Repo Published repo for the models Github Repo Algorithmic Transparency Recording Standard (ATRS) ATRS","tags":["WORKFORCE","NATURAL LANGUAGE PROCESSING","DATA VALIDATION","UNSTRUCTURED DATA","TEXT DATA","PYTHON","COMPLETE","DEPLOYED"]},{"location":"our_work/ratings-and-reviews/#_1","title":"NHS.UK: Automatic Moderation of Ratings & Reviews","text":"<ol> <li> <p>Prior to this project, the moderation work was done manually by a third-party company, hired on a fixed-term contract.\u00a0\u21a9</p> </li> </ol>","tags":["WORKFORCE","NATURAL LANGUAGE PROCESSING","DATA VALIDATION","UNSTRUCTURED DATA","TEXT DATA","PYTHON","COMPLETE","DEPLOYED"]},{"location":"our_work/redbox_copilot/","title":"Redbox Copilot","text":"<p>We supported the Redbox Copilot work as part of a cross-departmental team through the 10DS Cover Scheme (via the Evidence House community). You can learn more about the work that was done and the app itself at the AI.GOV.UK website </p> <p></p>","tags":["WORKFORCE","LLM","NATURAL LANGUAGE PROCESSING","GENERATION","TEXT DATA","PYTHON","COMPLETE"]},{"location":"our_work/redbox_copilot/#results","title":"Results","text":"<p>The app used a \u201cRetrieval Augmented Generation\u201d pipeline based on Langchain and used the LLM \u201cClaude\u201d. The app allowed the user to ask questions about documents \u2013 the pipeline would retrieve relevant document chunks which were then passed to the LLM which would generate a response.</p> <p>Our main contributions were in ensuring this response gave references for its statements, and ensuring these came through correctly (getting LLMs to provide structured responses can require a bit of prompt engineering). We also assisted in the evaluation of the pipelines performance.</p> <p>The prototype was successful, and work on it has continued within 10DS and i.AI.</p> Output Link Open Source Code &amp; Documentation Github Case Study Redbox Copilot","tags":["WORKFORCE","LLM","NATURAL LANGUAGE PROCESSING","GENERATION","TEXT DATA","PYTHON","COMPLETE"]},{"location":"our_work/redbox_copilot/#_1","title":"Redbox Copilot","text":"","tags":["WORKFORCE","LLM","NATURAL LANGUAGE PROCESSING","GENERATION","TEXT DATA","PYTHON","COMPLETE"]},{"location":"our_work/renal-health-prediction/","title":"Renal Health Prediction","text":"<p>Renal Health Prediction was selected as a project in Spring 2022 following a successful pitch to the AI Skunkworks problem-sourcing programme.</p>","tags":["DISEASES","DEEP LEARNING","NEURAL NETWORKS","CLASSIFICATION","MODELLING","STRUCTURED DATA","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/renal-health-prediction/#results","title":"Results","text":"<p>A proof-of-concept demonstrator written in Python (machine learning models, command line interface (CLI), Jupyter Notebooks).</p> Output Link Open Source Code &amp; Documentation Github Technical report PDF Pre-print (MedRxiv) PDF","tags":["DISEASES","DEEP LEARNING","NEURAL NETWORKS","CLASSIFICATION","MODELLING","STRUCTURED DATA","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/renal-health-prediction/#_1","title":"Renal Health Prediction","text":"","tags":["DISEASES","DEEP LEARNING","NEURAL NETWORKS","CLASSIFICATION","MODELLING","STRUCTURED DATA","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/sde_data_validation/","title":"Reusable New Data Product Validation Functions","text":"<p>All data provisioned into the NHS England Secure Data Environment (SDE) must be validated first. The old data product validation process was manual, time consuming and lengthy to re-run.\u200b</p> <p>Our objectives were to: - Boost the efficiency and consistency of the data validation process for the Data Access Request Service (DARS) \u200b - Make it re-usable to save time and uphold best practice\u200b - Share the code so others can benefit. \u200b</p>","tags":["WORKFORCE","DATA VALIDATION","STRUCTURED DATA","PSEUDONYMISED","PYTHON","COMPLETE","DEPLOYED"]},{"location":"our_work/sde_data_validation/#results","title":"Results","text":"<ul> <li>Validation time down from days to approximately 30 minutes\u200b</li> <li>Validation code was reusable on other datasets\u200b and has already been reused</li> <li>Consistent methodology compared to manual approach\u200b</li> <li>Enabled multiple potential issues that could have hampered research efforts to be addressed earlier.\u200b</li> </ul> Output Link Open Source Code &amp; Documentation Coming soon! Case Study N/A Technical report N/A Algorithmic Impact Assessment N/A","tags":["WORKFORCE","DATA VALIDATION","STRUCTURED DATA","PSEUDONYMISED","PYTHON","COMPLETE","DEPLOYED"]},{"location":"our_work/sde_data_validation/#_1","title":"Reusable New Data Product Validation Functions","text":"","tags":["WORKFORCE","DATA VALIDATION","STRUCTURED DATA","PSEUDONYMISED","PYTHON","COMPLETE","DEPLOYED"]},{"location":"our_work/swpclab/","title":"Developing Artificial Primary Care Records","text":"<p>Primary care records are crucial for understanding healthcare interactions at both the population and individual levels. However, these records are difficult to obtain and integrate with other services, hindering innovation due to data unavailability and privacy concerns.</p> <p>Our project aims to address this by developing a code base to generate primary care electronic health records. We start by creating a synthetic population that mirrors a region in England, and then adapt the US-based tool Synthea for the English NHS context.</p> <p></p> <p>Figure 1: High-level schematic of the different components of the project. Population level relationships are derived from the raw data and feed through a simple synthetic data generator to create a population. Clinical knowledge is captured as a clinical reference pathway (CRP) and fed into the engine. This engine takes the synthetic population and processes them through the CRPs. This results in records being created and adapted for the synthetic individuals which can then be viewed at a patient (EHR), service (GP system) or System (regional) levels.</p>","tags":["PRIMARY CARE","DISEASES","SIMULATION","MODELLING","GENERATION","STRUCTURED DATA","TIME SERIES","SYNTHETIC DATA","PAUSED","IN DEVELOPMENT"]},{"location":"our_work/swpclab/#project-overview","title":"Project Overview","text":"","tags":["PRIMARY CARE","DISEASES","SIMULATION","MODELLING","GENERATION","STRUCTURED DATA","TIME SERIES","SYNTHETIC DATA","PAUSED","IN DEVELOPMENT"]},{"location":"our_work/swpclab/#task-1-creating-a-synthetic-population-with-faker","title":"Task 1: Creating a Synthetic Population with Faker.","text":"<p>We begin by generating a table of fake but realistic individuals using a combination of the Faker and random libraries, based on demographic profiles and disease prevalence probabilities. These records are validated against the OneDevon dataset to ensure accuracy and privacy.</p>","tags":["PRIMARY CARE","DISEASES","SIMULATION","MODELLING","GENERATION","STRUCTURED DATA","TIME SERIES","SYNTHETIC DATA","PAUSED","IN DEVELOPMENT"]},{"location":"our_work/swpclab/#task-2-digitizing-clinical-pathways","title":"Task 2: Digitizing Clinical Pathways","text":"<p>To simulate real-world patient interactions, we translate clinical knowledge into digital formats. Using Synthea's graphical interface, we create JSON files representing clinical pathways, starting with the Hypertension and Hypertension Medication modules. This involves adapting US modules to match English drug names, values, and logic starting by aligning to the NICE guidelines.</p>","tags":["PRIMARY CARE","DISEASES","SIMULATION","MODELLING","GENERATION","STRUCTURED DATA","TIME SERIES","SYNTHETIC DATA","PAUSED","IN DEVELOPMENT"]},{"location":"our_work/swpclab/#task-3-customizing-synthea-for-the-english-nhs","title":"Task 3: Customizing Synthea for the English NHS","text":"<p>Whilst the adaption of synthea for international contexts is well established in their site, this results in records which still have many US elements. Adapting Synthea for the English NHS involves removing obsolete US elements like insurance logic and updating regional specifics such as town names and vaccination schedules. We are progressively modifying demographic, geographic, and societal health determinants to fit the English context.</p> Details of the Synthea Adaption <p>Stage 1: Removing non-English NHS functions and simplifying the Java to an MVP</p> <p>Functions relating to:</p> <ul> <li>Flexporter (functionality which could be brought back later)</li> <li>Payers and related managers</li> <li>Insurance plans</li> <li>Claims (mostly for medications)</li> <li>Income, healthcare expenses and coverage</li> <li>Cost</li> <li>Exporting as DSTU2 or STU3</li> <li>Cardiovascular disease module (as this is a US-based calculator)</li> <li>ASCVD, Framingam and C19 Immunizations (as these are all US-based and not applicable)</li> <li>CMSStateCodeMapper</li> </ul> <p>These functions have all been commented using a <code>UKAdp</code> tag to keep an audit trail. These adaptions result in 113 sections of code commented out across 16 files (all within the <code>src/main/java/org/mitre/synthea/</code>).</p> <p>Stage 2: Adapting Resource files for UK South West Region context</p> <ul> <li>Replace demographics.csv with South West towns and cities</li> <li>Replace fipscodes.csv with County GSS codes</li> <li>Update social determinants of health (sdoh.csv) file with food insecurity, severe housing cost burdens, unemployment, and vehicle access values correct for the UK regions.</li> <li>Replace timezones.csv with GMT</li> <li>Replace zipcodes.csv with uk based postcodes</li> <li>Keep birthweights.csv as US version (for the moment)</li> <li>Keep bmi_correlations.json as US version (for the moment)</li> <li>Keep cdc_growth_charts.json as US version (for the moment)</li> <li>Keep gbd_disability_weigths.csv as US version (for the moment)</li> <li>Update immunization_scheldule.json to vaccine schedules used in the UK</li> <li>Update synthea.properties to remove unused exporter and payer functionality and amend inputs for South West Region.</li> <li>Reduce the care settings down to hospitals, primary care and urgent care, and update these to have South West facilities.</li> </ul> <p>There are still many US-based nuances that need to be dealt with such as payer columns still appearing in the outputs.</p> <p>Stage 3: Nuances</p> <p>Coming. At the end of this stage we aim for a fully UK-base version.</p>","tags":["PRIMARY CARE","DISEASES","SIMULATION","MODELLING","GENERATION","STRUCTURED DATA","TIME SERIES","SYNTHETIC DATA","PAUSED","IN DEVELOPMENT"]},{"location":"our_work/swpclab/#next-steps","title":"Next Steps","text":"<p>Our work is ongoing, with updates available on our GitHub repository. Future plans are outlined in Figure 2, showcasing various potential directions for this project.</p> <p>Stay tuned for progress updates and check out our code development on GitHub.</p> <p></p> <p>Figure 2: Diagram of the potential to expand the tooling in a vairety of ways to increase fidelity of the generated records and include additional modalities.</p> Output Link Open Source Code &amp; Documentation Github - WIP Case Study Awaiting Sign-off","tags":["PRIMARY CARE","DISEASES","SIMULATION","MODELLING","GENERATION","STRUCTURED DATA","TIME SERIES","SYNTHETIC DATA","PAUSED","IN DEVELOPMENT"]},{"location":"our_work/swpclab/#_1","title":"Developing Artificial Primary Care Records","text":"","tags":["PRIMARY CARE","DISEASES","SIMULATION","MODELLING","GENERATION","STRUCTURED DATA","TIME SERIES","SYNTHETIC DATA","PAUSED","IN DEVELOPMENT"]},{"location":"our_work/synthetic-data-pipeline/","title":"Synthetic Data Generation Pipeline","text":"<p>The NHS AI Lab Skunkworks team has been releasing open-source code from their artificial intelligence (AI) projects since 2021. One of the challenges faced with releasing code is that without suitable test data it is not possible to properly demonstrate AI tools, preventing users without data access from being able to see the tool in action.</p> <p>One avenue for enabling this is to provide \u201csynthetic data\u201d, where new \u201cfake\u201d data is generated from real data using a specifically designed model, in a way that maintains several characteristics of the original data: Utility - is the synthetic data fit for its defined use? Quality - is the synthetic data a sufficient representation of the real data? Privacy - does the synthetic data \u2018leak\u2019 or expose any sensitive information from the real data?</p>","tags":["SIMULATION","GENERATION","RESEARCH","SYNTHETIC DATA","PATIENT IDENTIFIABLE DATA","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/synthetic-data-pipeline/#the-challenge","title":"The challenge\u2026","text":"<p>This project aimed to provide others with a simple, re-usable way of generating safe and effective synthetic data to be used in technologies that improve health and social care.</p> <p>Using real patient data for research and development carries with it safety and privacy concerns about the anonymity of the people behind the information. Various anonymisation techniques can be used to turn data into a form that does not directly identify individuals and where re-identification is not likely to take place. However, it is very difficult to entirely remove the chance of re-identification, so wide release of anonymised data will always carry some risks. Synthetic data removes the need for such concerns because there is no \u201creal patient\u201d connected with the data, so re-identification is not possible.</p> <p>Using a synthetic data generation model called SynthVAE, produced by the NHS Transformation Directorate\u2019s Analytics Unit,the Skunkworks team embarked on a joint project to produce a framework for generating synthetic data. The teams explored how SynthVAE could be used to generate synthetic data, how that data would be evaluated and how the whole process could be documented for others to re-use.</p>","tags":["SIMULATION","GENERATION","RESEARCH","SYNTHETIC DATA","PATIENT IDENTIFIABLE DATA","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/synthetic-data-pipeline/#ai-dictionary","title":"AI Dictionary","text":"<p>We have tried to explain the technical terms used in this case study. If you would like to explore the language of AI and data science, please visit our AI Dictionary.</p>","tags":["SIMULATION","GENERATION","RESEARCH","SYNTHETIC DATA","PATIENT IDENTIFIABLE DATA","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/synthetic-data-pipeline/#overview","title":"Overview","text":"<p>There are many ways to generate synthetic data, with SynthVAE being just one approach. One common challenge with synthetic data approaches is that they are usually configured specifically for a dataset. This is a problem because it means a significant amount of work is needed to update them for use with a different data source.  Additionally, once an approach has successfully produced data, it can be difficult to know whether what was generated using the approach is actually useful. Using approaches like SynthVAE currently requires rework of the source code each time a new dataset is used, and there is no standard set of checks that can be used for every dataset.</p> <p>The work carried out jointly by NHS AI Lab Skunkworks and the Analytics Unit sought to: Increase the range of synthetic data types that SynthVAE can generate (like categorical data and dates). SynthVAE originally used the SUPPORT dataset from PyCox, so finding a dataset with a wider range of data types would be helpful. Create a standard series of checks that can be carried out on the data produced, so that a user can better understand the characteristics of the synthetic data produced Implement a structure to allow users to run the full functionality with a single piece of code.</p>","tags":["SIMULATION","GENERATION","RESEARCH","SYNTHETIC DATA","PATIENT IDENTIFIABLE DATA","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/synthetic-data-pipeline/#what-we-did","title":"What we did","text":"<p>The two teams worked together to:</p> <ul> <li>identify a suitable open source dataset for the project.</li> <li>use this data source to generate an \u201cinput dataset\u201d that looks like real patient data</li> <li>adapt an existing synthetic code generator model (SynthVAE) and use it to produce synthetic patient data from the input dataset</li> <li>outline the checks that would need to be done to the synthetic data to confirm its quality and suitability</li> <li>pull these steps into a single user-friendly workflow process for anyone to use.</li> </ul>","tags":["SIMULATION","GENERATION","RESEARCH","SYNTHETIC DATA","PATIENT IDENTIFIABLE DATA","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/synthetic-data-pipeline/#creating-an-input-dataset","title":"Creating an input dataset","text":"<p>In order to further develop the capabilities of SynthVAE since it was first produced, an input dataset containing a number of different data types was required in order to broaden the range of the data produced. The teams chose one already in the public domain. This meant people wishing to use the code after release could access and use the same dataset with which the project was developed. MIMIC-III was selected because the size and variety of its data would enable us to produce an input file that would closely match the broad range of typical hospital data.</p> <p>We processed the raw MIMIC-III files to produce a single dataset which described treatment provided by a hypothetical set of patients. The resulting input file contained columns with numbers, categories and dates, as well as multiple entries for some patients. It looked similar to datasets that might be encountered in a real hospital setting, helping to keep this project as relevant as possible to potential stakeholders such as NHS data analysts and data scientists, as well as research teams within trusts who are interested in exploring the use of synthetic data.</p>","tags":["SIMULATION","GENERATION","RESEARCH","SYNTHETIC DATA","PATIENT IDENTIFIABLE DATA","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/synthetic-data-pipeline/#adapting-synthvae","title":"Adapting SynthVAE","text":"<p>SynthVAE, the Analytics Unit\u2019s \u201cVariational Autoencoder for generating Synthetic Data\u201d, uses an autoencoder architecture to train a model to compress the input data into a smaller number of variables, before attempting to reconstruct the original input information. Once the model is trained, the statistical distributions within the model are sampled and output data constructed from these samples. Due to the training process, the model tries to reconstruct  output data that looks like the original training data.</p> <p>SynthVAE was originally written primarily to generate synthetic data from both continuous data (data with an infinite number of values) and categorical data (data that can be divided into groups). The inclusion of dates in the new input dataset meant SynthVAE needed to be adapted to take the new set of variables.</p> <p>Once this was done, it was possible to use the input file to train a SynthVAE model, and then use that model to generate synthetic data. The model was used to generate a synthetic dataset containing several million entries, a substantial increase on volumes previously produced using SynthVAE.</p> <p>This wasn\u2019t without challenges, as SynthVAE hadn\u2019t been substantially tested using dates or large volumes of data. However, through close collaboration with the Analytics Unit, SynthVAE was successfully adapted to produce a synthetic version of the input data from MIMIC-III.</p>","tags":["SIMULATION","GENERATION","RESEARCH","SYNTHETIC DATA","PATIENT IDENTIFIABLE DATA","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/synthetic-data-pipeline/#creating-a-checking-process","title":"Creating a checking process","text":"<p>In order to evaluate the privacy, quality and utility of the synthetic data produced, a set of checks were needed. There is not currently an industry standard, so we chose to use an evaluation capability from Synthetic Data Vault (SDV), alongside other approaches which provide a broader range of assessments of the data. SDV\u2019s evaluation capability provides a wide range of metrics which are already implemented, giving a starting point for building a more complete evaluation approach. SDV\u2019s evaluation uses metrics to check whether your synthetic data would be a good substitute for the real data, without causing a change in performance (also known as the utility). The additional checks that were added aimed to make the evaluation of utility more robust, for example by checking there are no identical records in the synthetic and real datasets, but also to provide visual aids to allow the user to see what differences are present in the data.</p> <p>The checks included:</p> <ul> <li>Collision analysis - checking that no two  records are exactly the same in the input and synthetic datasets</li> <li>Correlation analysis - compares the relationship between the two datasets to see if patterns have been accurately preserved in the synthetic dataset</li> <li>Evaluating the Gower distance - looking at the closeness of similarity between the input and synthetic datasets to make sure they are not too similar</li> <li>Comparing each dataset using Principal Component Analysis - reducing the size of the data set to its principal components whilst keeping as much information as possible helps us see how similar the input and synthetic datasets are, and helps us to understand whether the synthetic dataset is useful</li> <li>Propensity testing - checking whether a model can differentiate between our real and synthetic data.We used a logistic regression model that had been trained on input data. We combined the real and synthetic data then fitted the logistic regression model to the data set. Using the fitted model, we could see how well it differentiated between the real and synthetic data by looking at its ability to predict how likely each row was real or synthetic.</li> <li>Comparison of the Voas-Williamson statistic - A global goodness of fit metric that compares the variation over degrees of freedom in the synthetic and ground truth data.</li> <li>Comparison of statistical distributions of the features - to get a high level view of the similarity of the two datasets, the categorical and numerical columns were compared visually. For a more in depth overview of both the real and synthetic datasets we used pandas-profiling to generate reports for each. Pandas profiling is a way of quickly exploring data using just a few lines of code instead of trying to understand every variable.</li> </ul> <p>These checks were combined and their results collected in a web-based report, to allow results to be packaged and shared with any data produced.</p>","tags":["SIMULATION","GENERATION","RESEARCH","SYNTHETIC DATA","PATIENT IDENTIFIABLE DATA","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/synthetic-data-pipeline/#creating-a-pipeline","title":"Creating a pipeline","text":"<p>To make the end-to-end process as user-friendly as possible, QuantumBlack\u2019s Kedro was employed. This is a pipelining library that allows functionality to be chained together, allowing a user to run a full set of scripts with a single command. It also allows a user to define all their parameters, features and settings in a configuration file, making it easier to know what is defined in the pipeline and how to change according to the needs of each user.</p> <p>The input data generation, SynthVAE training, synthetic data production and output checking processes were chained together, creating a single flow to train a model, produce synthetic data and then evaluate the final output.</p>","tags":["SIMULATION","GENERATION","RESEARCH","SYNTHETIC DATA","PATIENT IDENTIFIABLE DATA","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/synthetic-data-pipeline/#outcomes-and-lessons-learned","title":"Outcomes and lessons learned","text":"<p>The resulting code, to be released as open source (available to anyone to re-use), enables users to see how:</p> <ul> <li>an input dataset can be constructed from an open-source dataset, MIMIC-III</li> <li>SynthVAE can be adapted to be trained on a new input dataset with mixed data-types</li> <li>SynthVAE can be used to produce synthetic data</li> <li>synthetic data can be evaluated to assess it\u2019s privacy, quality and utility</li> <li>a pipeline can be used to tie together steps in a process for a simpler user experience.</li> </ul> <p>By using the set of evaluation techniques, concerns around the quality of the synthetic data can be directly addressed and measured using the variety of metrics produced as part of the report. The approach outlined here is not intended to demonstrate a perfectly performing synthetic data generation model, but instead to outline a pipeline that enables the generation and evaluation of synthetic data. Things like overfitting to the training data, and the potential for bias will be highlighted by the evaluation metrics but will not be remedied.</p> <p>Concerns around re-identification are reduced by using synthetic data, however they are not absolutely removed.To better understand the privacy of any patient data used to train a synthetic data generating model, the Analytics Unit have undertaken a project exploring the use of \u2018adversarial attacks\u2019 to prove what information about the training data can be ascertained from a model alone. The project focussed on a particular type of adversarial attack, a \u2018membership attack\u2019, and explored how different levels of information would influence what the attacker could learn about the underlying dataset, and therefore the implications to any individuals whose information was used to train a model.</p>","tags":["SIMULATION","GENERATION","RESEARCH","SYNTHETIC DATA","PATIENT IDENTIFIABLE DATA","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/synthetic-data-pipeline/#what-next","title":"What next?","text":"<p>AI Lab Skunkworks will be releasing the code from the project on our Github site to demonstrate how SynthVAE can be used in a practical, end-to-end configuration.</p> <p>The Analytics Unit is continuing to develop and improve SynthVAE, with a focus on improving the model\u2019s ability to produce high quality synthetic data.</p>","tags":["SIMULATION","GENERATION","RESEARCH","SYNTHETIC DATA","PATIENT IDENTIFIABLE DATA","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/synthetic-data-pipeline/#who-was-involved","title":"Who was involved?","text":"<p>This project was a collaboration between the NHS AI Lab Skunkworks  and the Analytics Unit within the Transformation Directorate at NHS England and Improvement.</p> <p>The NHS AI Lab Skunkworks is a team of data scientists, engineers and project leaders who support the health and social care community to rapidly progress ideas from the conceptual stage to a proof of concept.</p> <p>The Analytics Unit consists of a team of analysts, economists, data scientists and data engineers who provide leadership to other analysts who are working in the system and raise data analysis up the health and care system agenda.</p> Output Link Open Source Code &amp; Documentation Github Case Study Case Study","tags":["SIMULATION","GENERATION","RESEARCH","SYNTHETIC DATA","PATIENT IDENTIFIABLE DATA","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/synthetic-data-pipeline/#_1","title":"Synthetic Data Generation Pipeline","text":"","tags":["SIMULATION","GENERATION","RESEARCH","SYNTHETIC DATA","PATIENT IDENTIFIABLE DATA","PSEUDONYMISED","PYTHON","COMPLETE","EXPERIMENTAL"]},{"location":"our_work/tags/","title":"Project Tags","text":"<p>Below the tags are sorted by categories (Project Status, Data Sources, Domain Areas, Techniques, Coding Language, Data Types, Evaluation Metrics, and Project Type), however if you would like an alphabetical list of all of the tags it can be found here.</p> <ul> <li> <p> Domain Areas</p> <ul> <li>Primary care</li> <li>Secondary care</li> <li>Social Care</li> <li>Emergency care</li> <li>Diseases</li> <li>Public/Population Health</li> <li>Workforce &amp; Allocation</li> <li>Prescribing</li> <li>Financial</li> </ul> </li> <li> <p> Techniques</p> <ul> <li>Forecasting</li> <li>Classification</li> <li>Computer Vision</li> <li>Deep Learning</li> <li>Large Language Models (LLM)</li> <li>Machine Learning</li> <li>Neural Networks</li> <li>Natural Language Processing (NLP)</li> <li>Linkage</li> <li>Simulation</li> </ul> </li> <li> <p> Project Type (exploratory, modeling, etc.)</p> <ul> <li>Best Practice Guidance</li> <li>Data Validation</li> <li>Documentation</li> <li>Ethics</li> <li>Explainability</li> <li>Generation</li> <li>Linkage</li> <li>Modelling</li> <li>Reproducible Analytical Pipeline (RAP)</li> <li>Research</li> </ul> </li> <li> <p> Data Types</p> <p>Structure </p> <ul> <li>Structured</li> <li>Unstructured</li> </ul> <p>Data modality</p> <ul> <li>Multi-modal</li> <li>Sound/Audio</li> <li>Genomics</li> <li>Video/Image</li> <li>Text Data</li> <li>Time Series</li> </ul> <p>Data Accessibility/Privacy</p> <ul> <li>Patient Identifiable</li> <li>Synthetic</li> <li>Open Data</li> <li>Pseudonymised</li> </ul> </li> <li> <p> Coding Language</p> <ul> <li>Python</li> <li>SQL</li> <li>R</li> <li>Web Development (e.g. Javascript, HTML, CSS)</li> </ul> </li> <li> <p> Status</p> <ul> <li>Work in Progress</li> <li>Complete</li> <li>On Pause</li> </ul> <p>Deployment Level</p> <ul> <li>In Development</li> <li>Retired</li> <li>Deployed</li> <li>Experimental</li> </ul> </li> </ul>"},{"location":"our_work/tags/#alphabetical-list-of-tags","title":"Alphabetical List of Tags","text":""},{"location":"our_work/tags/#tag:best-practice","title":"BEST PRACTICE","text":"<ul> <li>            AI Assurance Research Path          </li> <li>            AI Deep Dive Workshops          </li> <li>            AI Ethics in Practice          </li> <li>            Data Linkage Community of Practice          </li> <li>            Privacy of Unstructured Data          </li> <li>            Quality Assurance Framework for Data Linkage          </li> <li>            Reproducible Analytical Pipelines Squad          </li> <li>            __index.md          </li> </ul>"},{"location":"our_work/tags/#tag:classification","title":"CLASSIFICATION","text":"<ul> <li>            AI Assurance Research Path          </li> <li>            AI Skunkworks Team          </li> <li>            Deep Learning to Detect Adrenal Lesions in CT Scans          </li> <li>            Impact of Commercial Data on Predictions          </li> <li>            Investigating Superpixels in LIME for Explaining Predictions of Facial Images          </li> <li>            Long Stayer Risk Stratification Baseline Models          </li> <li>            Parkinson's Disease Pathology Prediction          </li> <li>            Predicting Negligence Claims          </li> <li>            Renal Health Prediction          </li> </ul>"},{"location":"our_work/tags/#tag:complete","title":"COMPLETE","text":"<ul> <li>            AI Deep Dive Workshops          </li> <li>            AI Dictionary          </li> <li>            AI Models for Shortlisting Interview Candidates          </li> <li>            AI Skunkworks Team          </li> <li>            Adding a Clinical Focus to Evaluating MM Data Representations          </li> <li>            Applying &amp; Evaluating a Language Model to Patient Safety Data          </li> <li>            Bed Allocation          </li> <li>            CT Alignment &amp; Lesion Detection          </li> <li>            Creating a Generic Adversarial Attack for Synthetic Data          </li> <li>            Data Lens          </li> <li>            Deep Learning to Detect Adrenal Lesions in CT Scans          </li> <li>            Developing SynthVAE          </li> <li>            Impact of Commercial Data on Predictions          </li> <li>            Inequalities in Diabetes from PHM Data          </li> <li>            Investigating Superpixels in LIME for Explaining Predictions of Facial Images          </li> <li>            Length of Hospital Day Prediction          </li> <li>            Long Stayer Risk Stratification Baseline Models          </li> <li>            MPS Handbook          </li> <li>            NHS @Home Programme          </li> <li>            NHS.UK Automatic Moderation of Ratings &amp; Reviews          </li> <li>            Nursing Placement Scheduled Optimisation          </li> <li>            Parkinson's Disease Pathology Prediction          </li> <li>            Predicting Negligence Claims          </li> <li>            Privacy of Unstructured Data          </li> <li>            Quality Assurance Framework for Data Linkage          </li> <li>            Redbox Copilot          </li> <li>            Renal Health Prediction          </li> <li>            Reproducible Analytical Pipelines Squad          </li> <li>            Reusable Data Validation Process          </li> <li>            Synthetic Data From Real Data          </li> <li>            Synthetic Data Generation Pipeline          </li> <li>            Text Analysis using Structural Topic Modelling          </li> <li>            TxtRayAlign          </li> <li>            Waiting List Minimum Dataset (WLMDS) Proofs of Concept          </li> </ul>"},{"location":"our_work/tags/#tag:computer-vision","title":"COMPUTER VISION","text":"<ul> <li>            AI Assurance Research Path          </li> <li>            CT Alignment &amp; Lesion Detection          </li> <li>            Deep Learning to Detect Adrenal Lesions in CT Scans          </li> </ul>"},{"location":"our_work/tags/#tag:data-validation","title":"DATA VALIDATION","text":"<ul> <li>            Creating a Generic Adversarial Attack for Synthetic Data          </li> <li>            NHS @Home Programme          </li> <li>            NHS.UK Automatic Moderation of Ratings &amp; Reviews          </li> <li>            Reusable Data Validation Process          </li> <li>            Tool to Assess Privacy Risk of Text Data          </li> </ul>"},{"location":"our_work/tags/#tag:deep-learning","title":"DEEP LEARNING","text":"<ul> <li>            Parkinson's Disease Pathology Prediction          </li> <li>            Renal Health Prediction          </li> </ul>"},{"location":"our_work/tags/#tag:deployed","title":"DEPLOYED","text":"<ul> <li>            A&amp;E Forecasting Tool          </li> <li>            NHS.UK Automatic Moderation of Ratings &amp; Reviews          </li> <li>            Quality Assurance Framework for Data Linkage          </li> <li>            Reusable Data Validation Process          </li> <li>            Text Analysis using Structural Topic Modelling          </li> </ul>"},{"location":"our_work/tags/#tag:diseases","title":"DISEASES","text":"<ul> <li>            CVD Pathways          </li> <li>            Deep Learning to Detect Adrenal Lesions in CT Scans          </li> <li>            Impact of Commercial Data on Predictions          </li> <li>            Inequalities in Diabetes from PHM Data          </li> <li>            Investigating Superpixels in LIME for Explaining Predictions of Facial Images          </li> <li>            Parkinson's Disease Pathology Prediction          </li> <li>            Primary Care Data Generator          </li> <li>            Renal Health Prediction          </li> <li>            SynPath Simulator on Diabetes Pathway          </li> </ul>"},{"location":"our_work/tags/#tag:documentation","title":"DOCUMENTATION","text":"<ul> <li>            AI Dictionary          </li> <li>            AI Ethics in Practice          </li> <li>            MPS Handbook          </li> </ul>"},{"location":"our_work/tags/#tag:emergency-care","title":"EMERGENCY CARE","text":"<ul> <li>            Process Mining with East Midlands Ambulance Service          </li> </ul>"},{"location":"our_work/tags/#tag:ethics","title":"ETHICS","text":"<ul> <li>            AI Ethics in Practice          </li> <li>            Creating a Generic Adversarial Attack for Synthetic Data          </li> <li>            Data Linkage Community of Practice          </li> <li>            Inequalities in Diabetes from PHM Data          </li> <li>            Privacy of Unstructured Data          </li> <li>            Tool to Assess Privacy Risk of Text Data - Extended          </li> </ul>"},{"location":"our_work/tags/#tag:experimental","title":"EXPERIMENTAL","text":"<ul> <li>            Adding a Clinical Focus to Evaluating MM Data Representations          </li> <li>            Applying &amp; Evaluating a Language Model to Patient Safety Data          </li> <li>            CT Alignment &amp; Lesion Detection          </li> <li>            Data Lens          </li> <li>            Impact of Commercial Data on Predictions          </li> <li>            Investigating Superpixels in LIME for Explaining Predictions of Facial Images          </li> <li>            NHS Language Corpus          </li> <li>            Nursing Placement Scheduled Optimisation          </li> <li>            Predicting Negligence Claims          </li> <li>            Renal Health Prediction          </li> <li>            Synthetic Data Generation Pipeline          </li> <li>            TxtRayAlign          </li> <li>            Waiting List Minimum Dataset (WLMDS) Proofs of Concept          </li> </ul>"},{"location":"our_work/tags/#tag:explainability","title":"EXPLAINABILITY","text":"<ul> <li>            Data Linkage Community of Practice          </li> <li>            Investigating Superpixels in LIME for Explaining Predictions of Facial Images          </li> <li>            MPS Handbook          </li> <li>            Quality Assurance Framework for Data Linkage          </li> </ul>"},{"location":"our_work/tags/#tag:financial","title":"FINANCIAL","text":"<ul> <li>            Corporate Services: Pipeline rebuild          </li> <li>            Predicting Negligence Claims          </li> </ul>"},{"location":"our_work/tags/#tag:forecasting","title":"FORECASTING","text":"<ul> <li>            A&amp;E Forecasting Tool          </li> <li>            Ambulance Handover Delay Predictor          </li> <li>            Bed Allocation          </li> <li>            Waiting List Minimum Dataset (WLMDS) Proofs of Concept          </li> </ul>"},{"location":"our_work/tags/#tag:generation","title":"GENERATION","text":"<ul> <li>            Data Lens          </li> <li>            Differential Privacy in a VAE for Synthetic Data Generation          </li> <li>            Generic Patient Simulator          </li> <li>            NHS Synth          </li> <li>            Primary Care Data Generator          </li> <li>            Redbox Copilot          </li> <li>            SynPath Simulator on Diabetes Pathway          </li> <li>            Synthetic Data From Real Data          </li> <li>            Synthetic Data Generation Pipeline          </li> </ul>"},{"location":"our_work/tags/#tag:genomics-data","title":"GENOMICS DATA","text":"<ul> <li>            Embedded Data Scientists in the National Disease Registration Service          </li> </ul>"},{"location":"our_work/tags/#tag:in-development","title":"IN DEVELOPMENT","text":"<ul> <li>            Better Matching Algorithm          </li> <li>            Primary Care Data Generator          </li> </ul>"},{"location":"our_work/tags/#tag:linkage","title":"LINKAGE","text":"<ul> <li>            Better Matching Algorithm          </li> <li>            CVD Pathways          </li> <li>            Data Linkage Community of Practice          </li> <li>            Embedded Data Scientists in the National Disease Registration Service          </li> <li>            MPS Handbook          </li> <li>            Quality Assurance Framework for Data Linkage          </li> <li>            __index.md          </li> <li>            ePMA Auto Coding          </li> </ul>"},{"location":"our_work/tags/#tag:llm","title":"LLM","text":"<ul> <li>            AI Assurance Research Path          </li> <li>            AI Models for Shortlisting Interview Candidates          </li> <li>            Applying &amp; Evaluating a Language Model to Patient Safety Data          </li> <li>            Clinical Measurement Extractor          </li> <li>            Investigating Privacy Risks and Mitigations in Healthcare Language Models          </li> <li>            RAG          </li> <li>            Redbox Copilot          </li> <li>            Synthetic Clinical Notes          </li> <li>            Tool to Assess Privacy Risk of Text Data          </li> <li>            Tool to Assess Privacy Risk of Text Data - Extended          </li> </ul>"},{"location":"our_work/tags/#tag:machine-learning","title":"MACHINE LEARNING","text":"<ul> <li>            AI Assurance Research Path          </li> <li>            Ambulance Handover Delay Predictor          </li> <li>            CT Alignment &amp; Lesion Detection          </li> <li>            Cancer high-risk cohorts          </li> <li>            Clinical Measurement Extractor          </li> <li>            Impact of Commercial Data on Predictions          </li> <li>            Inequalities in Diabetes from PHM Data          </li> <li>            Length of Hospital Day Prediction          </li> <li>            Long Stayer Risk Stratification Baseline Models          </li> <li>            NHS Synth          </li> <li>            Nursing Placement Scheduled Optimisation          </li> <li>            Predicting Negligence Claims          </li> <li>            Risk stratification models for Population and Person Insights (PaPI)          </li> <li>            Waiting List Minimum Dataset (WLMDS) Proofs of Concept          </li> </ul>"},{"location":"our_work/tags/#tag:modelling","title":"MODELLING","text":"<ul> <li>            A&amp;E Forecasting Tool          </li> <li>            Bed Allocation          </li> <li>            Better Matching Algorithm          </li> <li>            Deep Learning to Detect Adrenal Lesions in CT Scans          </li> <li>            Developing SynthVAE          </li> <li>            Impact of Commercial Data on Predictions          </li> <li>            Including Mortality in Hypergraphs for Multi-morbidity          </li> <li>            Length of Hospital Day Prediction          </li> <li>            Long Stayer Risk Stratification Baseline Models          </li> <li>            Nursing Placement Scheduled Optimisation          </li> <li>            Parkinson's Disease Pathology Prediction          </li> <li>            Predicting Negligence Claims          </li> <li>            Primary Care Data Generator          </li> <li>            Process Mining with East Midlands Ambulance Service          </li> <li>            Renal Health Prediction          </li> <li>            Risk stratification models for Population and Person Insights (PaPI)          </li> <li>            Text Analysis using Structural Topic Modelling          </li> <li>            Transforming Healthcare Data With Graph-Based Techniques          </li> <li>            Waiting List Minimum Dataset (WLMDS) Proofs of Concept          </li> </ul>"},{"location":"our_work/tags/#tag:multi-modal","title":"MULTI MODAL","text":"<ul> <li>            Adding a Clinical Focus to Evaluating MM Data Representations          </li> <li>            TxtRayAlign          </li> <li>            Understanding Fariness and Explainability in Multimodal Approaches within Healthcare          </li> </ul>"},{"location":"our_work/tags/#tag:natural-language-processing","title":"NATURAL LANGUAGE PROCESSING","text":"<ul> <li>            AI Assurance Research Path          </li> <li>            AI Models for Shortlisting Interview Candidates          </li> <li>            Adding a Clinical Focus to Evaluating MM Data Representations          </li> <li>            Applying &amp; Evaluating a Language Model to Patient Safety Data          </li> <li>            Clinical Measurement Extractor          </li> <li>            Data Lens          </li> <li>            Enriching Clinical Coding for Neurology Pathways using MedCAT          </li> <li>            NHS Language Corpus          </li> <li>            NHS.UK Automatic Moderation of Ratings &amp; Reviews          </li> <li>            Privacy of Unstructured Data          </li> <li>            RAG          </li> <li>            Redbox Copilot          </li> <li>            Text Analysis using Structural Topic Modelling          </li> <li>            Tool to Assess Privacy Risk of Text Data          </li> <li>            Tool to Assess Privacy Risk of Text Data - Extended          </li> <li>            TxtRayAlign          </li> </ul>"},{"location":"our_work/tags/#tag:neural-networks","title":"NEURAL NETWORKS","text":"<ul> <li>            AI Models for Shortlisting Interview Candidates          </li> <li>            Length of Hospital Day Prediction          </li> <li>            Parkinson's Disease Pathology Prediction          </li> <li>            Renal Health Prediction          </li> <li>            Synthetic Data From Real Data          </li> </ul>"},{"location":"our_work/tags/#tag:open-data","title":"OPEN DATA","text":"<ul> <li>            AI Deep Dive Workshops          </li> <li>            AI Skunkworks Team          </li> <li>            Emerging Privacy Enhancing Technologies          </li> <li>            NHS Language Corpus          </li> <li>            Privacy of Unstructured Data          </li> <li>            RAG          </li> <li>            Understanding Fariness and Explainability in Multimodal Approaches within Healthcare          </li> </ul>"},{"location":"our_work/tags/#tag:patient-identifiable-data","title":"PATIENT IDENTIFIABLE DATA","text":"<ul> <li>            Better Matching Algorithm          </li> <li>            Investigating Privacy Risks and Mitigations in Healthcare Language Models          </li> <li>            MPS Handbook          </li> <li>            Privacy of Unstructured Data          </li> <li>            Synthetic Data From Real Data          </li> <li>            Synthetic Data Generation Pipeline          </li> </ul>"},{"location":"our_work/tags/#tag:paused","title":"PAUSED","text":"<ul> <li>            Generic Patient Simulator          </li> <li>            NHS Language Corpus          </li> <li>            Primary Care Data Generator          </li> <li>            Tool to Assess Privacy Risk of Text Data          </li> <li>            Tool to Assess Privacy Risk of Text Data - Extended          </li> </ul>"},{"location":"our_work/tags/#tag:population-health","title":"POPULATION HEALTH","text":"<ul> <li>            AI Skunkworks Team          </li> <li>            Better Matching Algorithm          </li> <li>            CVD Pathways          </li> <li>            Cancer high-risk cohorts          </li> <li>            Embedded Data Scientists in the National Disease Registration Service          </li> <li>            Inequalities in Diabetes from PHM Data          </li> <li>            Risk stratification models for Population and Person Insights (PaPI)          </li> </ul>"},{"location":"our_work/tags/#tag:prescribing","title":"PRESCRIBING","text":"<ul> <li>            Impact of Commercial Data on Predictions          </li> <li>            ePMA Auto Coding          </li> </ul>"},{"location":"our_work/tags/#tag:primary-care","title":"PRIMARY CARE","text":"<ul> <li>            CVD Pathways          </li> <li>            Including Mortality in Hypergraphs for Multi-morbidity          </li> <li>            NHS @Home Programme          </li> <li>            Primary Care Data Generator          </li> <li>            Transforming Healthcare Data With Graph-Based Techniques          </li> </ul>"},{"location":"our_work/tags/#tag:pseudonymised","title":"PSEUDONYMISED","text":"<ul> <li>            Bed Allocation          </li> <li>            CT Alignment &amp; Lesion Detection          </li> <li>            Nursing Placement Scheduled Optimisation          </li> <li>            Reusable Data Validation Process          </li> <li>            Synthetic Data Generation Pipeline          </li> <li>            Waiting List Minimum Dataset (WLMDS) Proofs of Concept          </li> </ul>"},{"location":"our_work/tags/#tag:publications","title":"PUBLICATIONS","text":"<ul> <li>            Our Team's Publications          </li> </ul>"},{"location":"our_work/tags/#tag:python","title":"PYTHON","text":"<ul> <li>            A&amp;E Forecasting Tool          </li> <li>            AI Assurance Research Path          </li> <li>            AI Models for Shortlisting Interview Candidates          </li> <li>            AI Skunkworks Team          </li> <li>            Adding a Clinical Focus to Evaluating MM Data Representations          </li> <li>            Ambulance Handover Delay Predictor          </li> <li>            Applying &amp; Evaluating a Language Model to Patient Safety Data          </li> <li>            Bed Allocation          </li> <li>            Better Matching Algorithm          </li> <li>            CT Alignment &amp; Lesion Detection          </li> <li>            CVD Pathways          </li> <li>            Cancer high-risk cohorts          </li> <li>            Clinical Measurement Extractor          </li> <li>            Corporate Services: Pipeline rebuild          </li> <li>            Creating a Generic Adversarial Attack for Synthetic Data          </li> <li>            Data Lens          </li> <li>            Deep Learning to Detect Adrenal Lesions in CT Scans          </li> <li>            Developing SynthVAE          </li> <li>            Differential Privacy in a VAE for Synthetic Data Generation          </li> <li>            Generic Patient Simulator          </li> <li>            Impact of Commercial Data on Predictions          </li> <li>            Inequalities in Diabetes from PHM Data          </li> <li>            Investigating Superpixels in LIME for Explaining Predictions of Facial Images          </li> <li>            Length of Hospital Day Prediction          </li> <li>            Long Stayer Risk Stratification Baseline Models          </li> <li>            MPS Handbook          </li> <li>            NHS Language Corpus          </li> <li>            NHS.UK Automatic Moderation of Ratings &amp; Reviews          </li> <li>            Nursing Placement Scheduled Optimisation          </li> <li>            Parkinson's Disease Pathology Prediction          </li> <li>            Predicting Negligence Claims          </li> <li>            Privacy of Unstructured Data          </li> <li>            RAG          </li> <li>            Redbox Copilot          </li> <li>            Renal Health Prediction          </li> <li>            Reproducible Analytical Pipelines Squad          </li> <li>            Reusable Data Validation Process          </li> <li>            Risk stratification models for Population and Person Insights (PaPI)          </li> <li>            Synthetic Clinical Notes          </li> <li>            Synthetic Data From Real Data          </li> <li>            Synthetic Data Generation Pipeline          </li> <li>            Tool to Assess Privacy Risk of Text Data          </li> <li>            Tool to Assess Privacy Risk of Text Data - Extended          </li> <li>            TxtRayAlign          </li> <li>            Waiting List Minimum Dataset (WLMDS) Proofs of Concept          </li> <li>            ePMA Auto Coding          </li> </ul>"},{"location":"our_work/tags/#tag:r","title":"R","text":"<ul> <li>            Embedded Data Scientists in the National Disease Registration Service          </li> <li>            Reproducible Analytical Pipelines Squad          </li> <li>            Waiting List Minimum Dataset (WLMDS) Proofs of Concept          </li> </ul>"},{"location":"our_work/tags/#tag:rap","title":"RAP","text":"<ul> <li>            CVD Pathways          </li> <li>            Cancer high-risk cohorts          </li> <li>            Corporate Services: Pipeline rebuild          </li> <li>            Reproducible Analytical Pipelines Squad          </li> </ul>"},{"location":"our_work/tags/#tag:research","title":"RESEARCH","text":"<ul> <li>            AI Ethics in Practice          </li> <li>            AI Models for Shortlisting Interview Candidates          </li> <li>            AI Skunkworks Team          </li> <li>            Applying &amp; Evaluating a Language Model to Patient Safety Data          </li> <li>            Emerging Privacy Enhancing Technologies          </li> <li>            Enriching Clinical Coding for Neurology Pathways using MedCAT          </li> <li>            Including Mortality in Hypergraphs for Multi-morbidity          </li> <li>            Investigating Privacy Risks and Mitigations in Healthcare Language Models          </li> <li>            NHS Language Corpus          </li> <li>            NHS Synth          </li> <li>            Privacy of Unstructured Data          </li> <li>            RAG          </li> <li>            Synthetic Data Generation Pipeline          </li> <li>            Tool to Assess Privacy Risk of Text Data          </li> <li>            Tool to Assess Privacy Risk of Text Data - Extended          </li> <li>            Transforming Healthcare Data With Graph-Based Techniques          </li> <li>            Understanding Fariness and Explainability in Multimodal Approaches within Healthcare          </li> <li>            Waiting List Minimum Dataset (WLMDS) Proofs of Concept          </li> </ul>"},{"location":"our_work/tags/#tag:secondary-care","title":"SECONDARY CARE","text":"<ul> <li>            A&amp;E Forecasting Tool          </li> <li>            Adding a Clinical Focus to Evaluating MM Data Representations          </li> <li>            Ambulance Handover Delay Predictor          </li> <li>            Bed Allocation          </li> <li>            CT Alignment &amp; Lesion Detection          </li> <li>            CVD Pathways          </li> <li>            Length of Hospital Day Prediction          </li> <li>            Long Stayer Risk Stratification Baseline Models          </li> <li>            Nursing Placement Scheduled Optimisation          </li> <li>            Synthetic Clinical Notes          </li> <li>            TxtRayAlign          </li> <li>            Waiting List Minimum Dataset (WLMDS) Proofs of Concept          </li> <li>            ePMA Auto Coding          </li> </ul>"},{"location":"our_work/tags/#tag:simulation","title":"SIMULATION","text":"<ul> <li>            Bed Allocation          </li> <li>            Developing SynthVAE          </li> <li>            Differential Privacy in a VAE for Synthetic Data Generation          </li> <li>            Generic Patient Simulator          </li> <li>            Nursing Placement Scheduled Optimisation          </li> <li>            Primary Care Data Generator          </li> <li>            Process Mining with East Midlands Ambulance Service          </li> <li>            SynPath Simulator on Diabetes Pathway          </li> <li>            Synthetic Data Generation Pipeline          </li> </ul>"},{"location":"our_work/tags/#tag:sql","title":"SQL","text":"<ul> <li>            Embedded Data Scientists in the National Disease Registration Service          </li> <li>            Length of Hospital Day Prediction          </li> <li>            Long Stayer Risk Stratification Baseline Models          </li> <li>            NHS @Home Programme          </li> </ul>"},{"location":"our_work/tags/#tag:structured-data","title":"STRUCTURED DATA","text":"<ul> <li>            A&amp;E Forecasting Tool          </li> <li>            AI Models for Shortlisting Interview Candidates          </li> <li>            Better Matching Algorithm          </li> <li>            CVD Pathways          </li> <li>            Cancer high-risk cohorts          </li> <li>            Corporate Services: Pipeline rebuild          </li> <li>            Creating a Generic Adversarial Attack for Synthetic Data          </li> <li>            Generic Patient Simulator          </li> <li>            Impact of Commercial Data on Predictions          </li> <li>            Inequalities in Diabetes from PHM Data          </li> <li>            Length of Hospital Day Prediction          </li> <li>            Long Stayer Risk Stratification Baseline Models          </li> <li>            MPS Handbook          </li> <li>            NHS @Home Programme          </li> <li>            NHS Synth          </li> <li>            Predicting Negligence Claims          </li> <li>            Primary Care Data Generator          </li> <li>            Renal Health Prediction          </li> <li>            Reusable Data Validation Process          </li> <li>            Risk stratification models for Population and Person Insights (PaPI)          </li> <li>            Synthetic Data From Real Data          </li> <li>            Waiting List Minimum Dataset (WLMDS) Proofs of Concept          </li> </ul>"},{"location":"our_work/tags/#tag:synthetic-data","title":"SYNTHETIC DATA","text":"<ul> <li>            AI Models for Shortlisting Interview Candidates          </li> <li>            Creating a Generic Adversarial Attack for Synthetic Data          </li> <li>            Developing SynthVAE          </li> <li>            Differential Privacy in a VAE for Synthetic Data Generation          </li> <li>            Generic Patient Simulator          </li> <li>            NHS Synth          </li> <li>            Primary Care Data Generator          </li> <li>            SynPath Simulator on Diabetes Pathway          </li> <li>            Synthetic Clinical Notes          </li> <li>            Synthetic Data From Real Data          </li> <li>            Synthetic Data Generation Pipeline          </li> <li>            Tool to Assess Privacy Risk of Text Data          </li> <li>            Tool to Assess Privacy Risk of Text Data - Extended          </li> </ul>"},{"location":"our_work/tags/#tag:text-data","title":"TEXT DATA","text":"<ul> <li>            Applying &amp; Evaluating a Language Model to Patient Safety Data          </li> <li>            Clinical Measurement Extractor          </li> <li>            Data Lens          </li> <li>            Embedded Data Scientists in the National Disease Registration Service          </li> <li>            NHS Language Corpus          </li> <li>            NHS.UK Automatic Moderation of Ratings &amp; Reviews          </li> <li>            Privacy of Unstructured Data          </li> <li>            RAG          </li> <li>            Redbox Copilot          </li> <li>            Synthetic Clinical Notes          </li> <li>            Text Analysis using Structural Topic Modelling          </li> <li>            Tool to Assess Privacy Risk of Text Data          </li> <li>            Tool to Assess Privacy Risk of Text Data - Extended          </li> <li>            ePMA Auto Coding          </li> </ul>"},{"location":"our_work/tags/#tag:time-series","title":"TIME SERIES","text":"<ul> <li>            A&amp;E Forecasting Tool          </li> <li>            Ambulance Handover Delay Predictor          </li> <li>            Developing SynthVAE          </li> <li>            Differential Privacy in a VAE for Synthetic Data Generation          </li> <li>            Including Mortality in Hypergraphs for Multi-morbidity          </li> <li>            Primary Care Data Generator          </li> <li>            SynPath Simulator on Diabetes Pathway          </li> <li>            Transforming Healthcare Data With Graph-Based Techniques          </li> </ul>"},{"location":"our_work/tags/#tag:unstructured-data","title":"UNSTRUCTURED DATA","text":"<ul> <li>            AI Assurance Research Path          </li> <li>            Adding a Clinical Focus to Evaluating MM Data Representations          </li> <li>            Applying &amp; Evaluating a Language Model to Patient Safety Data          </li> <li>            CT Alignment &amp; Lesion Detection          </li> <li>            Data Lens          </li> <li>            Enriching Clinical Coding for Neurology Pathways using MedCAT          </li> <li>            Investigating Superpixels in LIME for Explaining Predictions of Facial Images          </li> <li>            NHS Language Corpus          </li> <li>            NHS.UK Automatic Moderation of Ratings &amp; Reviews          </li> <li>            Parkinson's Disease Pathology Prediction          </li> <li>            Privacy of Unstructured Data          </li> <li>            RAG          </li> <li>            Synthetic Clinical Notes          </li> <li>            Text Analysis using Structural Topic Modelling          </li> <li>            Tool to Assess Privacy Risk of Text Data          </li> <li>            Tool to Assess Privacy Risk of Text Data - Extended          </li> <li>            TxtRayAlign          </li> </ul>"},{"location":"our_work/tags/#tag:visual-data","title":"VISUAL DATA","text":"<ul> <li>            AI Assurance Research Path          </li> <li>            Adding a Clinical Focus to Evaluating MM Data Representations          </li> <li>            CT Alignment &amp; Lesion Detection          </li> <li>            Deep Learning to Detect Adrenal Lesions in CT Scans          </li> <li>            Investigating Superpixels in LIME for Explaining Predictions of Facial Images          </li> <li>            Parkinson's Disease Pathology Prediction          </li> <li>            TxtRayAlign          </li> </ul>"},{"location":"our_work/tags/#tag:webdev","title":"WEBDEV","text":"<ul> <li>            AI Dictionary          </li> <li>            Bed Allocation          </li> <li>            Data Lens          </li> <li>            Quality Assurance Framework for Data Linkage          </li> </ul>"},{"location":"our_work/tags/#tag:wip","title":"WIP","text":"<ul> <li>            A&amp;E Forecasting Tool          </li> <li>            AI Assurance Research Path          </li> <li>            AI Ethics in Practice          </li> <li>            Better Matching Algorithm          </li> <li>            CVD Pathways          </li> <li>            Cancer high-risk cohorts          </li> <li>            Clinical Measurement Extractor          </li> <li>            Corporate Services: Pipeline rebuild          </li> <li>            Data Linkage Community of Practice          </li> <li>            Emerging Privacy Enhancing Technologies          </li> <li>            Synthetic Clinical Notes          </li> <li>            ePMA Auto Coding          </li> </ul>"},{"location":"our_work/tags/#tag:workforce","title":"WORKFORCE","text":"<ul> <li>            AI Deep Dive Workshops          </li> <li>            AI Dictionary          </li> <li>            AI Models for Shortlisting Interview Candidates          </li> <li>            Data Lens          </li> <li>            NHS.UK Automatic Moderation of Ratings &amp; Reviews          </li> <li>            Nursing Placement Scheduled Optimisation          </li> <li>            Predicting Negligence Claims          </li> <li>            Redbox Copilot          </li> <li>            Reproducible Analytical Pipelines Squad          </li> <li>            Reusable Data Validation Process          </li> <li>            Text Analysis using Structural Topic Modelling          </li> <li>            Tool to Assess Privacy Risk of Text Data - Extended          </li> </ul>"},{"location":"our_work/data-linkage-hub/","title":"Overview - Data Linkage Hub","text":"<p>Data Linkage is a business-critical process within many government organisations, including NHS England. Being able to link patients across their care journey enables both direct care services and research studies on admin data, which in turn, influences healthcare policies. So taking care of this important service is why the Data Linkage hub was created in the new NHS England.</p> <p>The role of the Data Linkage hub in NHS England includes:</p> <ul> <li>identifying points of collaboration with other government departments </li> <li>mapping the stakeholders involved in data linkage - both internal and external</li> <li>feeding user needs to the Data Linkage vision</li> </ul>","tags":["BEST PRACTICE","LINKAGE"]},{"location":"our_work/data-linkage-hub/#work-we-do","title":"Work we do","text":"<p>Info</p> <p>Click each heading to find out more!</p>","tags":["BEST PRACTICE","LINKAGE"]},{"location":"our_work/data-linkage-hub/#quality-assurance-framework","title":"Quality Assurance Framework","text":"<p>If we want to achieve a consistent and high quality approach to linking data, which allows for robust, transparent and auditable results, we also need a framework to operate within. Hence, this workstream aims at creating, testing and implementing in the business process a Quality Assurance Framework for Data Linkage.</p>","tags":["BEST PRACTICE","LINKAGE"]},{"location":"our_work/data-linkage-hub/#better-matching-algorithm","title":"Better Matching Algorithm","text":"<p>We're currently working on implementing a probabilistic linkage model using Splink, in order to improve linkage outcomes, and by extension, patient outcomes. </p>","tags":["BEST PRACTICE","LINKAGE"]},{"location":"our_work/data-linkage-hub/#community-of-practice","title":"Community of Practice","text":"<p>We are fostering a community of practice in NHS England to help people do the best linkage they can, and encourage them to be connected with the cross-government Data Linkage Champions network. The community of practice is open to any data linkage stakeholders in NHS England - to join the community of practice go here.</p>","tags":["BEST PRACTICE","LINKAGE"]},{"location":"our_work/data-linkage-hub/#mps-documentation","title":"MPS Documentation","text":"<p>We have been documenting how the Person_ID is generated via the Master Person Service, to make the current process of linking data in the NHS more transparent and easy to understand. </p>","tags":["BEST PRACTICE","LINKAGE"]},{"location":"our_work/data-linkage-hub/#_1","title":"Overview - Data Linkage Hub","text":"Output Link MPS Diagnostics Github Person_ID Handbook NHS England Website Quality Assurance Framework Work in Progress Link Community of Practice (internal only) Teams Channel","tags":["BEST PRACTICE","LINKAGE"]},{"location":"our_work/data-linkage-hub/linkage-projects/better-matching/","title":"Probabilistic Linkage Model","text":"","tags":["POPULATION HEALTH","LINKAGE","MODELLING","PYTHON","PATIENT IDENTIFIABLE DATA","WIP","STRUCTURED DATA","IN DEVELOPMENT"]},{"location":"our_work/data-linkage-hub/linkage-projects/better-matching/#crafting-a-model-that-suits-nhs-england-data-linkage-needs","title":"Crafting a model that suits NHS England data linkage needs","text":"<p>This project aims at developing an alternative data linkage model to MPS (Master Person Service) by creating a probabilistic linkage model using the package called  Splink, which was developed by the Ministry of Justice (MoJ) Data Science Team.</p> <p>The linkage pipeline consists of a few steps:</p> <ul> <li>Dataset Ingestion</li> <li>Pre-processing </li> <li>Blocking rules</li> <li>Distance Metrics</li> <li>Training</li> <li>Prediction </li> <li>Evaluation</li> </ul> <p>These can be used as a full pipeline or as individual building blocks.</p> <p>Each of these steps requires research into linkage best practice, testing on samples of our data, feasibility studies of computational power required, and then thorough evaluation. We are working with an incremental improvement plan and a series of iterative MVPs to ensure that the pipeline has the highest quality we can achieve within our computational limits. </p> <p>We have also added additional configuration to the pipeline to allow for a deduplication task. This is in order to try and identify possible duplicate records in the Personal Demographics Service (PDS).</p> <p>These diagrams illustrate the structure of the different notebooks in this repository. The diagrams show the <code>_linkage</code> version of each notebook, but for each there is also a <code>_dedupe</code> version with a similar structure.</p> <p>This first image describes the training of the model, and the process of it predicting linkages. </p> <p>This image shows the process that the predictions undergo for evaluation </p>","tags":["POPULATION HEALTH","LINKAGE","MODELLING","PYTHON","PATIENT IDENTIFIABLE DATA","WIP","STRUCTURED DATA","IN DEVELOPMENT"]},{"location":"our_work/data-linkage-hub/linkage-projects/better-matching/#building-a-model-with-transparency-in-mind","title":"Building a model with transparency in mind","text":"<p>Users of linked data have to rely on the accuracy of the process created by others as often the process of linking data is not under their control. That is why one of the main focus of the model we are building is transparency of the methods and explainability of the results.</p>","tags":["POPULATION HEALTH","LINKAGE","MODELLING","PYTHON","PATIENT IDENTIFIABLE DATA","WIP","STRUCTURED DATA","IN DEVELOPMENT"]},{"location":"our_work/data-linkage-hub/linkage-projects/better-matching/#_1","title":"Probabilistic Linkage Model","text":"Output Link Splink Linkage Pipeline Github","tags":["POPULATION HEALTH","LINKAGE","MODELLING","PYTHON","PATIENT IDENTIFIABLE DATA","WIP","STRUCTURED DATA","IN DEVELOPMENT"]},{"location":"our_work/data-linkage-hub/linkage-projects/cop/","title":"Data Linkage Community of Practice (DL CoP)","text":"","tags":["BEST PRACTICE","EXPLAINABILITY","ETHICS","LINKAGE","WIP"]},{"location":"our_work/data-linkage-hub/linkage-projects/cop/#why-do-we-want-a-community-of-practice","title":"Why do we want a Community of Practice?","text":"<p>In NHS England data linkage occurs at various stages of the data lifecycle, involving different stakeholders (from data engineers, to analysts) and happening across different platforms. There exist pockets of knowledge and expertise that operate independently of each others. The Community of Practice wants to support Data Linkage stakeholders in NHS England to share their expertise and best practices with colleagues across the organisation.  This is also in response to the Data Linkage Survey in which colleagues expressed a clear interest in cultivating a collaboration space.</p> <p></p>","tags":["BEST PRACTICE","EXPLAINABILITY","ETHICS","LINKAGE","WIP"]},{"location":"our_work/data-linkage-hub/linkage-projects/cop/#data-linkage-community-of-practice-mission","title":"Data Linkage Community of Practice: Mission","text":"<p>The mission of our community of practice is to facilitate collaboration and an exchange of knowledge, tools and innovative solutions among data linkage stakeholders within NHS England and with and outlook onto other government and research institutions, enabling members to share and adopt effective practices. </p>","tags":["BEST PRACTICE","EXPLAINABILITY","ETHICS","LINKAGE","WIP"]},{"location":"our_work/data-linkage-hub/linkage-projects/cop/#how-can-i-join","title":"How can I join?","text":"<p>You can request access by clicking here. This Teams Channel is restricted to only NHS England employees.</p>","tags":["BEST PRACTICE","EXPLAINABILITY","ETHICS","LINKAGE","WIP"]},{"location":"our_work/data-linkage-hub/linkage-projects/cop/#data-linkage-champions-network","title":"Data Linkage Champions Network","text":"<p>The CoP wants to connect closely with the Data Linkage Champions Network, a cross-government initiative created by the Government Data Quality Hub (DQ Hub) in the Office for National Statistics (ONS) to work better as a community to improve methods, their applications, and skills in the field of data linkage. If you want to know more about this, go to the Data Linkage Champions Network page.</p>","tags":["BEST PRACTICE","EXPLAINABILITY","ETHICS","LINKAGE","WIP"]},{"location":"our_work/data-linkage-hub/linkage-projects/cop/#_1","title":"Data Linkage Community of Practice (DL CoP)","text":"","tags":["BEST PRACTICE","EXPLAINABILITY","ETHICS","LINKAGE","WIP"]},{"location":"our_work/data-linkage-hub/linkage-projects/mps-handbook/","title":"MPS Documentation - the Person_ID handbook","text":"<p>The Person_ID is a unique patient identifier used by NHS England with the objective of standardising the approach to patient-level data linkage across different data sets. The Data Linkage team has produced a detailed documentation of the data linkage algorithm used to create the Person_ID, namely the Master Person Service (MPS). The figure below is an overview of the process. The full documentation is available in this NHS England site. The MPS handbook particularly focuses on how this algorithm is applied to Hospital Episode Statistics (HES). </p> <p></p> <p>To contact us about this work, email: nhsdigital.personidquestions@nhs.net</p> Output Link MPS Diagnostics Github Person_ID Handbook NHS England Website","tags":["EXPLAINABILITY","DOCUMENTATION","LINKAGE","COMPLETE","STRUCTURED DATA","PATIENT IDENTIFIABLE DATA","PYTHON"]},{"location":"our_work/data-linkage-hub/linkage-projects/mps-handbook/#_1","title":"MPS Documentation - the Person_ID handbook","text":"","tags":["EXPLAINABILITY","DOCUMENTATION","LINKAGE","COMPLETE","STRUCTURED DATA","PATIENT IDENTIFIABLE DATA","PYTHON"]},{"location":"our_work/data-linkage-hub/linkage-projects/qaf/","title":"Quality Assurance Framework for Data Linkage","text":"<p>Data Linkage is a business-critical process within many government organisations, including NHS England. Research publications, official statistics, but also many direct care applications depend on data linkage. Its importance is further amplified when considering privacy preserving principles that require to minimise the use of patients' personal identifiable information. Consequently, data linkage is initiated early in the data lifecycle, establishing a substantial reliance of downstream applications on the quality of the linkage process.</p> <p>However, too often data linkage is seen as an exclusive software development and data engineering exercise instead of a modelling challenge, and there is not an appropriate level of quality assurance applied at the different stages of the process. This is why we have worked on the Quality Assurance Framework for Data Linkage, which is a tool for data linkage practitioners to determine the necessary quality assurance levels at every stage of the data linkage process:</p> <p></p> <p>The required level of quality assurance varies by project and is determined by the data linker and data users. The triage questions in the framework provide a structured approach to deciding the minimum expected levels by type of project.</p> <p>The Quality Assurance Framework guides stakeholders to make well-informed choices based on a clear understanding of potential risks and benefits. Additionally, it can be used as a detailed record-keeping tool that helps evaluate and manage data linkage project aspects.</p> <p>Reach out to the Data Linkage Hub if you want to contribute to this project.</p>","tags":["BEST PRACTICE","EXPLAINABILITY","LINKAGE","COMPLETE","WEBDEV","DEPLOYED"]},{"location":"our_work/data-linkage-hub/linkage-projects/qaf/#_1","title":"Quality Assurance Framework for Data Linkage","text":"Output Link Quality Assurance Framework Find the website here","tags":["BEST PRACTICE","EXPLAINABILITY","LINKAGE","COMPLETE","WEBDEV","DEPLOYED"]},{"location":"what_is_data_science/","title":"What is Data Science?","text":"<p>Quoting the Digital, Data and Technology (DDaT) Capability Framework:</p> <p>Data science is a broad and fast-moving field spanning maths, statistics, software engineering and communications. Data scientists will often work as part of a multidisciplinary team, using data and analytics to inform and achieve organisational goals.</p> <p>Data scientists will often use programming languages such as Python and R (among others!) to solve problems within a business or to inform decisions, working with other colleagues such as analysts and data engineers.</p> <p>Some examples of the kinds of problems data science can help with are:</p> <ul> <li>Getting more value out of unstructured data (e.g. text, images, audio) through \"Natural Language Processing\", \"Neural Networks\" and recently \"Large Language Models\" (e.g. ChatGPT).</li> <li>Modelling systems and forecasting (though compared with statisticians and economists, this tends to be more empirical, requiring more focus on evaluation).</li> <li>Explaining existing models and their performance - communication is a key pillar of data science: making the complex understandable to everyone else.</li> <li>Enriching and transforming data, such as through linkage, feature engineering, artificial and synthetic data generation.</li> <li>Classification and regression - that is saying what something is, or if something will or won't happen, and/or quantifying something unknown.</li> </ul> <p>Artificial Intelligence and Machine Learning are techniques which are widely associated with data science and data scientists, and which can be applied to basically any of the problems listed above. See also this article on the \"Seven Patterns of AI\".</p> How do data scientists differ from analysts and data engineers? <p>Data science can be quite hard to pin down, as it covers a lot of different techniques, and problems, and data scientists themselves often have a lot of overlap with analysts and data engineers. Analysts and engineers might well use data science techniques in their work!</p> <p>However, generally data scientists are more focused on looking ahead, embracing and exploiting new techniques across a range of different types of data, e.g. unstructured data, such as text, images, audio.</p>"},{"location":"what_is_data_science/Benefits%20of%20Data%20Science%20in%20the%20NHS/","title":"Benefits of Data Science in the NHS","text":"<p>Data science can be helpful in solving a number of problems. Specifically, this could lead to benefits such as:</p> <ul> <li>better resource planning (e.g. improving bed allocation using AI)</li> <li>increased responsiveness to demand and seasonal pressures (e.g. A&amp;E demand prediction)</li> <li>improved data quality via better data linkage, leading to higher quality analytics and decision making.</li> <li>making existing processes more efficient, benefitting patients and the public (e.g. NLP based automation of NHS website review moderation)</li> <li>helping our colleagues work more efficiently and get the most out of our existing and new technology (e.g. Reproducible Analytical Pipelines)</li> </ul>"},{"location":"what_is_data_science/How%20you%20can%20learn%20Data%20Science/","title":"How You Can Learn Data Science","text":"<p>Data science isn't just for data scientists! As a profession, we're passionate about sharing these skills and techniques.</p> <p>For this purpose we've put together a monthly newsletter with valuable insights, training opportunities and events for people interested in learning more about the various aspects of data science, further developing their skills, and progressing in their career:</p>  [NHS England Data Science Professional Development Newsletter](https://nhsengland.github.io/datascience-pd-newsletter/){ .md-button .md-button--primary target=\"_blank\"}  <p>Note</p> <p>The newsletter is targeted towards members of the NHS England Data Science Profession, so some links may only be accessible to those with the necessary login credentials, however the newsletter and its archive are available for all at the link above.</p> <p>Through AnalystX we also support the NHS Data Science Community which is the home of spreading data science knowledge within the NHS.</p> <p>You can also learn a lot about data science by simply getting to know the wider cross-government/health community:</p> <ul> <li>Govt Data Science Community</li> <li>NHS R Community</li> <li>NHS Pycom</li> <li>AnalystX</li> </ul> <p>These have some overlap, but each has it's own niche, depending on what language or area you want to focus more on (e.g. Python, R, specific use cases).</p>"},{"location":"articles/archive/2025/","title":"2025","text":""},{"location":"articles/archive/2024/","title":"2024","text":""},{"location":"articles/archive/2023/","title":"2023","text":""},{"location":"articles/category/volunteering/","title":"Volunteering","text":""},{"location":"articles/category/data-science-interviews/","title":"Data Science Interviews","text":""},{"location":"articles/category/professional-development/","title":"Professional Development","text":""},{"location":"articles/category/hsma/","title":"HSMA","text":""},{"location":"articles/category/waiting-lists/","title":"Waiting Lists","text":""},{"location":"articles/category/python/","title":"Python","text":""},{"location":"articles/category/quantum/","title":"Quantum","text":""},{"location":"articles/category/optimisation/","title":"Optimisation","text":""},{"location":"articles/category/events/","title":"Events","text":""},{"location":"articles/category/rap/","title":"RAP","text":""},{"location":"articles/category/ethical-ai/","title":"Ethical AI","text":""},{"location":"articles/category/comms-and-marketing/","title":"Comms and Marketing","text":""},{"location":"articles/category/presentation/","title":"Presentation","text":""},{"location":"articles/category/blog/","title":"Blog","text":""},{"location":"articles/category/llms/","title":"LLMs","text":""},{"location":"articles/category/generative-ai/","title":"Generative AI","text":""},{"location":"articles/category/evaluation/","title":"Evaluation","text":""},{"location":"articles/category/assurance/","title":"Assurance","text":""},{"location":"articles/category/explainability/","title":"Explainability","text":""},{"location":"articles/category/image-classification/","title":"Image Classification","text":""},{"location":"articles/category/nhs-websites/","title":"NHS Websites","text":""},{"location":"articles/category/privacy/","title":"Privacy","text":""},{"location":"articles/category/annotation-tools/","title":"Annotation Tools","text":""},{"location":"articles/page/2/","title":"Articles and Blog posts","text":""}]}